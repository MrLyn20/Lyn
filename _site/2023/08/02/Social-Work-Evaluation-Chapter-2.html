<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Part II" />
<meta property="og:description" content="Part II" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/02/Social-Work-Evaluation-Chapter-2.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/02/Social-Work-Evaluation-Chapter-2.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-02T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-02T00:00:00+09:00","datePublished":"2023-08-02T00:00:00+09:00","description":"Part II","headline":"Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/02/Social-Work-Evaluation-Chapter-2.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/02/Social-Work-Evaluation-Chapter-2.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=4a2f578e3a0ec0e4dbfeae7b6ac7a783c80ed551">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>2 August 2023</small>
<h1>Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations</h1>

<p class="view">by JAMES R. DUDLEY</p>

<p><strong>Part II</strong></p>

<p><strong>Orientation to the Bigger Picture of Evaluations</strong></p>

<p>What’s Next?</p>

<p>The chapters in Part II focus on the bigger picture of evaluations. In Chapter 2, several theoretical perspectives and approaches on evaluation are presented along with the key perspectives used in the book. The history of evaluations beginning over 100 years ago are also highlighted. Professional ethics that are important to follow are highlighted in Chapter 3, and some common types of evaluations evident in agencies are described in Chapter 4. Chapter 5 begins to delve into how evaluations are conducted and provides guidelines for how to focus an evaluation.</p>

<p>These chapters are intended to provide a general background of relevant information for the chapters that follow them.</p>

<h1 id="chapter-2-the-influence-of-history-and-varying-theoretical-views-on-evaluations"><strong>Chapter 2 The Influence of History and Varying Theoretical Views on Evaluations</strong></h1>

<p>Evaluations can take many different forms. We will look at why in Chapter 2.</p>

<p>Evaluations are conducted in many different ways and this chapter focuses on the question of why. It is partially because there are so many different reasons why evaluations are conducted. As described in Chapter 1, evaluations are used to fulfill many different purposes. They can be to determine the effectiveness of a program or practice intervention, its efficiency, its level of quality, or its relevance to helping a particular client group.</p>

<p>Further, evaluations can vary in method. For example, from being small exploratory case studies of one individual to large explanatory studies involving big samples and well-tested measurement instruments. Evaluations can also vary from being more inductive or more deductive and more qualitative or more quantitative in method. Moreover, evaluators have widely varied approaches based on their disciplines. The discipline of social work, being one of them, is largely the focus of this text.</p>

<p>The chapter addresses important historical events and several theoretical perspectives that have influenced what evaluations are about. These varying theoretical perspectives are important to know so that we can be more fully aware of why evaluators do what they do. The chapter begins by highlighting some pertinent historical events that have significantly influenced the evolution of evaluations over more than one hundred years. This is followed by a description of several theoretical perspectives and approaches on evaluations. The chapter closes with a synthesis of the many perspectives and an introduction to how they have contributed to the concept of evaluation developed in this text.</p>

<h2 id="1-relevant-events-in-history"><strong>1. RELEVANT EVENTS IN HISTORY</strong></h2>

<h3 id="11-the-impact-of-historical-events-on-program-evaluation">1.1 The Impact of Historical Events on Program Evaluation</h3>

<p>The history of social programs has had an enormous impact on what we refer to as evaluations today. Evaluations have emerged through a long and tumultuous history of experimentation with social programs. Several historical events have been instrumental in ensuring that social programs are increasingly accountable to the larger society and to the funding and regulatory agencies that oversee their activities. Concurrently, social workers who function within these programs are also becoming more accountable for what they do.</p>

<p>Program evaluations have not always been a central concern in the functioning of social programs. However, even administrators of social programs operating during the settlement movement and Charity Organization Societies in the early part of the past century were mindful that their programs must work. However, their costs were relatively low, they often depended on volunteers, and they operated at the local level without state and federal governmental assistance or regulation. Perhaps this local control and management ensured that those who oversaw the early programs kept a close watch on their volunteers and activities. Further, they most likely used common sense to determine if they were making headway in accomplishing their goals.</p>

<p>Since then, social problems have become much more complex and expensive to solve. Unfortunately at this point in time, available solutions will, at best, require broad governmental support at an enormous cost that many segments of society may not be willing to allow. Problems like poverty, chronic and severe diseases and other medical problems, addictions, and marital strife are examples. Also, these social programs have expanded to serve infinitely more and varied types of people.</p>

<p>The New Deal legislation in the 1930s ushered in the beginning of national government involvement in social programs by establishing old-age pensions and public assistance for the first time. The public assistance programs, for example, were intended to provide temporary relief to poor people who could not find employment. The emphasis in these programs was on helping limited numbers and only during the Great Depression. Yet, as we know in retrospect, public assistance programs developed into a huge, burgeoning system of social services that became permanent, not temporary, relief.</p>

<p>Social programs soared in the 1960s. Historical events of that decade brought major changes in public policies, particularly in the areas of civil rights, poverty, housing, employment, and de-institutionalization of people with mental illnesses and mental retardation. President Johnson’s Great Society and War on Poverty were instrumental in creating numerous programs, including local Community Action Councils, Job Corp, Neighborhood Youth Corp, Community Legal Services, and Head Start, among others. But the War on Poverty promised much more than it could deliver, as it claimed it could achieve broad goals like eliminating poverty and building strong low-income communities and local community control.</p>

<p>In addition, the 1962 and 1967 Social Security Amendments created new initiatives including social services for welfare recipients who were receiving financial assistance. Rehabilitation became the mantra that replaced relief, and training people for useful work became the goal for addressing prolonged dependency. The 1967 Social Security Amendments separated eligibility for public assistance and social services in the hope of overcoming family disorganization and conflicts. These new social service functions were to be filled by professionals with advanced degrees in social work and other fields who would help poor people strengthen their family ties and become employed, educated, and ultimately self-sufficient. These initiatives not only promised more than they could deliver, but they also led to runaway federal funding to pay for them.</p>

<p>These and other events moved program evaluation to the top of the list of priorities for legislators, funding agencies, and program providers. The pie-in-the-sky promises and expanding federal budgets devoted to social programs created an environment in which a rising social movement of fiscal conservatives emerged and demanded more emphasis on financial cutbacks and fiscal management, program accountability, and evaluations.</p>

<p>After the euphoric reform days of the 1960s, social programs began to face major problems in the 1970s and 1980s. Such programs, particularly those funded by governmental agencies, were no longer popular among many people in positions of power and came under increasing attack. Accountability was still lacking for many of these programs, which left them exposed to growing criticism. Their ambitious agendas sought too much, including overcoming dependency on government and eliminating poverty. Ironically, the opposite appeared to be happening as the number of recipients in public programs soared and costs to the public exploded.</p>

<p>The growing demand for social program accountability culminated in the passage of the Government Performance and Results Act of 1993. This act required strategic planning by all governmental agencies focusing on client outcomes and especially required definitions of how their outcomes would be achieved. The Act gave the U.S. Congress tools to hold all governmental programs and those receiving governmental funds accountable. From here on, the federal government intended to base budget decisions on the agencies’ reports of performance and success in achieving their intended outcomes. This federal act was so important to social work that it became a focus of a key article in the journal Social Work (Kautz, Netting, Huber, Borders, &amp; Davis, 1997).</p>

<p>From this point on, programs would be expected to develop annual performance plans with clear goals and objectives for their programs and employees, quantifiable measurements of these goals, strategies for achieving goals and objectives, a description of the program evaluations that they would use, and a presentation of the results demonstrating the extent to which they had reached their identified goals. These new policies were implemented in 1997. By the end of that year, almost one hundred federal agencies delivered their plans to Congress as required. However, the new act helped federal overseers realize that evaluation plans did not offer an easy or quick fix.</p>

<p>Performance goals and measures were not as results-oriented as expected, and some goals were found not to be objective or measurable. Further, these goals and objectives were not always linked to the individual programs and day-to-day agency activities intended to affect them. Nevertheless, the Government Performance and Results Act has become a key catalyst in an ongoing social movement aimed to hold social programs accountable for documenting their achievements. Amazingly, virtually every program proposal submitted to a funding agency today requires an evaluation.</p>

<p>Today, the development of program and practice evaluations has become more and more professional. Professions like social work, education, and other disciplines have formed professional associations to promulgate standards for high-quality, acceptable evaluations and a regular conference context for sharing ideas and experiences in implementing evaluations. These types of activities occur in professional associations like the National Association of Social Workers and the Council on Social Work Education (CSWE). The CSWE gives major emphasis to evaluating BSW and MSW social work programs as part of its accreditation standards. Multidisciplinary organizations like the American Evaluation Association are also actively involved in promoting similar purposes, and there are at least eight professional journals that focus exclusively on evaluation (see Chapter 6).</p>

<p>The Impact of Historical Events on Practice Evaluation Prominent social workers and government officials have frequently challenged the social work profession and other helping professions to be accountable for their practice effectiveness. Back in 1973, Joel Fischer authored a landmark article in Social Work titled “Is Casework Effective? A Review,” in which he examined all of the studies that he could find to determine whether social casework (what we call practice with individuals currently) was effective. Fischer assumed that effectiveness could be determined only by using a classic experimental design that included a control group that would not receive social casework services. He found seventy studies that he had to discard because they did not have a control group. Only eleven studies met this criterion and thus were reviewed. Unfortunately, Fischer found no evidence from the eleven studies that social casework worked; in some instances, he claimed that client progress declined. This was an embarrassing study that got a lot of bad press for social work at that time.</p>

<p>Numerous evaluations and meta-analyses have been published since Fischer’s study that have focused on the effectiveness of specific social work interventions in helping a variety of client populations (for an example of such a meta-analysis, see Harding &amp; Higginson, 2003). In brief, this meta-analysis uncovered findings not too dissimilar from Fischer’s. Of twenty-two evaluations of interventions to help cancer and palliative care patients, few even used experimental and quasi-experimental designs that could evaluate the interventions’ effectiveness.</p>

<p>The social work profession has been involved in a long, arduous, and often impressive search for new ways to examine how to scrutinize social work interventions to ensure that they work for clients. This examination has continued, sometimes in dramatic ways, up to the present. Gambrill and Gibbs (2017) have made major contributions to the development of critical-thinking principles that practitioners can use. In addition, several authors (e.g., Gibbs, 2003; Glicken, 2005; O’Hare, 2005, 2015) have expanded on the concept of evidence-based practice. O’Hare introduces evidence-based concepts that apply to specific client conditions and circumstances like schizophrenia, depression, personality disorders, and child abuse and neglect.</p>

<p>Glicken has reviewed what evidence-based practice is and how it can be enhanced and measured. He has also focused on evidence-based practice with different client groups including those with substance abuse problems, personality disorders, and mental illnesses. The importance of the therapeutic relationship and the challenges of measuring its impact were also explored by Glicken including variations based on race, gender, and ethnicity. Finally, Fischer and colleagues (e.g., Bloom, Fischer, &amp; Orme, 2009) have helped develop more sophisticated practice evaluation tools, especially additional ways to apply a single system design to evaluating practice and analyzing the results of single-system designs. Greater use of standardized scales is also encouraged by these authors.</p>

<p>Most of the authors writing about evidence-based practice have emphasized the importance of research concepts like experimental and quasi-experimental designs, internal and external validity, influences of sampling, both quantitative and qualitative designs, standardized scales and other related topics. Unfortunately, these are not usually the topics of much interest in most practice courses and with most students in practice tracks. Instead, it seems they have much more invested in developing their own practice approaches that can be artful, complex, and sometimes unique. This makes their approaches difficult if not impossible to replicate for evaluations.</p>

<p>It is the author’s observation, for example, that most social work programs offer separate courses on practice and evaluating practice. The practice courses typically give little attention to the details of applying the tools for evaluating practice such as a single-system design and leave this up to research and evaluation courses taught by a different set of teachers. In this regard, more attention has always been needed in most social work academic programs to infuse the practice and research curricula and the preparedness of faculty to implement it.</p>

<p>Social work faculty, students, and practitioners who are weaker in either area (practice or research) should recommit themselves to further develop their expertise in that neglected area. Social work programs and the profession generally must also do their part in stressing the important work that still needs to be done in forging the critical relationship between delivering practice and measuring its effectiveness. The accreditation agency, the CSWE is to be commended for highlighting evaluation as one of nine overall social work competencies in its accreditation standards (CSWE, 2015). It reads as follows: “Competency 9: Evaluate Practice with Individuals, Families, Groups, Organizations, and Communities.”</p>

<h2 id="2-varying-views-on-theoretical-approaches"><strong>2. VARYING VIEWS ON THEORETICAL APPROACHES</strong></h2>

<p>The evaluation enterprise is by no means monolithic or uniform in the way it is conducted. It has been driven by many different perspectives. Because all the theoretical perspectives and approaches existing in the evaluation field are too numerous to cover in this short introduction (e.g., Stufflebean, 2001), five are highlighted that seem to be most pertinent to social work. They are the results-oriented approach, feminist approaches, empowerment approaches, experimental and quasi experimental designs, and fourth-generation evaluations. Note that these five theoretical approaches also sometimes have overlapping features. The specific concepts, principles, and skills that each of them uses are described next to assists readers in determining what you may choose to incorporate into your own evolving eclectic evaluation approach.</p>

<hr />
<p>Table 2.1. Five Theoretical Approaches</p>

<ul>
  <li>Results-oriented</li>
  <li>Feminist</li>
  <li>Empowerment</li>
  <li>Experimental and quasi-experimental</li>
  <li>Fourth-generation</li>
</ul>

<hr />

<h3 id="21-results-oriented-approach"><strong>2.1 Results-Oriented Approach</strong></h3>

<p>The results-oriented approach focuses on performance, outcomes, and accountability. Evidence-based program and practice approaches and outcome evaluations are most important in this approach. This approach has emerged as a public necessity because social programs have often fallen short of meeting their goals or have had difficulty communicating how they were achieving them (Wholey, 1999).</p>

<p>Advocates of the results-oriented approach believe that it is important to convince a variety of stakeholders that the outcomes they achieve are the ultimate reason why they are in business. In this regard, Wholey presents this three-step process: (a) develop agreement among stakeholders on goals and strategies, (b) regularly measure and evaluate performance goals, and (c) use performance results to improve programs and enhance accountability to stakeholders and the public. This approach gives most of its attention to the ultimate point of a program’s existence: socially desirable outcomes for clients.</p>

<p>As mentioned in Chapter 1, Martin and Kettner (1996) give primary attention to performance and its measurement based on elements such as the efficiency and effectiveness of a social program. Efficiency involves calculating the amount of service provided and the number of clients who complete the program (program outputs) and comparing the outputs to the costs involved (program inputs). Effectiveness, another element of performance measurement, focuses on the outcomes of social programs or results, impact, and accomplishments. Examples could include the number of parents who stop abusing their children as the result of a parenting-skills training program or, in an adoption agency, the number of children successfully placed with new parents. The concepts of program inputs, outputs, and outcomes are developed further in later chapters.</p>

<h3 id="22-feminist-approaches"><strong>2.2 Feminist Approaches</strong></h3>

<p>Feminist evaluations are defined by the substantive areas that some researchers choose to study. Feminist evaluators are likely to pursue evaluations that increase social justice for the oppressed, especially but not exclusively for oppressed women. They are inclined to focus on the relative positions and experiences of women in relation to men and the effects that gender issues have on both genders (Deem, 2002). Examples of topics in agency organizations include gender discrimination in hiring and promotion, the low representation of women in administrative and other leadership roles, salary inequities based on gender, and family-supportive policies of employers. Other feminist topics could include the roles of men and women in parenting children and assuming household tasks, enforcement of child support, the earning power of single-parent households, and the availability of quality day care for low-income families. In recent years, feminist evaluations are also becoming increasingly evident in other countries (e.g., Brisolara, Seigart, &amp; SenGupta, 2014).</p>

<p><strong><em>An Example of a Feminist Study: Women’s Salaries</em></strong></p>

<blockquote>
  <p>Gibelman (2003) examined the issue of men’s and women’s salaries in the human services by analyzing existing data from the Bureau of Labor Statistics. She found that salary disparities continue to exist. She attributes these disparities to discrimination patterns. Gibelman recommended several strategies to combat such discrimination, including public and professional education and advocacy. She also pointed out that the gender discrimination experienced by social workers is also evident among some client groups.</p>
</blockquote>

<p>In addition, feminist evaluations can be partially defined by the research methods that they prefer. Most feminist researchers prefer qualitative over quantitative methods because of the flexibility built into this method and the value that it places on uncovering new knowledge about and insight into interventions at deeper levels of meaning. Some suggest that similar principles guide both feminist studies and qualitative methods (Skeggs, 2001). Both feminist and qualitative methodologists are sensitive to a power differential between evaluators and participants in evaluations, and both are concerned that the participants gain something from their involvement in an evaluation.</p>

<p>Both feminist and qualitative methodologists are also likely to assume accountability to the wider community as well as to the research participants. In addition, many feminist evaluators attempt to use principles of the participatory action approach (PAR), such as involvement of participants in as many of the steps of planning and implementing an evaluation as possible. It should be noted that some evaluators do not perceive feminist evaluation as a distinct approach; instead, they view it as a stance taken by some within a broader evaluation approach.</p>

<p>Several authors have covered feminist perspectives in social work practice (e.g., Bricker-Jenkins, Hooyman, &amp; Gottlieb, 1991; Butler-Mokoro &amp; Grant, 2018; Brisolara, Seigart, &amp; SenGupta, 2014). Their practice perspectives have focused on person-centered approaches, inclusiveness, collaboration with clients, and empowerment. These qualities also tend to be highlighted by other evaluators and encourage clients to become as fully involved as possible in evaluations to benefit the clients’ welfare (e.g., Brisolara, Seigart, &amp; SenGupta, 2014).</p>

<h3 id="23-empowerment-approaches"><strong>2.3 Empowerment Approaches</strong></h3>

<p>Empowerment is a familiar action term in social work. It refers to promoting increased power for clients and equipping them to assume greater control over their lives. Empowerment approaches advocate for both client rights and client responsibilities in their lives. An empowerment philosophy can be especially important to evaluation activities. For example, it is evident in the empowerment evaluation approach of Fetterman, Kaftarian, and Wandersman (2015). The value orientation of their approach is to help people help themselves and to improve their programs using self-evaluation and self-reflection techniques. Fetterman (2003) describes a basic, logical, and systematic approach to facilitating self-evaluation. Program participants are helped to conduct their own evaluations, and they use outside evaluators and other experts as coaches. Evaluation concepts, techniques, and findings are used to help specific oppressed groups or communities develop their self-determining capacity and improve their programs. Such evaluators typically play the roles of coach, facilitator, expert consultant, and critical friend.</p>

<p>Fetterman et al. (2015) distinguish between empowering processes and empowering outcomes. Empowering processes help people develop the skills needed for them to become independent problem-solvers and decision makers (Zimmerman, 2000). These processes are critical in providing people with a stake in gaining control over their future, obtaining needed resources, and critically understanding their social environment. Macro practice approaches taught in social work programs emphasize these processes. Empowered outcomes, in some contrast, are the consequences that empowerment processes seek. Examples include creating or strengthening organizational networks, creating more accessibility to community resources, and greater citizen control over community decision-making.</p>

<p>“Inclusive evaluations,” developed by Mertens (2003), can be useful to an empowerment approach; she advocates for a deliberate inclusion of groups and communities that have been historically discriminated against on the basis of race, ethnicity, culture, gender, social class, sexual orientation, and disability. This perspective aims to redress power imbalances in society by involving these oftenoverlooked stakeholders and taking seriously and accurately their views and needs.</p>

<p>This perspective promotes social justice outcomes and takes issue with deficit models that blame those affected by a problem.</p>

<p>PAR is a practical type of empowerment model that can be used in evaluations. Sometimes referred to as participant action research or critical action research, PAR is readily familiar in social work research (DePoy, Hartman, &amp; Haslett, 1999). As its perspective, PAR has an interest in actively involving the research participants or subjects in all or most steps of the process. Some of the key PAR principles are the following:</p>

<ul>
  <li>Collaborate with those affected by the problem to clearly articulate the social problem, its scope, and all the people to consider as stakeholders.</li>
  <li>Articulate the purpose of the change that the research is designed to accomplish.</li>
  <li>Have both professional and lay researchers on the team.</li>
  <li>Train the lay researchers in how to design, conduct, and use appropriate research methods.</li>
  <li>Report findings in accessible formats for all stakeholder groups.</li>
</ul>

<p>In specific terms, implementation of PAR can occur in all or some of the Seven Evaluation Steps. For example, if involving community stakeholders in every step is not realistic, they can have a significant role in some steps. For example, client and community stakeholders can be particularly important in the first step (identify the problem to be evaluated) by helping articulate program issues that need to be addressed. Clients can also be involved in the last step (disseminate the results) by assisting in dissemination of evaluation results to various community groups and getting feedback and greater community involvement.</p>

<p><strong><em>Example of an Evaluation with PAR Principles to Support Social Action</em></strong></p>

<blockquote>
  <p>Reese, Ahern, Nair, O’Faire, and Warren (1999) initiated a program evaluation using PAR principles that resulted in improved access to hospice services for  African Americans. Collaboration occurred between the research participants and practitioners throughout the study. The evaluators’ activities began with a small qualitative study with African American pastors. This pilot study was followed by    a larger quantitative study of African American hospice patients that documented their access barriers to hospice. Finally, a social action effort was initiated to engage local health care providers in addressing these access barriers. The findings of their studies were used to facilitate this social action effort.</p>
</blockquote>

<h3 id="24-experimental-and-quasi-experimental-designs"><strong>2.4 Experimental and Quasi-Experimental Designs</strong></h3>

<p>Advocates of experimental designs believe that experimental and quasi-experimental designs are superior to other designs because they deliver scientifically credible evidence of the impact of a program on the clients’ welfare (Cook &amp; Campbell, 1979). They argue that experimental designs and the randomized samples that they use are feasible and ethical. Further, the features of these designs are the only way to rule out the influence of other factors, such as other means of helping clients and the maturity processes for participants that inevitably occurs over the time of an experiment.</p>

<p>Philosophies deriving from this traditional scientific research paradigm are reviewed only briefly in this section, as readers are expected to be well acquainted with this approach from research methods courses in professional degree programs. In addition, Chapter 9 is partially devoted to experimental and quasi-experimental designs used in program and practice outcome evaluations. If you wish to have more details about experimental and quasi-experimental designs, go to Chapter 9.</p>

<h3 id="25-fourth-generation-evaluations"><strong>2.5 Fourth-Generation Evaluations</strong></h3>

<p>Lincoln (2003) and Guba and Lincoln (1989) advocate for a “fourth-generation” philosophy for this era that strives to redress power imbalances and expand the repertoire of data-gathering and data analysis methods used in evaluations. They argue that we must move beyond the modernist and Eurocentric philosophies that tend to believe that rational, orderly investigations and deductions in the social sciences will suffice in arriving at social truth, as it has in the physical sciences. A postmodernist would argue that no single method or design can produce anything more than a partial truth. A postmodernist distrusts all methods equally and recognizes and admits the situational limitations of the knower, whether based in science, literature, the spiritual realm, or another vantage point. Lincoln views politics as a major voice of influence in her inquiries, and her perspective considers the needs of global society, not just Western society. This global perspective particularly seeks to hear and value the often-unheard indigenous voices of other societies.</p>

<p>Guba and Lincoln (1989) also advocate for the use of different types of qualitative inquiries along with the more traditional quantitative methodologies in evaluations. Their writings offer qualitative methodologies and techniques in many forms. Qualitative methods used in evaluations are covered extensively in Chapters 8 and 10 and other parts of this book. In terms of the practical aspects of evaluation, these theorists warn that evaluators who fail to pay attention to the growing number of pluralistic voices in social life will find these voices in a chorus of opposition to any evaluation with a single method of inquiry. Their perspective may be most helpful as a reminder that there is no one perfect evaluation approach and we must always be prepared to think of alternatives to what is currently in vogue.</p>

<h2 id="3-synthesis-of-these-evaluation-perspectives"><strong>3. SYNTHESIS OF THESE EVALUATION PERSPECTIVES</strong></h2>

<p>As mentioned earlier, the previously discussed five perspectives on evaluation are examples drawn from a larger pool of theoretical perspectives. At this point, it is important to ask how we can make sense out of the differences that exist among these perspectives? Also, what might they mean for a social worker wanting to be an evaluator or conduct their own evaluations? This section looks more closely at these questions. How might the specific features of each perspective be relevant in the different agency settings where you may be located? Hopefully, this section will encourage many of you to begin to craft your own evaluation approaches that may combine features of some or all of these perspectives.</p>

<h3 id="31-pattons-five-distinct-paradigms"><strong>3.1 Patton’s Five Distinct Paradigms</strong></h3>

<p>Patton (2002), an evaluator who tends to favor qualitative evaluations, offers five distinct paradigms. Each paradigm, in part, captures elements of the types of evaluations that are pertinent to social service agencies, schools, and institutions that employ human service workers. These paradigms include the traditional social science research paradigm, the social constructionist, artistic and evocative, the pragmatic and utilitarian, and critical change.</p>

<ol>
  <li>Traditional social science research paradigm: The traditional social science research paradigm emphasizes objectivity and the independence of the evaluator from the group studied, and it minimizes the possibility of investigators’ bias. It also pays close attention to validity, reliability, generalizability, and a marked preference for quantitative measurement and experimental designs. This paradigm is expanded in the work of Rossi, Freeman, and Lipsey (2003).</li>
  <li>Social construction and constructionist: This paradigm asserts the inevitability of subjectivity, uses multiple perspectives, favors qualitative inquiry, and offers alternative criteria to validity and reliability for judging methodological quality (e.g., trustworthiness, authenticity, and gathering findings that enhance and deepen understanding). Responding to multiple stakeholder perspectives is a hallmark of constructionist evaluation. This paradigm is expanded in the work of Guba and Lincoln (1989) and Greene (2000).</li>
  <li>Artistic and evocative paradigm: This paradigm, while not widely used in evaluations, uses role-playing and dramatic techniques, poetry, and other literary forms to gather and present data, as well as short stories and narrative techniques to report findings. The criteria for judging artistic and evocative evaluations include creativity, aesthetic quality, and interpretive vitality. The findings of such studies can open the program world to the evaluation audience through literary and dramatic devices and offer a deep sense of the lived experience of program participants. Eisner’s (1991) work models this paradigm.</li>
  <li>Pragmatic and utilitarian paradigm: This paradigm emphasizes the specific practical, informative needs of users of an evaluation. Criteria include responsiveness to the needs of stakeholders, situational adaptability, attending to interactive engagement between the evaluator and stakeholders, methodological flexibility, and preparation of findings that respond to stakeholders’ needs. Patton’s (2008, 2015) utilization-focused evaluations highlight this paradigm.</li>
  <li>Critical change paradigm: In this paradigm, evaluators are involved in evaluations primarily as change agents by bringing a critical change orientation and an explicit agenda to uncover political, economic, and social inequalities; to raise people’s consciousness; and to strive to change the balance of power in favor of the less powerful. A purpose of such evaluations is to increase social justice and increase the ability of the oppressed to represent their own interests through the evaluation and follow-up actions. Critical change criteria undergird empowerment evaluations (e.g., Fetterman, 2003), diversity-inclusive evaluations (e.g., Mertens, 2003), and deliberate democratic evaluations that involve values-based advocacy for democracy (e.g., House &amp; Howe, 2000).</li>
</ol>

<p>Based on these five paradigms, what are the implications for social workers who conduct evaluations? Social workers need aspects of most if not all of these paradigms. They need the objectivity and rigor of the traditional social science research paradigm that is covered extensively in later chapters.</p>

<p>The social constructionist paradigm is also useful because a growing number of concepts that are involved in social work interventions and evaluations are difficult to measure quantitatively. As a result, social work evaluators are increasingly using qualitative methods as well. Chapters 7, 8, and 10, among others, discuss some of these issues in more depth. Also, the social constructionist paradigm has relevance and flexibility in response to the concerns of multiple stakeholders, especially the program recipients.</p>

<p>Social work, a very practical profession, stresses a utilitarian perspective like the pragmatic and utilitarian paradigm, as social work evaluations must be relevant, practical, and responsive to evolving program and practice needs in social agencies. In addition, among the ethical priorities of a social work evaluator is the persistent concern for social justice; many social work evaluators will gravitate toward evaluations that focus on macro practice interventions that can bring about social change beneficial to clients, in keeping with the critical change paradigm. Social change and its role in evaluations is covered further in various chapters of the book.</p>

<p>In conclusion, the evaluation enterprise is by no means monolithic or uniform in its theoretical perspective. It has been driven by many different perspectives and can generate many different benefits for the recipients of programs and practice.</p>

<h3 id="32-formative-and-summative-evaluations"><strong>3.2 Formative and Summative Evaluations</strong></h3>

<p>Finally, an important way to distinguish evaluations is to ask whether they are formative or summative. This is the traditional way in which evaluations are distinguished in many program evaluation texts. Formative evaluations focus on planning for a program and improving its delivery of services. Program processes are important to examine, correct, and enhance because of the direct impact they have on the outcomes for program recipients. These evaluations tend to be exploratory and utilitarian in that their findings can be immediately channeled back into enhancing a program, solving a problem, or filling an obvious omission in a program. The agency sponsoring the program usually initiates and conducts such evaluations; the agency staff or outside consultants chosen by the agency drive the evaluation.</p>

<p>Summative evaluations focus on the outcomes of programs and attempt to answer the ultimate question: Did the intervention reach its goals or make a real difference in the lives of recipients? These evaluations have a finality to them in that they attempt to measure whether a program was effective. An external agent, such as an independent evaluator or governmental agency, typically conducts such evaluations to ensure that they are objective. They primarily use a narrower set of research design options, such as experimental and single system. The results of these studies can be decisive in determining whether a program continues, expands, or terminates. Funding and regulatory agencies are most interested in summative evaluations because they provide the kinds of information that they need to make major decisions about future funding.</p>

<p>These two distinct types of evaluations, formative and summative, are somewhat parallel to the distinction in research between exploratory and explanatory studies. Exploratory studies are conducted to learn more about a phenomenon when not much is currently known. Exploratory studies mostly identify general research questions to answer. Explanatory studies are conducted to find support for theoretical explanations that have already had some confirmation in previous studies. Formative evaluations are like exploratory studies in that they are exploratory in nature and not intended to provide results that can help make major decisions about a program. Summative evaluations are like explanatory studies in that they are used to bring finality to a program’s future; either it did or it did not have a significant impact on recipients (see Table 2.2).</p>

<p>Yet, these two types of evaluations, formative and summative, are helpful only to a limited extent. As mentioned in Chapter 1, most evaluations conducted by social workers will be formative. Summative evaluations are less likely to be initiated by a social agency. Nevertheless, the purposes of summative evaluations are important to understand, as they have a vital role in decisions about a program’s future. Research designs used in summative evaluations are a focus of Chapter 9 and are illustrated in other parts of the book.</p>

<h3 id="33-evidence-based-interventions"><strong>3.3 Evidence-Based Interventions</strong></h3>

<p>Evidence-based interventions are not a philosophy or theoretical perspective; they are a pledge of assurance that programs and practice interventions are effective and ultimately accountable for what our clients need. Regardless of the specific perspective that an evaluator selects to conduct an evaluation, it should attempt to be as evidence-based as possible. Evidence-based interventions have been defined in medicine as the integration of the best research evidence with clinical expertise and patient values (Straus, Glasziou, Richardson, &amp; Haynes, 2011). Furthermore, Gambrill (1999), a social worker, states that “it involves integrating individual practice expertise with the best available external evidence from systematic research as well as considering the values and expectations of clients” (p. 346). Based on these definitions, evidence-based interventions are defined in this book as interventions that use the best available external evidence that the intervention is effective. Evidence comes mostly from practice experience and research studies with quasi experimental or experimental designs. It is also important that evidence-based sources are consistent with the values and expectations of the clients who receive such interventions.</p>

<p>The words evidence and evidence-based are often used throughout this book. Evidence is something observed first-hand with some or all the senses (sight, hearing, taste, touch, smell). Evidence documenting the effectiveness of a program or practice intervention needs to be empirical or reflect some form of reality in the world. When evidence of a phenomenon is not directly observable, then a secondhand source needs to be found that is reliable and valid. Agency case records and questionnaires responses are examples of second-hand sources of evidence.</p>

<p>Evidence can take many forms and can be used with varying degrees of confidence. The best evidence is based on evaluation studies using the scientific method. Yet, even the results they produce provide evidence at differing levels of confidence. For example, a pre/posttest design only measures how a client functions before and after an intervention, which is much weaker evidence than a pre/posttest experiment design using a control group or a comparison group.</p>

<p>Another important consideration about evidence pertains to who the clients are that benefited from the intervention. Evidence may be available indicating that a specific intervention works with a group of predominantly white mothers in a suburban area of a large city, but we must be cautious not to necessarily assume that this evidence is also relevant to mothers of different ethnic groups or mothers living in large cities, small towns, or rural environments. An evaluation determines whether or not an intervention is effective with a specific group that receives the services. They could be predominately African American, Euro-American, or Mexican, high income or poor. Whatever the characteristics of the clients, they need to be known because we can probably assume that the evidence is more likely applicable to other people similar to this group than people with different characteristics or circumstances. Generalizing from one group to another should involve caution in this respect.</p>

<h3 id="34-evidence-based-interventions-with-cancer-and-palliative-care-patients"><strong>3.4 Evidence-Based Interventions with Cancer and Palliative Care Patients</strong></h3>

<p>Harding and Higginson (2003) systematically reviewed articles from several relevant professional journals to identify interventions for helping noninstitutionalized cancer and palliative care patients and their caregivers. They found twenty-two relevant interventions. Overall, they concluded from their review that one-to-one interventions are a means of providing effective support, education, and building problem-solving and coping skills. However, they are time-consuming and costly and may be unacceptable to many caregivers. Group work interventions, in contrast, were also reported to be widely effective for support and education for both caregivers and patients.</p>

<p>The challenges of obtaining empirical evidence of a phenomenon can be illustrated in an outcome study in which the frequency of positive contact that occurs between a client with mental illness and other people can be an outcome measure of successful social adaptation. How can this be determined empirically? It can be observed first-hand if someone were available to observe the interactions of a client all day long without interfering with the contacts in any way. However, this may not be a realistic or ethically sound method of obtaining such information. A second-hand source of such information, such as a relative, friend, or the self reporting of the client, is another possible way to count contacts, albeit subjective and likely inaccurate. Second-hand sources can be inaccurate, so the challenge is to make sure that such reports are as accurate as possible. One way to increase this possibility is to obtain multiple sources, such as reports from two or more people and compare them.</p>

<h3 id="35-a-unique-international-source-of-evidence-based-information"><strong>3.5 A Unique International Source of Evidence-Based Information</strong></h3>

<p>The Collaboration or C2 <www.campbellcollaboration.org> is an international source of evidence-based information for programs and practice approaches. Its offices are in several international cities. It may be beneficial to become familiar with the resources that they provide. It is “an independent, international, non-profit organization that provides decision-makers with evidence-based information to empower them to make well-informed decisions about the effects of interventions in the social, behavioral and educational arenas.” C2 has a strategic network of renowned scholars and practitioners worldwide that can provide evidence-based information in two ways: (a) by preparing, maintaining, and disseminating systematic reviews of existing social science evidence, and (b) by random controlled trials using their own databases. C2’s vision is noble as it promotes positive social change and can improve programs and services across the world.</www.campbellcollaboration.org></p>

<p>“Best practices” has been a frequently used term in social agencies and is also relevant to the discussion of evidenced-based interventions. Best practices are the “best” or most effective programs or practices known to be effective in helping people overcome their problems. For this reason, evidence-based practices, in many ways, are strongly correlated with best practices. For example, best practices could be the best evidence-based practices known to exist for a specific client group.</p>

<h2 id="4-key-perspectives-for-the-book"><strong>4. KEY PERSPECTIVES FOR THE BOOK</strong></h2>

<p>This review of several types of perspectives and approaches in conducting evaluations reveals some common and overlapping elements. Those that are most compatible with the evaluation perspectives used by the author include the following:</p>

<ul>
  <li>Importance of science, neutrality, and objective methods;</li>
  <li>Use of critical-thinking skills;</li>
  <li>Recognition that achieving socially acceptable outcomes is an essential program expectation;</li>
  <li>Recognition that the purposes of evaluations can vary widely and dictate what is to be done;</li>
  <li>Importance of working closely with stakeholders and being accountable to them;</li>
  <li>Following a professional ethical code;</li>
  <li>Flexibility in the use of various methodologies, including considering the use of qualitative methods when they can be helpful;</li>
  <li>Affirmation and promotion of diversity; and</li>
  <li>Commitment to social and economic justice.</li>
</ul>

<h2 id="5-three-stage-approach"><strong>5. THREE-STAGE APPROACH</strong></h2>

<p>The book is organized around the three developmental stages that programs and practice interventions go through; they are planning, implementation, and outcome. This three-stage approach was introduced in Chapter 1 in the discussion about the logic model and is central to the book because evaluations address important issues at all three stages.</p>

<p>When discussing evaluation perspectives and approaches, it is important to remember that a program or a practice intervention is the primary focus. Furthermore, within programs and practice interventions, evaluations give specific attention to a variety of entities called elements. Let’s look at some of the elements of</p>

<p>program and practice interventions that need to be considered. They include input elements, process elements, and outcome elements. (See Figure 2.1.)</p>

<p><img src="https://i.imgur.com/muvMZNi.png" alt="Imgur" /></p>

<p><strong><em>Program Elements</em></strong></p>

<blockquote>
  <p>Important elements of a program are pertinent to an evaluation and may need to be identified at each of the three stages. Input elements are the focus during the planning stage of a program. Input elements can include identifying the potential group of clients to be served, the program intervention to be used, any technologies to be considered, and the various types of staff that need to be hired. During the implementation stage, the elements are referred to as process elements. Examples of process elements include ongoing monitoring of how the intervention is being implemented, and ongoing training provisions for the staff. The elements to be considered during the outcome stage are referred to as outcome elements. They could include, for example, data on the extent to which clients’ reach their goals after completing the program.</p>
</blockquote>

<p><strong><em>Practice Elements</em></strong></p>

<blockquote>
  <p>Let’s look more closely at how these three stages are evident in practice interventions. At the practice level, an evaluation could investigate the input elements before the practice intervention is implemented. Some important input elements in practice are similar to those for programs. They can include identifying the potential group of clients to be served and choosing the practice interventions to be used. The credentials that practitioners will need to have to qualify for their positions, such as a professional degree, work experience, and other special skills are also input elements.</p>
</blockquote>

<p>During the implementation of a practice intervention, important process elements can be important to evaluate including monitoring how the practice intervention is being carried out or implemented. Finally, the outcome stage considers how the intervention has impacted the clients. Outcome elements are primarily measures of client progress or lack of it.</p>

<h3 id="51-client-satisfaction-with-various-practice-approaches"><strong>5.1 Client Satisfaction with Various Practice Approaches</strong></h3>

<p>Client satisfaction studies are sometimes used to evaluate how well a practice approach is being implemented based on the clients’ perceptions. A client satisfaction study by Baker, Zucker, and Gross (1998) sought, among other things, to determine the perceptions of five groups of inpatient mental health clients who were receiving different treatment modalities and services in five different facilities. The evaluator presumed that the clients with severe mental illness “know what’s good for themselves.” Practice approaches varied across facilities. For example, one facility offered extensive assistance with daily activities, while another offered training groups in psychosocial skills, and a third emphasized case management, family services, and individual counseling. The results revealed that client satisfaction was not found to be significantly different among the five facilities, although some approaches that were used received low ratings.</p>

<h2 id="6-summary"><strong>6. SUMMARY</strong></h2>

<p>The history of both program and practice evaluations are highlighted in the chapter and provide a fuller understanding of why evaluations are so important and why they take the forms that they do. Five theoretical approaches are described to illustrate some of the varied ways that evaluators’ view the evaluation process. These five approaches include results-oriented approaches, feminist approaches, empowerment approaches, use of experimental and quasi-experimental designs, and fourth-generation evaluations. In addition, nine principles are identified that reflect the author’s preferences for how evaluations should be conducted. They comprise</p>

<p>(a) the importance of science, neutrality, and objective methods;</p>

<p>(b) use of critical-thinking skills;</p>

<p>(c) recognition that achieving socially acceptable outcomes is an essential program expectation;</p>

<p>(d)recognition that the purposes of evaluations can vary widely and dictate what is to be done;</p>

<p>(e) the importance of working closely with stakeholders and being accountable to them;</p>

<p>(f) following a professional ethical code;</p>

<p>(g) flexibility in the use of various methodologies, including considering the use of qualitative ones;</p>

<p>(h) affirmation and promotion of diversity;</p>

<p>(i) a commitment to social and economic justice.</p>

<p>Evidence-based programs and practice interventions are also defined and elaborated on in the chapter and their importance is emphasized. Evaluations are described as the primary source in determining whether programs and practice interventions are evidence-based. The three-stage approach is also introduced further. This organizational framework is helpful in informing you about each of the three stages of program and practice development and the wide range of possible ways to focus an evaluation.</p>

<h2 id="discussion-questions-and-assignments"><strong>DISCUSSION QUESTIONS AND ASSIGNMENTS</strong></h2>

<ul>
  <li>President Johnson’s Great Society and War on Poverty, which were instrumental in creating programs such as local Community Action Councils, Job Corps, Neighborhood Youth Corp, Community Legal Services, and Head Start.</li>
  <li>The 1962 Social Security amendments, which established initiatives such as services for welfare recipients, rehabilitation, and training for useful work to overcome prolonged dependency.</li>
  <li>The passage of the Government Performance and Results Act of 1993, which requires all governmental agencies to focus on client outcomes and define how they are to be achieved.</li>
  <li>Joel Fischer’s (1973) article “Is Casework Effective? A Review.”</li>
</ul>

<ol>
  <li>In small groups, choose one of the historical events listed next, review what is summarized about the event in the chapter, and find an additional source of information related to evaluations. Discuss within your small groups what you learned about the event and its importance in shaping evaluations and then share what you have learned with the class.</li>
  <li>
    <p>Select one of the five theoretical evaluation approaches. Find out more about this approach by reading about it in this and other texts and then find a report of an evaluation in a professional journal that is an example of this approach.</p>
  </li>
  <li>
    <p>What is your view of the role of empowerment in evaluation? How can an evaluator promote empowerment through an evaluation? What are your views about the limits of empowerment in evaluations, if any?</p>
  </li>
  <li>Briefly describe a program evaluation that could be conducted at your agency. What is the overall purpose of the evaluation? What could be accomplished by conducting this evaluation? Which perspectives described in the chapter would have the most impact on how you would conduct this evaluation? Explain why?</li>
  <li>Why is it important to know the theoretical perspective of an evaluator? Pick one of the five approaches described in the chapter and discuss how it can affect, both positively and negatively, how an evaluator conducts a study.</li>
</ol>

<h2 id="references"><strong>REFERENCES</strong></h2>

<ul>
  <li>Baker, L., Zucker, P., &amp; Gross, M. (1998). Using client satisfaction surveys to evaluate and improve services in locked and unlocked adult inpatient facilities. Journal of Behavioral Health Services Research, 25(1), 51–68.</li>
  <li>Bloom, M., Fischer, J., &amp; Orme, J. G. (2009). Evaluating practice: Guidelines for the accountable professional (6th ed.). Boston, MA: Allyn &amp; Bacon.</li>
  <li>Briar-Lawson, K., &amp; Zlotnik, J. L. (Eds.). (2002). Evaluation research  in child welfare: Improving outcomes through university-public agency partnerships. Binghamton, NY: Haworth Press.</li>
  <li>Bricker-Jenkins, M., Hooyman, N. R., &amp; Gottlieb, N. (Eds.). (1991). Feminist social work practice in clinical settings. Thousand Oaks, CA: SAGE.</li>
  <li>Brisolara, S., Seigart, D., &amp; SenGupta, S. (Eds.). (2014). Feminist evaluation and research: Theory and practice. New York, NY: Guilford Press.</li>
  <li>Butler-Mokoro, S., &amp; Grant, L. (Eds.). (2018). Feminist perspectives on social work practice: The intersecting lives of women in the 21st by century. New York, NY: Oxford Press.</li>
  <li>Cook, T. D., &amp; Campbell, D. T. (1979). Quasi-experimentation: Design and analysis issues for field settings. Skokie, IL: Rand McNally.</li>
  <li>Council on Social Work Education. (2015). Educational policy and accreditation standards. Retrieved from <a href="https://www.cswe.org/getattachment/Accreditation/Accreditation-">https://www.cswe.org/getattachment/Accreditation/Accreditation-</a> Process/2015-EPAS/2015EPAS_Web_FINAL.pdf.aspx</li>
  <li>Deem, R. (2002). Talking to manager-academics: Methodological dilemmas and feminist research strategies. Sociology, 36(4), 835–856.</li>
  <li>DePoy, E., Hartman, A., &amp; Haslett, D. (1999). Critical action research: A model for social work knowing. Social Work, 44(6), 560–569.</li>
  <li>Eisner, E. (1991). The enlightened eye. New York, NY: Macmillan.</li>
  <li>Fetterman, D. (2003). Empowerment evaluation strikes a responsive chord. In S. I. Donaldson &amp; M. Scriven (Eds.), Evaluating social programs and problems: Visions for the new millennium (pp. 63–76). Mahwah, NJ: Erlbaum.</li>
  <li>Fetterman, D. M., Kaftarian, S. J., &amp; Wandersman, A. (Eds.). (2015). Empowerment evaluation: Knowledge and tools for self-assessment and accountability (2nd ed.). Thousand Oaks, CA: SAGE.</li>
  <li>Fischer, J. (1973). Is casework effective? A review. Social Work, 18(1), 5–20.</li>
  <li>Gambrill, E. (1999). Evidence-based practice: An alternative to authority-based practice.</li>
  <li>Families in Society, 80(4), 341–350.</li>
  <li>Gambrill E., &amp; Gibbs, L. (2017). Critical thinking for helping professionals: A skills-based workbook (4th ed.). Thousand Oaks, CA: Pine Forge Press.</li>
  <li>Gibbs, L. (2003). Evidence-based practice for the helping professions: A practical guide with integrated multimedia. Pacific Grove, CA: Thomson.</li>
  <li>Gibelman, M. (2003). So how far have we come? Pestilent and persistent gender gap in pay. Social Work, 48(1), 22–32.</li>
  <li>Glicken, M. (2005). Improving the effectiveness of the helping professions: An evidence-based approach to practice. Thousand Oaks, CA: SAGE.</li>
  <li>Greene, J. C. (2000). Understanding social programs through evaluation. In N. K. Denzin &amp;</li>
  <li>Y. S. Lincoln (Eds.), Handbook of qualitative research (2nd ed., pp. 981–999). Thousand Oaks, CA: SAGE.</li>
  <li>Guba, E., &amp; Lincoln, Y. (1989). Fourth generation evaluation. Thousand Oaks, CA: SAGE.</li>
  <li>Harding, R., &amp; Higginson, I. J. (2003). What is the best way to help caregivers in cancer and palliative care? A systematic literature review of interventions and their effectiveness. Palliative Medicine, 17, 63–74.</li>
  <li>House, E. R., &amp; Howe, K. R. (2000). Deliberative democratic evaluation. New Directions for Evaluation, 85, 3–12.</li>
  <li>Kautz, J. R., Netting, F. E., Huber, R., Borders, K., &amp; Davis, T. S. (1997). The Government Performance and Results Act of 1993: Implications for social work practice. Social Work, 42(4), 364–373.</li>
  <li>Lincoln, Y. S. (2003). Fourth generation evaluation in the new millennium. In S. I. Donaldson &amp; M. Scriven (Eds.), Evaluating social programs and problems: Visions for the new millennium (pp. 77–90). Mahwah, NJ: Erlbaum.</li>
  <li>Martin, L. L., &amp; Kettner, P. M. (1996). Measuring the performance of human service programs. Thousand Oaks, CA: SAGE.</li>
  <li>Mertens, D. M. (2003). The inclusive view of evaluation: Visions for the new millennium. In S. I. Donaldson &amp; M. Scriven (Eds.), Evaluating social programs and problems: Visions for the new millennium (pp. 91–107). Mahwah, NJ: Erlbaum.</li>
  <li>O’Hare, T. (2005). Evidence-based practices for social workers: An interdisciplinary approach. Chicago, IL: Lyceum Books.</li>
  <li>O’Hare, T. (2015). Evidence-based practices for social workers: An interdisciplinary approach (2nd ed.). Chicago, IL: Lyceum Books.</li>
  <li>Patton, M. Q. (2002). Feminist, yes, but is it evaluation? New Directions for Evaluation, 96, 97–108.</li>
  <li>Patton, M. Q. (2008). Utilization-focused evaluation (4th ed.). Thousand Oaks, CA: SAGE.</li>
  <li>Patton, M. Q. (2015). Qualitative research &amp; evaluation methods: Integrating theory and practice (4th ed.) Thousand Oaks, CA: SAGE.</li>
  <li>Reese, D., Ahern, R., Nair, S., O’Faire, J., &amp; Warren, C. (1999). Hospice access and use by African Americans: Addressing cultural and institutional barriers through participatory action research. Social Work, 44(6), 549–559.</li>
  <li>Rossi, P., Freeman, H., &amp; Lipsey, M. (2003). Evaluation: A systematic approach (7th ed.). Thousand Oaks, CA: SAGE.</li>
  <li>Skeggs, B. (2001). Feminist ethnography. In P. Atkinson, A. Coffey, &amp; S. Delamont (Eds.), Encyclopedia of ethnography (pp. 426–442). London, England: SAGE.</li>
  <li>Straus, S. E., Glasziou, P., Richardson, W. S., &amp; Haynes, R. B., (2011). Evidence-based medicine: How to practice and teach E. M. (4th ed.). New York, NY: Elsevier.</li>
  <li>Stufflebean, D. L. (2001). Evaluation models. New Directions for Evaluation, 89, 7–98.</li>
  <li>Wholey, J. S. (1999). Quality control: Assessing the accuracy and usefulness of performance measurement systems. In H. P. Hatry (Ed.), Performance measurement: Getting results (pp. 217–239). Washington, DC: Urban Institute.</li>
  <li>Zimmerman, M. A. (2000). Empowerment theory: Psychological, organizational, and community levels of analysis. In J. Rappaport &amp; E. Seldman (Eds.), Handbook of community psychology (pp. 43–63). New York, NY: Plenum.</li>
</ul>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
