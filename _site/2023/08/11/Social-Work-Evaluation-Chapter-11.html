<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chapter 11 Preparing and Disseminating a Report of Findings | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Chapter 11 Preparing and Disseminating a Report of Findings" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CHAPTER 11 Preparing and Disseminating a Report of Findings" />
<meta property="og:description" content="CHAPTER 11 Preparing and Disseminating a Report of Findings" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/11/Social-Work-Evaluation-Chapter-11.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/11/Social-Work-Evaluation-Chapter-11.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-11T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Chapter 11 Preparing and Disseminating a Report of Findings" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-11T00:00:00+09:00","datePublished":"2023-08-11T00:00:00+09:00","description":"CHAPTER 11 Preparing and Disseminating a Report of Findings","headline":"Chapter 11 Preparing and Disseminating a Report of Findings","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/11/Social-Work-Evaluation-Chapter-11.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/11/Social-Work-Evaluation-Chapter-11.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=97b06ba2048c98509e06ce5ad3ed82faa3ff7a7f">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>11 August 2023</small>
<h1>Chapter 11 Preparing and Disseminating a Report of Findings</h1>

<p class="view">by JAMES R. DUDLEY</p>

<h1 id="chapter-11-preparing-and-disseminating-a-report-of-findings">CHAPTER 11 Preparing and Disseminating a Report of Findings</h1>

<p><em>How do we prepare a report that is useful and optimizes the possibilities of positive changes for clients?</em></p>

<p>This chapter explores the final two steps of the Seven Evaluation Steps: step 6 (preparing a report of the findings and recommendations) and step 7 (disseminating the report to others). Once the data are gathered from an evaluation and analyzed, preparing a report naturally follows. Several different types of reports are possible, depending on the needs of various stakeholders. A report can be communicated orally, in written form, or both; comprehensively or in summary form. The formatting options are almost unlimited. They can be a technical research report, a formal presentation at a public meeting, a set of topics and questions covered at a staff workshop, a series of informal discussions, a brief written summary, a para- graph for state legislators, a manuscript, a press release, or other formats.</p>

<p>Once the report is completed, the findings are expected to be disseminated to all stakeholders and interested parties. This dissemination, of course, should occur in the most useful form possible. Ideally, findings are disseminated to several different types of stakeholders; some are obvious, such as funding agencies and agency administrators overseeing the program. Other important stakeholders can be easily overlooked or forgotten, such as clients, family members, and pertinent community groups.</p>

<p>Although different strategies are available to disseminate findings, they largely depend on the format chosen for the report. In other words, the two steps of preparing a report and disseminating findings are largely intertwined. The decision about a report’s format determines how and even if it will be useful to some stakeholders. For example, a lengthy technical research report may be useful to a funding agency or a professional journal but not to other stakeholders, such as clients and their families. For this reason, it is best to begin by exploring the needs of each stakeholder, finding out what kinds of information and what format will be most helpful to each of them, and proceeding to select the most useful reports and means of distributing the findings.</p>

<h2 id="considering-the-input-of-stakeholders">CONSIDERING THE INPUT OF STAKEHOLDERS</h2>

<p>Like all the previous steps of an evaluation, it is very important to spend an adequate amount of time thinking about and planning these final steps. Unfortunately, a poorly thought-out and written report can easily negate all the hard work up to this point. Likewise, neglecting to give enough time and serious thought to how to disseminate findings could deter what an evaluation was intended to achieve.</p>

<p>The best way to begin these two final steps is to consider who the stakeholders and other potential readers of the report will be (e.g., funding agencies, board members, advisory committee members, administrators, program service providers, legislators, current or prospective clients, community groups, advocacy groups, professionals, the public). Some stakeholders may be designated as primary and others secondary, which can help prioritize where more of the time and energy should be devoted to dissemination (Bamberger, Rugh, &amp; Mabry, 2019; Linfield, &amp; Posavac, 2019; Morris, Fitz-Gibbon, &amp; Freeman, 1987; York, 2009). The evaluation sponsor and others who requested the evaluation are examples of primary stakeholders, as are a funding agency, the agency’s board of directors, a chief executive officer, program directors, other key staff members, and any other decision makers. Secondary stakeholders are more likely community groups, other staff members, clients, other organizations interested in the program, the public, and legislators. Although the latter groups may be considered secondary, they should not be overlooked.</p>

<p>Each group is likely to have different needs related to a report, and the differences are usually reflected in how easy it is to understand the material and how relevant and useful the information is. Timing may also be important for stakeholders, as timing may be crucial to a decision-making timetable. Therefore, it is likely that different types of reports and strategies for dissemination are needed, especially to address a diverse audience of stakeholders.</p>

<p>It is usually wise to ask stakeholders several types of questions before you begin to write a report. Morris et al. (1987) suggest the following questions:</p>

<ul>
  <li>What do you want to know about the evaluation and its results and why? What expectations do you have for a report?</li>
  <li>Will you likely answer any important questions or make any decisions about policies, programs, or practice approaches based on the report’s findings? If so, what are these questions?</li>
  <li>Are there any acceptable criteria that you have adopted to claim success or lack of it? For example, do you expect a minimum amount of increase in client scores from pretest to posttest or a minimally acceptable percentage of clients who are satisfied with services?</li>
  <li>Are there any ethical or political considerations that need to be carefully thought out before preparing the report? For example, are there controversial issues that need to be addressed cautiously and in a delicately balanced way? Is there any potential resistance to some of the findings that could come from specific groups?</li>
  <li>Should the findings of the study be made available to clients? If so, in what form?</li>
  <li>How can the clients benefit from the findings? What can be done to maximize client benefits?</li>
</ul>

<h2 id="format-of-the-report">FORMAT OF THE REPORT</h2>

<p>Several communication formats are possible for sharing evaluation findings with stakeholders, including several possibilities in written form, such as a technical research report, an executive summary, a manuscript for a professional journal, a special report for clients, an agency brochure, a press release, or a one-page flyer or memo (Linfield, &amp; Posavac, 2019; Morris, Fitz-Gibbon, &amp; Freeman, 1987; York, 2009). Oral report options could include a curriculum for a staff workshop, a presentation before a public or community meeting, a poster session at a conference, personal discussions with some stakeholders, or a discussion that social workers can have with their clients in which they share some of the findings with them.</p>

<h3 id="technical-reports">Technical Reports</h3>

<p>A technical report tends to be formal and lengthy. It is a comprehensive report that documents all the steps in the evaluation process. It is the format that is most helpful for assessing whether the evaluation methods used are based on sound scientific principles, and it is the best format for demonstrating evidence-based research. A technical report uses a format like that of a formal research report. An outline of a typical technical report of a research study is likely to have all the following items:</p>

<ul>
  <li>An abstract that summarizes the purpose of the study, the methods and sampling approach used, findings, and recommendations</li>
  <li>Background information on the research problem or topic, including a literature review, relevant viewpoints, and theories that help explain the topic</li>
  <li>The purposes of the study, including research questions and/or hypotheses</li>
  <li>A description of the characteristics of the sample and sampling approach used</li>
  <li>Description of the informed consent protocol</li>
  <li>Description of the data collection approach and specific measurement instruments</li>
  <li>A data analysis plan</li>
  <li>Findings, which can be an overall summary, highlights of the most important findings in tables and graphs, and results of data analysis</li>
  <li>Recommendations for how to use findings</li>
  <li>Limitations of the study (e.g., important questions that were not asked, poorly constructed questions, small sample size, an availability sampling approach, no baseline data collected, important uncontrolled extraneous variables)</li>
</ul>

<p>Technical reports are usually most appropriate for summative evaluations. This type of report is often what funding agencies, program administrators, and others need because they wish to review the scientific aspects of an evaluation. A technical report can also become a master copy of the evaluation processes and results that can be reduced and modified to fit the needs of other smaller reports.</p>

<h3 id="executive-summaries">Executive Summaries</h3>

<p>An executive summary is short, often no more than one page. It usually contains a summary of a sentence or two for each section of a larger report. For example, an executive summary developed for a technical research report would summarize background information on the program concern, the purpose of the study, the research design, the findings, recommendations, limitations, and other implications. The summary is sometimes referred to as an abstract, particularly for journal submissions.</p>

<p>Executive summaries usually precede a technical report and help readers become familiar with the contents of the technical report and whether it is worthwhile to read the full report or only sections. Executive summaries are useful for any stakeholders who may be interested in reading the technical report that it summarizes.</p>

<h3 id="journal-publications">Journal Publications</h3>

<p>Many times, an evaluation contributes something new in policy or practice that may be a breakthrough worthy of dissemination to a national or even international audience. Perhaps the program model that is evaluated is worth sharing with policy makers or other providers because it addresses a common or especially challenging problem area, or findings from the evaluation may be new evidence-based knowledge that a specific program model or practice theory has been effective in helping a specific client population. There are several excellent evaluation journals that publish manuscripts addressing these and other policy and practice breakthroughs (see the list of journals in Chapter 6).</p>

<h3 id="reports-to-clients">Reports to Clients</h3>

<p>Seldom do clients hear about the results of an evaluation relevant to them. Often such reports can be shared, but they must fit the clients’ specific needs and be presented in readily understandable language. A possibility is to share just one set of findings from an evaluation. For example, a university conducted a study on binge drinking to document the extent to which binge drinking was a problem among students; it found that more than 95 percent of student participants indicated that they did not binge drink. The university sponsor thought that this finding could encourage students to avoid binge drinking, so signs stating this finding were posted at various locations on campus. The finding was part of a larger effort by the university’s dean of student services to overcome the misperception that binge drinking was becoming a pattern among students.</p>

<p>Similarly, a finding on the success rate of clients completing a program could be publicized openly in an agency. For example, an evaluation might highlight the high rate of success of graduates of a program on key client outcome measures such as the following:</p>

<ul>
  <li>Finding a job after participating in a job-training program</li>
  <li>Remaining in recovery after completing a residential program for substance abusers</li>
  <li>Lower divorce rate among graduates of a marital enrichment program than those that did not graduate</li>
  <li>An increase in self-reported satisfaction of participants’ parental role after attending a series of parent-training sessions</li>
</ul>

<p>There are other creative ways to inform clients about findings. A simple flyer available at the front door of an agency or posted on a community bulletin board can highlight findings as well. These highlights should be prepared so that they are easily read and understood. Creativity is of utmost importance in preparing these reports, with graphics, photos, and personal testimonies among the resources to consider. It might be a good idea to attach a tear-off sheet to flyers for clients to give feedback or to make suggestions for follow-up actions.</p>

<p>Another way to share evaluation findings with clients is in the context of the worker-client relationship. In many instances, findings from an evaluation and the program or practice concerns that created the need for the evaluation can be introduced into conversations with clients. Discrete findings that directly relate to what a client is working on can be quite helpful if timed well. Client satisfaction evaluations are a good illustration of this. The findings of a client satisfaction study can be introduced in a conversation between a social worker and client as follows:</p>

<blockquote>
  <ul>
    <li><em>Worker</em>: We just completed our survey of client satisfaction. Do you remember filling it out?</li>
    <li><em>Client</em>: Yes, I did, but I am not expecting anything to come of it. These things are usually done to benefit the big people like the director of the agency.</li>
    <li><em>Worker</em>: Well, one finding revealed that a large percentage of the clients were not very satisfied with our group therapy program. They indicated that the topics that are covered and the discussions didn’t really help clients solve the problems that they came here to solve in the first place.</li>
    <li><em>Client</em>: I could have told you that a long time ago. You didn’t need a study to find that out. We [clients] often talk among ourselves about how we hate that group and wish we didn’t have to attend it.</li>
    <li><em>Worker</em>: Do you want to know more about what we found were the reasons for dissatisfaction with the group therapy program? I have them here . . .</li>
  </ul>
</blockquote>

<p>In these conversations, the findings should touch on issues of relevance to clients; in addition, an opportunity can be introduced for the client to react to the findings and to add his or her own interpretations and thoughts. Also, it is important to reassure clients that the agency is committed to doing something about client dissatisfaction because of the importance of their viewpoints. Some initial recommendations of the study could be mentioned in such conversations as well, with feedback sought from the clients’ perspectives. The feedback sessions should be documented and reported back to the evaluator and other stakeholders as a next step in resolving the identified problems.</p>

<h3 id="agency-public-relations-reports">Agency Public Relations Reports</h3>

<p>The findings of a program evaluation can be a major news item for an agency when preparing its annual report or creating a brochure to inform other agencies and prospective clients about programs. Evaluations with positive findings are obvious materials for public relations promotions. A public child welfare program, for example, needed more positive press to highlight its success in finding adoptions for children. So, it used some key findings from an evaluation and included them in its annual report. The presentation, however, was more like newspaper headlines than a technical report. With minor changes, it read: “There has been an INCREASE in the number of foster children being adopted each year. From 895 in 2010 to 1,379 in 2014. . . . In 2012, the number of children in foster care has DROPPED for the FIRST TIME since 2000 to below 10,000 children.”</p>

<h3 id="staff-workshops">Staff Workshops</h3>

<p>Staff workshops and other types of staff gatherings are excellent venues for discussing most evaluation findings. Staff workshops are mentioned here because they usually focus on training, skills development, and information sharing. Staff members are key stakeholders who have direct contact with clients and firsthand exposure to their problems; they may also be the best-informed source on the functioning of programs and practice.</p>

<p>Staff members are among the most important stakeholders of evaluations of programs and practice interventions, whether they are recognized as stakeholders or not. Problems can easily arise if they are excluded from participating or from having a significant voice in an evaluation that affects them. It is quite possible that their exclusion will result in unnecessary complications, misunderstandings, and conflicts with agency administrators and the evaluators. Because staff cooperation is essential for most evaluations to occur optimally, evaluators will want these relationships to be cooperative and collaborative.</p>

<p>The findings of most evaluations are relevant for staff members to hear about and discuss. During the planning stage for a new program, the findings of a needs assessment can provide a range of information about prospective clients for the program. Staff members need this information for several reasons, such as to identify and recruit clients and assess their suitability for a program. During the implementation stage of a program, staff workshops could be used to discuss the findings of a staff morale study or a staff satisfaction study. It would also be useful for staff to become informed about findings on program accessibility, as they will need to implement many of the recommended strategies for reaching underserved groups.</p>

<p>Client satisfaction studies are directly relevant to staff and can provide helpful feedback on the interventions that need improvement. Program quality evaluations, as described in Chapter 4, are designed to involve staff members as reviewers and peer evaluators. Evaluations that monitor their practice and involvement in programs are directly pertinent to them and can give them feedback on their interventions. Finally, outcome evaluations inform staff about whether their interventions were effective in helping clients. One evaluation that attempted to articulate a new model of practice illustrates how teams of social workers can become involved by responding to the findings of the evaluation (Gardner, 2000).</p>

<h3 id="presentations-to-the-public">Presentations to the Public</h3>

<p>In many instances, the findings of an evaluation should be communicated to the public and to those who cannot be easily identified for an invitation. Also, the community surrounding agencies may have a special stake in an evaluation’s findings. Community residents could be invited to hear about an agency’s plans. Examples in when this might be important include an agency that provides group homes in the community to people with mental retardation, and a Head Start program located in a local church or settlement house.</p>

<p>Sometimes public meetings are open to all and advertised as such; other times the meetings can be by invitation and a range of people may be invited to attend, such as civic leaders, volunteers, staff representatives of other agencies, local officials, and other active citizens. The format of the meeting can range from a formal presentation to an open discussion, but it is most important that the report of findings be prepared in a form that is pertinent and understandable to participants. These meetings could be opportune times to seek feedback from the participants on recommendations before they are implemented.</p>

<p>Testifying at public hearings is another outlet for sharing evaluation findings. Sometimes the findings of an evaluation can document, in the public spotlight, the progress or lack of it for clients. For example, evaluation findings may support offenders in the criminal justice system that are on probation or parole and need verification of their progress or achievements. Data can be provided on the progress of individual clients as court testimony for their early release for good behavior. Evaluation findings can also be valuable for class action suits on behalf of groups of clients who need documentation that they are improving in their behavior from designated program interventions. An example of this kind of documentation is provided in a longitudinal evaluation by Dudley, Calhoun, and Ahlgrim-Delzell (2002). Evaluation findings can also be publicized through a newspaper column or series, a radio talk show, or a television program. Stories of individual clients can be the most effective way to portray to the public the plight of specific types of people and the possibilities of their overcoming such obstacles. A qualitative evaluation is typically a descriptive account of the lives of a group of clients and makes an excellent source for such a story. Such evaluations can illustrate how a group of clients learned to cope with their problems or were helped by a program to become more independent and satisfied with their lives. Informed consent procedures can be used to protect the identity of clients who are exposed in these stories by using fictitious names, making minor changes in their personal circumstances to prevent identification, and restricting access to narrative evaluation material.</p>

<h3 id="informal-exchanges-of-information-with-stakeholders">Informal Exchanges of Information with Stakeholders</h3>

<p>Another format for sharing findings with stakeholders is to arrange informal exchanges with specific stakeholders. In some instances, it may be important to provide handouts of findings as part of an informal oral report for special stakeholders such as board officers and advisory committees. These materials can be flexibly presented so that recipients can fully air their views and ideas for follow-up. Stakeholders can also be consulted about the best venue for holding larger meetings of board members, advisory committees, and the community at large, as they may know best what aspects of the findings to share and how to do so most effectively.</p>

<h2 id="strategies-for-preparing-a-report">STRATEGIES FOR PREPARING A REPORT</h2>

<p>As was emphasized earlier in the chapter, reports can take many forms. They can be long or short, oral or written, and they can use many formats. Preparing the report may take considerable or little time, depending on its nature. Either way, however, it needs to be thoughtfully planned and executed. Four likely tasks to consider in preparing a report are highlighted next: clarifying the purpose of the report and its targeted audience, presenting helpful background information, organizing and presenting the findings, and making recommendations.</p>

<h3 id="clarifying-the-purpose-of-the-report">Clarifying the Purpose of the Report</h3>

<p>Theoverallpurposeofareportistodescribethefindingsandtoofferrecommendations   to address the concerns about a program or practice intervention. However, as indicated earlier, a report can vary widely depending on who the stakeholders are. Before preparing the report, it is important to hold discussions with stakeholders to identify their needs. The specifics of the report can be determined from such discussions. Key specifics to consider include making decisions about which ones to highlight. In addition, the recommendations will likely be most important to include.</p>

<p>Equally important to selecting the specifics for the report are the strategies for disseminating them. Important questions to ask about dissemination include, how can the report be presented to address the interests of the stakeholders and how can stakeholders become involved in contributing to the implementation of the recommendations and any plans for follow-up?</p>

<h3 id="background-information">Background Information</h3>

<p>Most reports will need to include some background information on the evaluation before the findings. A technical report may do so in a lengthy and thorough way, whereas brief reports to other stakeholders may only mention helpful details in a few sentences, followed by any questions. Most stakeholders will want to be reminded about the purpose of the evaluation, the evaluation questions, the types of people studied, and the methods. They may also want to be reminded of the concerns that led to initiation of the evaluation and the options currently available to address what to do about a program now.</p>

<h1 id="organizing-and-presenting-the-findings">Organizing and Presenting the Findings</h1>

<p>Next, the findings of the evaluation need to be organized and presented so that they are readable, relevant, and useful. Organizing the findings is a very important task that entails focusing the findings, ensuring their accuracy and clarity, and using visual aids when appropriate. Focusing the findings depends on the needs of stakeholders and on the initial evaluation questions. In this regard, the evaluation questions can provide a helpful focus and an organizing framework for the report, as the findings are supposed to answer the evaluation questions.</p>

<blockquote>
  <p><strong>Example of Preparing a Report for a Qualitative Study</strong></p>

  <p>A study (Leary, 2003) was conducted of several volunteers of churches in an ecumenical program that offers overnight shelter to small groups of homeless people once every week during the winter months. Initially, the researcher identified the three research questions as the focus of the report: (a) What are the needs of the homeless guests? (b) What efforts have the churches made to try to meet these needs? and (c) What efforts might churches try in the future? Although the evaluator explored all three questions in focus groups, volunteer participants gave  much more attention to the third question; it turned out that both the ecumenical program and the volunteer participants perceived the responses to question    3 to be the central set of findings of importance to them. Therefore, the evaluator decided that the major focus of the report would be on the third question.</p>
</blockquote>

<p>It is also important to present findings accurately. Some of the questions to ask related to accuracy could be the following: Are the correct statistical tests used to analyze the data? Is there a balance of positive and negative findings when both exist? Are tables and graphs prepared accurately and are they easy to understand? Do the tables and graphs focus on the most important findings?</p>

<p>A report that includes only uninterrupted written or oral material can be boring and tedious to read. Therefore, in preparing a report, consider using tables, graphs, and other visual aids to creatively highlight and summarize important findings. The Statistical Package for the Social Sciences (SPSS), Microsoft Excel, and other computer programs can be used to prepare graphics. Tables are useful for consolidating several data points into a single visual presentation to highlight an important set of findings. For example, a table can highlight the characteristics of research participants, compare the outcomes of a treatment and comparison group, or compare the pretest and posttest measures of a one-group pretest–posttest design.</p>

<p>Infographics is another newer electronic resource that can also be a helpful tool. Infographics are visual representations of complex concepts; in these instances, data are presented in an easy to read and enjoyable graphic form. This tool is designed to condenses large amounts of information into a form in which it is much more easier for readers to absorb. Clients, community groups, and others who can easily become bored by more traditional presentations may find these reports to be much more readable. Infographics seem most useful to use in brief reports like handouts, bulletin board displays, power point presentations, and Facebook posters. Infographics can be used to visually display basic statistics, timelines, processes, geographic information, comparisons, hierarchical and other content (Smith, 2018). Google “infographics in social work” to find numerous examples of how infographics can be used in social work.</p>

<p>Table 11.1 presents a summary of two subgroups of female teenage participants, those who were maltreated in childhood and those who were not, to highlight their differences. Tables can be challenging to read as this one is. Note that the table reveals, using asterisks, that the two subgroups were significantly different based on family structure, race and ethnicity, and welfare status. Note also that the first two columns of data in the table are presented as percentages and data in the third column are actual numbers. This way you can calculate how many people in numbers were maltreated in the first column. Note that the total number in each category in column 3 does not always total 249 teenagers as some teenagers likely did not respond to these questions.</p>

<p>Pie and bar charts are other ways to highlight findings. Bar charts can compare two or more groups of people on a specific measure. Pie charts are useful when a circular display communicates findings more effectively than a linear bar chart.</p>

<p>Qualitative findings can be presented in several different ways, as case studies to illustrate significant issues affecting one or a few people on a pertinent topic, as responses organized into fewer general categories, or themes.</p>

<p><img src="https://i.imgur.com/ygMP1uJ.png" alt="Imgur" /></p>

<h1 id="making-recommendations">Making Recommendations</h1>

<p>Next, the findings are to be interpreted. Interpretations in evaluation studies often take the form of practical recommendations. The recommendations are an important section of a report which needs to be developed carefully. Some evaluators suggest that recommendations are the most important part of the report because stakeholders tend to turn to this section for guidance.</p>

<p>Guidelines are helpful when preparing recommendations for a research report. For example, begin by attempting to draw some general conclusions from the findings. What do they seem to say? Next, list specific recommendations in such a way that they are relevant and useful to stakeholders.</p>

<p>Recommendations must be supported by the findings. Recommendations should not be a wish list or an excuse to promote a specific viewpoint unsupported by findings. A helpful exercise is to identify each recommendation in a report and then identify the specific findings of the evaluation, if any, that support each recommendation. When there are no findings to support a recommendation, it should be omitted.</p>

<p>Recommendations are usually stated tentatively, especially in formative studies, to permit the stakeholders to offer their own thoughts. Recommendations should be practical and address such questions as whether the program should be continued, modified, expanded, or take another course. Good recommendations are stated in a way that is helpful to the agency and to clients. Sometimes, the more that recommendations are elaborated on, the more helpful they are. For example, the evaluator may find out in an evaluation on staff morale that staff morale is low for several reasons, not one, and they are all supported by findings. Examples of recommendations from a staff morale study could include raising salaries, improving supervision, and/or reducing caseloads or workloads.</p>

<p>The discussion of a study’s recommendations should also mention any limitations that need to be considered in implementing them. For example, an evaluation about the school adjustment of children born and raised in the United States may not have direct applicability for children who are recent immigrants. Examples of types of limitations that should be reported are a small sample, questions that were poorly formulated or not asked, a convenience sample or other nonprobability sampling approach, a low response rate for a questionnaire, no baseline data collected, and uncontrolled important external variables.</p>

<h1 id="strategies-for-disseminating-reports">STRATEGIES FOR DISSEMINATING REPORTS</h1>

<p>A proactive strategy is needed to inform the stakeholders about the results of an evaluation. The information that stakeholders receive should be clear, thorough, and relevant to their needs. Because stakeholders have diverse needs, the information they receive should take into consideration such things as what to emphasize, how much material to report, and useful recommendations. Numerous strategies are available that reflect various ways to communicate findings. Oral options include presentations, informal discussions, conversations between practitioners and clients, staff workshops, and poster sessions in the community. Providing helpful written materials are equally important and can take the form of technical reports, summaries and sections of technical reports, published results in professional journals, memos and flyers, brochures and other educational materials, and more.</p>

<p>A report’s preparation and dissemination are usually planned together. A dissemination strategy often naturally evolves from the format of a report. For example, a technical report is usually disseminated in written form and could be explained in a presentation. In contrast, findings of interest to clients are likely to be focused and shared informally through discussions and question-and-answer formats. Identification of a dissemination strategy begins with discussions with stakeholders about what they need and find useful. From there, an oral or written report is planned, as well as a strategy to share the report in meaningful ways with stakeholders.</p>

<p>Table 11.2 summarizes many options available for the range of stakeholders. Some types of stakeholders, such as funding agencies and agency board members, are likely to want to receive lengthy technical reports and summaries. These reports can be disseminated through informal discussions and more formal presentations with question-and-answer formats. Other types of stakeholders (e.g., neighbors, representatives of civic organizations and agencies, the public) may prefer special reports or brief summaries that can be presented and discussed in a more informal and spontaneous manner.</p>

<p>There are several reasons to devote time, money, and energy to disseminating evaluation findings. The most practical reason is to involve stakeholders in any follow-up action for addressing a program or practice concern. Usually, evaluation reports include a list of recommendations to be considered, such as whether a program should be continued, expanded, changed, or discontinued after an evaluation is completed. One could easily reason that the more that stakeholders are involved in the decision-making process, the better the decisions will be because they will be based on a wide range of relevant perspectives.</p>

<p><img src="https://i.imgur.com/gfkjCQW.png" alt="Imgur" /></p>

<p>In addition, many stakeholders will expect to hear about the results of an evaluation, especially if they participated in the earlier planning and execution steps. Leaving some stakeholders out of the final step could lead them to feel that they are not important participants. Staff members, clients, and community groups are among the stakeholders most often likely to be left out of this final step, with negative consequences not only for them but for the intervention as well. Staff members and clients should be more active participants in the decision-making process, especially in an advisory capacity. Their involvement and feedback can enhance the functioning of interventions.</p>

<p>A one-page handout was used to inform a group of thirty Latino people and others in their community about the results of a needs assessment in which they participated. These people were experiencing food insecurity problems, and the focus of this needs assessment was on finding out their preferences for overcoming food insecurity. The handout was made available in both Spanish and English.</p>

<h2 id="handout-of-findings-of-a-needs-assessment-of-food-insecure-latino-people">Handout of Findings of a Needs Assessment of Food Insecure Latino People</h2>

<p>This box is for a report that fills one page and is on a different file. It can be made slightly smaller.</p>

<p>The file name is SWE 3—Chapter 11—Handout to Latino participants.docx</p>

<p>Another reason to give special attention to disseminating findings is that agencies usually want stakeholders to be ongoing participants in supporting programs and practice interventions, and many stakeholders want this as well. Viewing the findings can provide them with new information and insights about an intervention and the provider agency. It can help them become more informed in their roles as stakeholders, and they can discover the important role that evaluations play in enhancing programs and practice.</p>

<p>Ultimately, evaluation is an accountability measure. It addresses important questions such as the following:</p>

<ul>
  <li>Is the program or practice intervention focusing on the target population with the greatest need?</li>
  <li>Is the intervention meeting the specified short-and long-term needs of the target population?</li>
  <li>Is the intervention being implemented with high standards?</li>
  <li>Are clients satisfied with the program?</li>
  <li>Is the intervention achieving its goals and objectives?</li>
  <li>Is it cost-effective?</li>
</ul>

<p>These are questions that should not be left solely to evaluators and program administrators. Involving an active team of stakeholders, a viewpoint that has been advocated throughout the book, can optimize the likelihood that accountability will be real and substantive.</p>

<center>## SUMMARY</center>

<p>The chapter explores the final two steps in conducting an evaluation: preparing a report of the findings and disseminating the findings to others. Evaluators are encouraged to actively involve stakeholders in these two steps as they have in previous steps. Evaluators can begin to prepare a report by exploring with each stakeholder the kinds of information that they will need and the most readable format. The stakeholders’ articulation of their needs will then help determine the kinds of reports they will find most helpful. As a result, evaluation reports can be quite varied. They can be oral or written, brief or comprehensive, public hearings or private discussions with staff members and clients. It all depends upon the needs of each stakeholder. Several different types of reports are described in the chapter including technical research reports, formal presentations at a public meeting, discussion topics covered at staff workshops, informal question and answer discussions with some stakeholders, summary reports, manuscripts for professionals, press releases, and relevant and helpful discussions with clients.</p>

<p>The contents of a report are also discussed in the chapter with four important tasks being highlighted. These tasks are clarifying the purpose of the report and its targeted audience, presenting helpful background information, giving special attention to organizing and presenting the findings, and making useful recommendations that are substantiated by the findings. Dissemination of the report is the last step and a crucial one if the findings and recommendations are going to be effectively utilized. A report’s preparation and dissemination are usually planned together, and a dissemination strategy often naturally evolves from the format of a report. Table 11.2 summarizes many of the options available for reports and the strategies for their dissemination.</p>

<h2 id="discussion-questions-and-assignments">DISCUSSION QUESTIONS AND ASSIGNMENTS</h2>

<ol>
  <li>Select an evaluation report from a social work or evaluation journal. Critique the findings section by answering the following questions:
    <ul>
      <li>What are the major findings?</li>
      <li>Were the findings presented clearly? If not, what was unclear? How could they be presented more clearly?</li>
      <li>Were the tables easy to understand? If not, why not?</li>
      <li>How are the findings useful to you as a practitioner?</li>
      <li>Are the limitations of the study discussed? What are the limitations?</li>
    </ul>
  </li>
  <li>Using the same evaluation report used in question 1, determine how well the recommendations are supported by the findings by answering the following questions:
    <ul>
      <li>Identify each recommendation.</li>
      <li>Identify the specific findings, if any, that support each recommendation.</li>
      <li>In your opinion, is each recommendation adequately supported by a specific finding. If not, what more seems to be needed to conclude this recommendation?</li>
      <li>What recommendations were useful to you as a social worker?</li>
    </ul>
  </li>
</ol>

<h1 id="references">REFERENCES</h1>

<ul>
  <li>Bamberger, M., Rugh, J., &amp; Mabry, L. (2019). <em>Real world evaluation: Working under budget,</em> <em>time, data, and political constraints</em> (3nd ed.). Los Angeles, CA: SAGE.</li>
  <li>Dudley, J., Calhoun, M., &amp; Ahlgrim-Delzell, L. (Eds.). (2002). <em>Lessons learned from a law-</em> <em>suit: Creating services for people with mental illness and mental retardation</em>. Kingston, NY: National Association for the Dually Diagnosed Press.</li>
  <li>Dudley, J. R., &amp; Vasquez, L. (2019). <em>The voices of Latina people faced with food insecurity.</em> Unpublished manuscript, University of North Carolina, Charlotte, NC.</li>
  <li>Gardner, F. (2000). Design evaluation: Illuminating social work practice for better outcomes. <em>Social Work, 45</em>(2), 176–182.</li>
  <li>Leary, A. (2003). <em>Room in the inn: Exploring church involvement in serving the homeless</em>. Unpublished student research project, University of North Carolina, Charlotte, NC.</li>
  <li>Linfield, K. J., &amp; Posavac, E. J. (2019). <em>Program evaluation: Methods and case studies</em> (9th ed.). Upper Saddle River, NJ: Prentice Hall.</li>
  <li>Morris, L. L., Fitz-Gibbon, C. T., &amp; Freeman, M. (1987). <em>How to communicate evaluation findings</em>. Newbury Park, CA: SAGE.</li>
  <li>Smith, C. (1996). The link between childhood maltreatment and teenage pregnancy. <em>Social</em> <em>Work</em> <em>Research, 20</em>(3), 131–141.</li>
  <li>Smith, D. (2018). <em>Growing your library career with social media</em>. Cambridge, MA: Elsevier.</li>
  <li>York, R. O. (2009). Evaluating human services: A practical approach for the human service professional. Boston, MA: Pearson.</li>
</ul>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
