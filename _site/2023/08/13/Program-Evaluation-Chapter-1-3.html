<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Program Evaluation Chapter 1-3 | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Program Evaluation Chapter 1-3" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Preface" />
<meta property="og:description" content="Preface" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/13/Program-Evaluation-Chapter-1-3.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/13/Program-Evaluation-Chapter-1-3.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-13T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Program Evaluation Chapter 1-3" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-13T00:00:00+09:00","datePublished":"2023-08-13T00:00:00+09:00","description":"Preface","headline":"Program Evaluation Chapter 1-3","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/13/Program-Evaluation-Chapter-1-3.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/13/Program-Evaluation-Chapter-1-3.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=d646b21d8d4bf4f7a455f43bebf1582640cadebe">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>13 August 2023</small>
<h1>Program Evaluation Chapter 1-3</h1>

<p class="view">by JAMES R. DUDLEY</p>

<h1 id="preface">Preface</h1>

<p>Welcome to the exciting field of program evaluation and to this introduction to the practice of it. If you do   not already know that program evaluation is a fascinating area, I encourage you to pay particular attention to the Evaluator Profiles throughout this book and the accounts of how many of them discovered this almost hidden treasure. As you will learn, it involves a set of skills and practices that can provide valuable insights to crucial human services as well as provide important components for careers, or even an entire career. But unlike many occupations such as medical doctors or professors, too many people know little or nothing about  it, so few children say, “I want to be a program evaluator when I grow up.” Many current evaluators can tell wonderful stories about how they discovered this great field that they never knew existed.</p>

<p>This textbook is intended as a solid introduction to the field for those who are ready to build a good foundation. On the one hand, it covers all the main aspects and provides substantial practical insights. Students who learn this material diligently, especially if they also have the opportunity to have some hands-on experiences while doing so, should be prepared then to venture into formal program evaluations, although  many will be glad to start as part of a team. On the other hand, there are a number of specific aspects of professional program evaluation that would benefit from further specialized study, such as particular approaches to evaluation like empowerment and utilization, or specific forms of evaluation like needs assessment. This good introduction does not pretend to be fully comprehensive, and those who are ready to tackle the full field have many excellent advanced books and formal programs to choose from.</p>

<p>Still, this ninth edition builds on a fine tradition even as it ventures into some new territory. Emil Posavac’s editions have made a long-standing contribution to the field. I (Ken Linfield) have used his textbook every year since I started teaching our doctoral course now twelve years ago, and it was the one used for years before I started. As I join in writing this new edition as the first author, I am especially pleased to expand the case studies and profiles to all fourteen chapters, providing more extended examples of a broader range of evaluations and evaluators. The case studies provide details from the evaluators’ perspectives even as they have also been organized with the MISSION framework introduced in this edition. The MISSION acronym is presented to help evaluators, especially those newer to the field, to remember easily the broad  range of elements in program evaluation—Meeting needs, Implementation, Stakeholders, Side effects, Improvement focus, Outcomes, and Nuances (see especially Chapter 1).</p>

<p>Another extension of the solid material in this text matches the developments of our time, adding resources available online to the hard copy textbooks or the e-books that present the standard chapters. These “eResources” provide an important span of information that would not fit in a traditional textbook—multiple examples of evaluation reports, extensive descriptions and summary tables of the growing number of models of evaluation, and much more. Further, active hyperlinks and other materials like spreadsheets provide a dynamic option impossible with traditional textbooks. And not only do the eResources also permit some level of interaction with the first author, there are some lighthearted sections as well—check out the video of my “Program Evaluator” song and others. Perhaps more importantly, regular updates will address changes since  the publication of the book you are reading.</p>

<p>So—welcome to the great field of program evaluation. I invite you to find the portions that particularly connect with what you are passionate about—improving services for people who have endured traumatic events, attending to a population that is important to you, building on essential values like evidence and integrity, or another. Enjoy the ride!</p>

<h1 id="1program-evaluation">1 Program Evaluation</h1>

<h2 id="an-overview">An Overview</h2>

<p>Suppose you are working for the Student Counseling Center at Western Tech in a year or two. The center plans to offer a Sexual Assault Prevention Program to women and men in the following month, and you have been asked to develop a plan that will permit the center’s staff to decide whether (a) to offer the program again because it is successful, (b) to alter the program in order to make it more useful to participants, or (c) to drop the program because it fails to meet a need. You might stop reading for a moment and write down the steps you think are necessary to measure the success of such a program. Among other points, consider what evidence would indicate the program is successful, what would mean that it should be improved, and what would show that it should be ended.</p>

<p>Because the center wishes to maintain and promote useful services to students, evaluating a program that might reduce sexual assaults on campus is an important assignment. The staff of the center would not want to devote resources for a program that does not meet the needs of students on campus; in such a case, something more helpful should be offered instead. If the program is good, you would want your plan to detect its strengths; where it needs improvement, you would want to detect its limitations. When you are finished with this text, you will be ready to tackle this project. To begin, what is program evaluation?</p>

<p>The practice of evaluating one’s own efforts is as natural as breathing. Cooks taste their gravy and sauce and basketball players watch to see whether their shots go in. Indeed, it would be most unwise after turning on the hot water to neglect to check the water temperature before stepping into a shower. At a basic level, program evaluation is nothing more than applying this commonsense practice to settings in which organized efforts, called “programs,” are devoted to helping people who need education, medical treatment, job training, safe streets, welfare assistance, safety while traveling in airplanes, recreational services, or any of the thousands of services provided in a modern society. Program evaluation is a methodology to learn the depth and extent of need for a human service and whether the service is likely to be used, whether the service is sufficient to meet the unmet needs identified, and the degree to which the service is offered as planned and actually does help people in need at a reasonable cost without unacceptable side effects. Utilizing research methods and concepts from psychology, sociology, administration and policy sciences, economics, and education, program evaluators seek to contribute to the improvement of programs.</p>

<p>There are several crucial differences between the natural, almost automatic, evaluations we carry on as we work and play and the practice of program evaluation in organizational settings. These differences make program evaluation harder than the self-evaluations that we all do. First, organizational efforts are nearly always carried out by a team—whether that be a school team of teachers, coaches, and administrators; a hospital team of physicians, nurses, and technicians; or a welfare department team of social workers, counselors, and file clerks. This specialization means that the responsibility for program effectiveness is shared among many people. Furthermore, the results of such programs—an educated student, a well patient, or an effective young mother—are not the sole responsibility of any individual.</p>

<p>Second, most programs attempt to achieve objectives that can only be observed sometime in the future rather than in a matter of minutes, as is the case with a cook adding a spice to a simmering pot or a painter repairing a damaged wall. In other words, as the time between an activity and the desired outcome of that activity lengthens, it becomes less clear what we are to observe in order to decide that the activity is being carried out appropriately or not and what could be done to improve the result of the activity. In fact, the   choice of criteria to use in deciding whether a program is working well creates debates among evaluators, program personnel, clients, and funding agencies. How to respond when a problem is detected is often just as controversial.</p>

<p>Third, in contrast to evaluating our own efforts, there are many different parties involved in the evaluations of programs. For example, the effectiveness of teaching in a school may be assessed by evaluators who work for the central administration or the school board. It would not be surprising that individual staff members might feel threatened if the evaluators do not have their trust because, in an extreme case, the livelihood of the teachers might be harmed by an incompetently done program evaluation.</p>

<p>Last, programs are usually paid for by parties other than the clients of a program. Funds for the salaries of nurses are obtained from payments to hospitals by insurance companies and government agencies, and schoolteachers are paid through taxes collected by school districts. Although nearly all nurses and teachers  want to do effective work, their job security is at least in the short term more dependent on keeping the  program funders satisfied than serving their patients and students well.</p>

<p>Such complications as these make program evaluation considerably more difficult to do than the evaluations of our own activities that we perform so easily and naturally. This text focuses its discussion on evaluations of programs in human services, such as education, medical care, welfare, rehabilitation, or job training. Program evaluation is one form of the general discipline of evaluation that includes employee assessment, quality control in manufacturing, and policy analysis, among other activities (Scriven, 2003). The various kinds of evaluation are sometimes confused with each other. All forms of evaluation involve discovering the value and worth of something; however, the focuses, purposes, and methodologies are quite different.</p>

<h2 id="evaluation-areas-that-need-to-be-considered">EVALUATION AREAS THAT NEED TO BE CONSIDERED</h2>

<p>A common assumption by people who are not familiar with program evaluation is that all evaluators do is measure the outcome of a program—perhaps counting how many of the clients in a substance abuse treatment program no longer use the substance, either at the end of the program or at a later time, or determining the typical change in knowledge (hopefully an increase) for those who have attended a series of workshops. As will be seen below, measuring outcomes is a very important part of program evaluation. But it is only one part. We will make the case briefly here and then repeatedly throughout this text that program evaluation is a much broader set of activities than just measuring the outcome of a program and that good evaluators consistently think about this wider range of elements. The following paragraphs introduce the seven general areas that program evaluators need to consider. It should be noted that some evaluations may not explicitly include all these elements, and at times a single one of the components may be a complete evaluation (for example, a  needs assessment). But choosing not to include one or more elements should be a deliberate decision based on  a clear rationale, not a matter of focusing so intently on a limited set of areas that one forgets other considerations.</p>

<h3 id="meeting-needs">Meeting Needs</h3>

<p>Most often, program evaluations are conducted on existing programs where there is a reasonable likelihood that the programs will continue in somewhat similar fashion for the foreseeable future. As a result, the original need that led to the program may not be raised as an important point of the evaluation; nearly everyone involved may just assume that doing something to address that need is a worthwhile activity. In fact, many people may be unable to imagine not having certain programs. Most of us assume that education is necessary for children and that we must have police and fire departments in our communities without needing to document precise details of what those children do not yet know or of the potential for crime and fires. As you will see in Chapter 6, “The Assessment of Need,” there are a number of important aspects of this component of program evaluation beyond just documenting that there is a legitimate problem that the program addresses. One overarching point, however, is a focus on unmet needs. Although we all need to breathe air, we do not have large numbers of programs providing air nor evaluations to discuss them. Rather obviously, programs are developed when one or more needs are not already addressed. So evaluators consider unmet needs—whether in an area that has never been addressed or the extent to which current needs are not fully met by existing programs. Various other aspects can also be considered, such as how the need might be expected to change in the future or how the need is perceived differently by different people.</p>

<h3 id="implementation">Implementation</h3>

<p>A common problem with programs is that some are either never implemented as planned or are implemented  in such a way that people in need receive no or minimal benefit. Consequently, it is essential that evaluators confirm points ranging from the fact that program personnel have been hired and offices have been opened to documenting how many clients have found the program and the nature of the services provided. When the primary focus addresses such issues, the project may be called an implementation evaluation, in contrast to an outcome evaluation that also considers the results of the interventions.</p>

<p>Although the monitoring function of program evaluation may at first strike the reader as unnecessary,   there are many examples of programs that, although funded, were never put into practice. Even more common, descriptions of programs in agency materials may be accurate only with regard to what was intended, but not what is currently happening. In fact, a recent estimate notes “that failure rates for organizational change implementation typically range from 28% to as high as 93%” (Durand, Decker, &amp; Kirkman, 2014, p. 404). To be fair, the actual practices could be better than originally planned, rather than worse. In any case, an essential area for evaluators is monitoring the details of how the program has been implemented.</p>

<h3 id="stakeholders">Stakeholders</h3>

<p>In the process of conducting an evaluation, evaluators might generally expect to have some contact with the clients who are served by the program, especially indirect contact, by considering data from the clients such as information on their changed condition or their responses to a questionnaire. But it is essential that evaluators take into account the many people who are involved with a program in one way or another, those who provide the service as well as those who receive it, and those who are supportive from a distance as well as those whose connection is either mixed or even opposed to some degree. The idea that all of these people have a stake of some form in the program has led to the term “stakeholders” as a general way to talk about them.</p>

<p>Staff providing the services are obviously highly involved both because they deal day-to-day with the  clients and because they generally are paid to provide the services. Paying attention to staff as stakeholders involves many points such as recognizing that therapists often know important details about clients beyond what is contained in agency records, that staff may either resist change or be eager for it, and that most employees have some level of apprehension about the results of an evaluation in terms of their own future. Of course, there are a range of staff positions in most agencies—therapists, administrators, support staff, and others. In addition, most agencies have a board that oversees general operations. Often there are others in the community who care about the program in some way—typically including family and perhaps friends of clients, people in the community who are passionate about the specific need being addressed, and occasionally those who are affected by financial aspects of the program. At times, those who live near a treatment facility may have reactions to details of the operation such as traffic or clients’ behavior or still other points.</p>

<p>On the one hand, it is impossible for evaluators to know everything about all stakeholders and completely unrealistic to try to make all stakeholders happy. But on the other hand, it is essential that evaluators make reasonable efforts to learn about the major stakeholders and any important current issues. A common element is that the staff providing services, administrative staff, and board members have different expectations for the evaluation itself. If administrators want to improve efficiency, board members want to lower costs, and therapists want more freedom to extend treatment, there are important implications for the progress of an evaluation and what evaluators should expect regarding cooperation from the stakeholders.</p>

<p>Recently, evaluation researchers have explored the principles that appear to support successful collaboration with stakeholders, such as clarifying the motivation for collaboration (Shulha, Whitmore, Cousins, Gilbert, &amp;  al Hudib, 2016) as well as what can be learned about the involvement of stakeholders in evaluation more generally, such as noting that studies have found substantial evidence both for benefits and for liabilities (Brandon &amp; Fukunaga, 2014).</p>

<h3 id="side-effects">Side Effects</h3>

<p>Efforts to solve problems usually create some undesirable side effects (Sieber, 1981). For example, medications that relieve one health problem may create problems in other systems of the body; sometimes these side effects are minor, sometimes they require medical attention or a change in medication, and sometimes, though rarely, they are fatal. Patients and physicians must therefore weigh the dangers of side effects, neither minimizing  their importance nor avoiding helpful treatments. Even when the benefits appear clearly to outweigh the costs  to medical professionals, others may perceive these effects differently. An important current challenge is that some parents come to different conclusions about the comparative value of vaccinations than physicians (Harmsen, Mollema, Ruiter, Paulussen, de Melker, &amp; Kok, 2013). A parallel situation occurs in the area of educational and social service programs. Welfare policies can help people during crises to reestablish their productive roles in society; on the other hand, welfare policies can create long-term dependency (McKnight, 1995). Programs to provide social support to isolated people can have negative effects that have typically been ignored (Lincoln, 2000). Special education classes permit children with unique needs to learn at a comfortable pace, yet being in such a class carries a stigma. Consequently, program planners seek to develop services that provide benefits but minimize negative side effects. On the other hand, unforeseen side effects are occasionally positive: When people learn new skills, self-esteem often increases (Dawes, 1994; Felson, 1984).</p>

<h3 id="improvement-focus">Improvement Focus</h3>

<p>Most of these seven areas cover elements of programs that good evaluators consider, measure, and even  analyze with formal statistical procedures. The area of improvement focus, in contrast, describes an attitude or approach that evaluators take throughout the course of an evaluation. This pervasive aspect involves considering not just how good the program is now, but how it can become better in the future. The improvement focus is especially important when the evaluation occurs early in the life of a program and is intended to catch and fix any problems. Such evaluations are often called formative evaluations in contrast to summative evaluations that may come at or even after the conclusion of the program. Formative evaluations explicitly look for ways to improve the program, whether by correcting actual mistakes or unanticipated problems or by building on particularly successful components. Even with summative evaluations, however, an improvement focus will make an important contribution whether the program is directly replicated or adapted  to fit new settings, circumstances, or methods.</p>

<p>To be clear, this improvement focus does not mean that evaluators are never critical of problems or shortcomings, let alone of failures in a program. Remember that the primary purpose of an evaluation is to determine the value or worth of a program, so if a particular intervention is wasting time or money, or, worse, actively harming clients, evaluators have an obligation to report on these problems clearly. But even with substantial limitations, most programs have value that can be identified and used as a basis for better work in the future. Although it is possible that an evaluation may find there is nothing redeemable in a given program, such conclusions are rare especially when evaluators take the improvement focus seriously.</p>

<h3 id="outcomes">Outcomes</h3>

<p>It should be fairly obvious that an evaluation of a program would consider the results such as how clients improve or other details about how those involved in the program have changed because of their participation. If the proof of the pudding is in the eating, the proof of the program is commonly seen in those who have gone through the program. Those who attend educational programs are expected to know more. Clients in treatment for problems should show fewer (or no) problems; they might also show higher levels of functioning</p>

<p>—increased strengths, not just fewer weaknesses. Broader programs such as in public health might be expected to lead to changes at the community level—lower incidence of particular diseases or other problems among the entire population, not just with identified individual. The concept of outcomes is a general one,   but the details vary considerably depending on the particular program.</p>

<p>As noted above, for many people examining outcomes is the most obvious, if not the only, aspect they look for in an evaluation. Although it is essential that good evaluators think about the other areas, outcomes need substantial attention, too. Improvement in clients could be seen in a reduction of problems, in an increase of skills, or in both. Clients’ own perception of skill may be important as may observations by others that their abilities have increased. The most appropriate time frame for documenting changes might be at the end of treatment, six months after treatment, or six years later. Increasingly, it is not enough for evaluators to show that those who participated in programs are different following the program—there is an emphasis on showing that the program contained the “active ingredients” that caused the change (Gates &amp; Dyson, 2017). Chapters 8 through 11 will address the many ways that evaluations consider evidence, not just that there are good outcomes, but that the programs are responsible for those good outcomes. As you will see, at times there are at best gentle hints that the program may have led to the changes whereas under the best conditions, there may be very clear reasons to believe that the program caused the good effects. Evaluators examine outcomes and the evidence that those outcomes are caused by the program.</p>

<p>When the specifics about outcomes are clear, the task may still not be finished. There are a number of   ways to compare aspects of the outcomes that are important in some evaluations. Although many agencies  have one program with a single focus and the goal is simply to have good results from it, others have multiple programs with more sophisticated criteria and where programs are compared with one another in terms of the value of the benefit overall or in terms of the cost for a given benefit. A program with moderately good outcomes may not have good enough results to justify its continuation if resources are threatened. The larger area of outcomes can thus be subdivided into various kinds of comparisons—by cost, by value to the stakeholders, or by other constraints such as how long it takes to achieve a given result. All these ways of examining outcomes are potential areas for evaluators to consider, and will be addressed in a variety of ways later in this text.</p>

<h3 id="nuances-mechanisms">Nuances (Mechanisms)</h3>

<p>Although evaluations have often aimed simply to document that participants have benefitted sufficiently, another growing trend is to examine more sophisticated questions such as specific reasons why the good outcomes have occurred—both in terms of what mechanisms were involved as well as considering how those  in different groups or conditions experience better or worse results (Solmeyer &amp; Constance, 2015). Were all  the services provided (and funded) essential ingredients in the good results or were some of the activities pleasant for the clients but otherwise irrelevant ways to use staff time, effort, and the funders’ money? Elements of therapy that continue out of habit or tradition rather than because they have been shown to lead   to improvements are good candidates to be reconsidered and, potentially, eliminated. Another important point is that some of those receiving services may not benefit like the rest of the group either because of their existing characteristics or because they react to the treatment differently. A treatment that works well with  older adolescents but is not effective with the youngest clients might appear to yield good outcomes overall,  but understanding the different effects could lead to using a different, but better, treatment for the younger children.</p>

<p>One important aspect of examining mechanisms in an evaluation is that measures of the suspected mechanisms must be included. If therapists or evaluators believe that cognitive restructuring is an essential part of useful therapy, the evaluators will need to assess how much each client successfully restructured thoughts. With such measures, the evaluators can analyze the degree to which clients with greater restructuring tended to improve more. If the effect is consistent and large enough, that would provide evidence to support the use of that component. In addition to specific techniques, a somewhat broader construct such as a measure of the degree to which the planned intervention is followed can also address the question of mechanisms (Abry, Hulleman, &amp; Rimm-Kaufman, 2015). More detailed discussions of these specifics will appear in the sections on data analysis, especially Chapter 11 on experimental approaches.</p>

<h3 id="mission">MISSION</h3>

<p>Many evaluators new to the profession find it challenging to remember these aspects of good evaluations. To help students and others, “MISSION” is an acronym for these seven areas—Meeting Needs, Implementation, Stakeholders, Side Effects, Improvement, Outcomes, and Nuances. These seven elements provide an all- purpose set of categories for evaluators to consider regardless of the evaluation. As noted above, it is possible that in a particular case, the evaluator might choose to minimize or even ignore the issue of needs if all stakeholders were already so thoroughly convinced of their reality. But not addressing one of these areas  should be a deliberate choice. And more than one evaluator has forgotten to pay attention to side effects only  to regret the oversight later. To help students remember these elements, the MISSION acronym will be a theme throughout this text, explicitly used in the case studies to illustrate both the various forms these components can take as well as the times when good evaluations may pay little or even no attention to some of them.</p>

<h2 id="common-types-of-program-evaluations">COMMON TYPES OF PROGRAM EVALUATIONS</h2>

<p>The primary goals of program evaluation, just discussed, can be met using a number of different types of program evaluations; the major ones involve studies of need, process, outcome, and efficiency.</p>

<h3 id="assess-needs-of-the-program-participants">Assess Needs of the Program Participants</h3>

<p>An evaluation of need seeks to identify and measure the level of unmet needs within an organization or community. Assessing unmet needs is a basic first step before any effective program planning can begin. Program planning involves the consideration of a variety of alternative approaches to meet needs. In selecting some alternatives and discarding others, planners engage in a form of program evaluation, one that occurs before the program is even begun. The close association between program planning and program evaluation is indicated in the title of the journal <em>Program Planning and Evaluation</em>.</p>

<p>As part of the assessment of need, evaluators may examine the socioeconomic profile of the community, the level of social problems within the community, and the agencies and institutions currently serving the community. Through close contact with residents and local leaders, evaluators can learn which aspects of a program are likely to be useful and which might be unacceptable, thus adding depth to the inferences drawn from statistical summaries suggesting critical unmet needs. Sometimes program evaluators believe that there is a pressing need for a sense of self-determination or empowerment (Fetterman &amp; Wandersman, 2004).</p>

<h3 id="examine-the-process-of-meeting-the-needs">Examine the Process of Meeting the Needs</h3>

<p>Once a program has been developed and begun, evaluators turn to the task of documenting the extent to which implementation has taken place, the nature of the people being served, and the degree to which the program operates as expected. The activities making up the program, such as hours of counseling, number of classes held, and hours police officers patrolled, are called “outputs.” Evaluations of process involve checking on the assumptions made while the program was being planned. Do the needs of the people served or the community match what was believed during planning? Is there evidence to support the assessment of needs made during the planning stage? Do the activities carried out by the staff match the plans for the program? What evidence can be found that supports the theoretical assumptions made by the program planners? It is crucial to learn how a program actually operates before offering the same services at additional locations or with other populations.</p>

<p>Under normal situations, the information necessary for an evaluation of process is available in the records  of an agency sponsoring the program; however, the information may be recorded in a way that is hard to use. For example, information on application forms often is not summarized, and records of actual services received may not permit easy analysis. Furthermore, sometimes no records are kept on a day-to-day basis. In Chapter 7 on program monitoring and information systems, some very simple methods of developing an information system for a human service setting are illustrated. In addition to quantitative information, an evaluation of process may well benefit from unstructured interviews with people using and not using the service. Such qualitative information often provides points of view that neither evaluators nor service providers had considered.</p>

<h3 id="measure-the-outcomes-and-impacts-of-a-program">Measure the Outcomes and Impacts of a Program</h3>

<p>If a study of implementation shows that a program has been implemented well and that people seek and secure its services, a measurement of the program’s outcomes may become a focus of an evaluation. An evaluation of outcome can take on several levels of complexity. The most elementary level concerns the condition of those who have received services. Are program participants doing well? Do they possess the skills being taught? A more challenging evaluation would compare the performance of those in the program with those not receiving its services. An even more challenging evaluation would show that receiving the program’s services caused a change for the better in the condition of those participating in the program (Boruch, 1997).</p>

<p>Although program managers hope that their programs will elicit positive changes in people, the causes of behavioral changes are difficult to pin down. For example, many people begin psychotherapy during a life crisis. If after several months of counseling they feel better, the change could be due to the counseling, the natural resolution of the crisis, or something else entirely. In a work setting, a change in procedures may result in better morale because of increased efficiency or because the workers feel that management cares about their well-being. Or perhaps an improved national economic outlook reduced the workers’ anxiety about possible  job loss. Discovering the causes of behavioral changes requires sophisticated techniques since an organization must continue to provide services while the evaluation is being conducted. Experienced evaluators are not surprised by tensions between gathering information and providing services.</p>

<p>Besides the limitations on the choice of research design, evaluators seeking to assess the outcome of a program often discover that people hold different opinions about what constitutes a successful outcome. A job-training program paying unemployed people to learn job skills may have been planned so that they can  later obtain jobs with private companies. City officials may view the training as a reward for people who have worked in local election campaigns, and the trainees may view the program as a good, albeit temporary, job.  An evaluator would not adopt just one of these views of the desired outcome of the program.</p>

<p>Assessing the maintenance of improvement creates another problem when evaluating outcomes. People leaving a drug rehabilitation program typically return to the same community that contributed to their problems in the first place. Despite their good intentions, people treated for alcoholism are often unable to withstand the influence of their peers. Changing long-standing behaviors is difficult. Although positive changes may be observed after a person’s participation in a program, the changes may be only superficial and disappear in a matter of months, weeks, or days. If so, was the program a failure? When positive outcomes do occur, their effects can influence other behaviors and even improve the condition of other people (e.g., children); such long-term outcomes are called “impacts.”</p>

<h3 id="integrate-the-needs-costs-and-outcomes">Integrate the Needs, Costs, and Outcomes</h3>

<p>Even when evaluators can show that a program has helped participants, they must also deal with the question  of costs. Just as a family must make choices in how a budget is spent, governments and agencies must select from among those services that might be supported. A successful program that requires a large amount of resources simply may not be a good choice if a similar outcome can be achieved with lower cost. If an  evaluator is asked to compare two or more programs designed to effect similar outcomes, efficiency can be assessed in a fairly straightforward manner; this would be called a cost-effectiveness analysis (Levin &amp; McEwan, 2001). Real difficulties occur when evaluators are asked to compare the efficiency of programs designed to achieve different outcomes, sometimes for different groups of people. For example, should a university spend funds to reduce alcohol abuse among students or to increase the number of tutors available? Although competing purposes are seldom contrasted in such a stark fashion, no organization can do everything that might be worthwhile; choices have to be made. Evaluations that incorporate high level integration of multiple elements have the potential to provide information to administrators to help them   make such choices (King, 2017).</p>

<p>Note that there is a logical sequence to these four general types of evaluations. Without measuring need, programs cannot be planned properly; without a plan, implementation would be haphazard; without effective implementation, outputs are not provided adequately and good outcomes cannot be expected; and without achieving good outcomes, there is no reason to worry about efficiency. A premature focus on an inappropriate evaluation question is likely to produce an evaluation with little value (Wholey, 2004).</p>

<h2 id="important-issues-in-evaluations">IMPORTANT ISSUES IN EVALUATIONS</h2>

<h3 id="time-frames-of-needs">Time Frames of Needs</h3>

<p><em>Short-Term Needs</em></p>

<p>Many programs are set up to help people in crises. For example, medical care is needed for injuries or illnesses, and emotional and financial support are often needed after a crime, a natural disaster, or a home fire. Some educational services are designed to meet specific needs of employees required to learn new skills. To be effective, such services should be available on short notice.</p>

<p><em>Long-Term Needs</em></p>

<p>To meet some needs, services must be available for a long period of time. Children need education for many years. Psychotherapy, treatment for chronic illnesses, training for prison inmates, and rehabilitation for alcoholism and drug abuse are examples of programs that focus on problems that cannot be resolved in a short time period.</p>

<p><em>Potential Needs</em></p>

<p>Ideally, potential problems can be averted. Preventive programs are supported so that problems can be avoided or at least postponed. The effectiveness of immunization, health education, or business security programs is judged by the degree to which problems do not occur. Clearly, evaluations of prevention programs must differ from evaluations of programs developed to relieve current problems.</p>

<h3 id="extensiveness-of-programs">Extensiveness of Programs</h3>

<p>Some programs are offered to small groups of people with similar needs, others are developed for use at many sites throughout a county or a state, and yet others are written into federal laws to be offered throughout a nation. Although tools useful in program evaluation apply to evaluations carried out at all levels, there are considerable differences between an evaluation of a day hospital program in Memorial Hospital and an evaluation of psychiatric services supported by Medicaid. Although it would be quite reasonable for these evaluations to use some of the same measures of adjustment, the decisions faced by the administrators of the local program differ so greatly from those faced by Medicaid managers that the evaluations must differ in  scale, focus, and type of recommendations.</p>

<p>The discussions throughout this text focus on evaluations of smaller programs likely to be encountered by evaluators working for school districts, hospitals, personnel departments, social service agencies, city or state governments, and so on. Although some national programs are mentioned, most readers of an introductory   text are more familiar with local government or educational programs than they are with national programs. Even when a program is supported by federal funds, implementation at a local agency differs markedly from those in other localities. The characteristics of programs are not like medications, which can be given in easily specified 20 mg doses. Thus, organizations with support from local governments, foundations, and federal agencies are all expected to carry out program evaluations and submit them to the funding source.</p>

<p>For studies done at a local level, statistical procedures seldom need to be particularly complicated partially because many administrators find statistics to be an incomprehensible—if not intimidating—topic. Complicated analyses require data from larger numbers of people than are usually available to evaluators working with local organizations. Evaluators can often show how to improve data summaries and presentations so that program information can be used to answer questions faced by program administrators.</p>

<h2 id="purpose-of-program-evaluation">PURPOSE OF PROGRAM EVALUATION</h2>

<p>There is only one overall purpose for program evaluation activities: contributing to the provision of quality services to people in need. Program evaluation contributes to quality services by providing feedback from program activities and outcomes to those who can make changes in programs or who decide which services are to be offered. Without feedback, human service programs (indeed, any activity) cannot be carried out effectively. Our bodily processes require feedback systems; similarly, feedback on behavior in organizations is also crucial for the success of an organization. Delayed feedback, not clearly associated with the behavior being examined, is not very informative.</p>

<p><a href="#_bookmark1">Figure 1.1</a> illustrates the place of program evaluation as a feedback loop for a human service program. Assessing needs, measuring the implementation of programs to meet those needs, evaluating the achievement of carefully formed goals and objectives, and comparing the level of outcome with the costs involved relative to similar programs serve to provide information from which to develop program improvements (Schermerhorn, Hunt, &amp; Hunt, 2008; Wholey, 1991) or to make wise choices among possible programs (Levin &amp; McEwan, 2001).</p>

<p>Feedback can be provided for different purposes. First, as noted earlier, evaluations can strengthen the  plans for services and their delivery in order to improve the outcomes of programs or to increase the efficiency of programs; these formative evaluations (Scriven, 1967) are designed to help form the programs themselves. Alternately, summative evaluations can help us decide whether a program should be started, continued, or chosen from two or more alternatives (Scriven, 1967). There is a finality to summative evaluations; once the value and worth of the program have been assessed, the program might be discontinued. In actuality, very few single evaluations determine whether or not a program will be continued (Cook, Leviton, &amp; Shadish, 1985). Many sources of information about programs are available to administrators, legislators, and community leaders. Because program evaluation is only one of these sources, evaluators are not surprised when clear evaluation findings do not lead to specific decisions based on the evaluation. Chelimsky (1997) uses the terms <em>evaluation for development</em> and <em>evaluation for accountability</em> to refer to the formative and summative purposes, respectively.</p>

<p><em>Figure 1.1</em> The place of evaluation as a feedback loop for a human service program.</p>

<p>Once a program has been carefully refined using feedback from formative evaluations and found to be effective using a summative evaluation, frequent feedback is still needed to maintain the quality of the program. This third form of evaluation is called “monitoring”; its purpose is quality assurance (Schwartz &amp; Mayne, 2005; Wholey, 1999). This can be as simple as checking the speedometer from time to time while on   a highway and as complicated as examining how international aid is used (Svensson, 1997). Monitoring critical process and outcome variables to verify that an effective program stays effective is a crucial activity after programs have been successfully implemented. Furthermore, monitoring can be expected to identify problems when the community or economic conditions change. As efforts are made to resolve those problems, monitoring feedback takes on a formative function as well. If problems cannot be resolved, monitoring can come to serve a summative purpose also.</p>

<p>Some evaluations are also carried out to learn about programs; Chelimsky (1997) uses the term <em>evaluation</em> <em>for knowledge</em> to describe these. This purpose is seldom part of the work of evaluators working on specific programs. Their work might be used by others to deepen our understanding of, for example, rehabilitation programs, but this occurs only after evaluations are completed and shared. Evaluators recognize the need to develop better understandings of programs and intervention strategies; however, this text does not focus on such types of evaluation because they involve more ambitious undertakings than most evaluators deal with regularly.</p>

<h2 id="the-roles-of-evaluators">THE ROLES OF EVALUATORS</h2>

<h3 id="principles-competencies-and-credentialing">Principles, Competencies, and Credentialing</h3>

<p>Along with the above points about components, types, and issues in program evaluations, there are additional ways to think about what makes for good evaluations and good evaluators. Although on one level, describing such expectations might seem straightforward, in fact, there is no simple answer to either of these two similar but not identical points. (Although good evaluators should be expected to produce good evaluations, assessing people and reports involve quite different processes.) Still there are a number of relevant details about the current state of the field in these areas, with both similarities and differences in approaches across the world. The main categories are Standards or Principles, Competencies, and Credentialing, and the following brief summary describes the state in fall 2017. For updates since this edition was published, see the eResources.</p>

<p>The American Evaluation Association (2017) has adopted “Guiding Principles for Evaluators” that cover five basic areas such as Integrity/Honesty, while the Canadian Evaluation Society (CES) has adopted standards that cover five perspectives such as Utility and Accuracy (Yarbrough, Shulha, Hopson, &amp; Caruthers, 2011). Like many organizational principles, many of the points are more aspirational than precise. For example, one of the specific AEA principles is that evaluators should “seek to maintain and improve their competencies in order to provide the highest level of performance in their evaluations” (American Evaluation Association, 2017, p. 313), which does not indicate how much improvement is enough. Still, the intent of the principles and standards is obvious, and blatant failures to follow them can be noted.</p>

<p>In some areas, the general agreement in the field has not yet crystallized into uniform policies. Proposals  for establishing a list of competencies are more than a decade old (King, Stevahn, Ghere, &amp; Minnema, 2001; Stevahn, King, Ghere &amp; Minnema, 2005) and current research in the field refers to these proposals favorably (e.g., Galport &amp; Azzam, 2017). The details fit well with both the principles and common practice, noting  crucial elements such as showing ethical approaches like respecting clients and considering public welfare, demonstrating knowledge about appropriate research methods, and being responsive to the specific context of  a given program (Stevahn et al., 2005). The Canadian Evaluation Society (2010) formally adopted a set of Competencies, while the American Evaluation Association has not done so at this time, although many evaluators continue to demonstrate their voluntary commitment to those values.</p>

<p>The CES also offers the designation of “Credentialed Evaluator” to CES members who apply for it and meet the criteria (Kuji-Shikatani, 2015). The possibility of a credentialing process in the United States and the related possibility of an accreditation process for programs that train evaluators continue to be raised and discussed (Altschuld &amp; Engle, 2015; King &amp; Stevahn, 2015; Kuji-Shikatani, 2015; LaVelle &amp; Donaldson, 2015; McDavid &amp; Huse, 2015; Seidling, 2015; Shackman, 2015). The trade-offs between more uniform quality on the one hand and flexibility to address substantially different needs and contexts on the other have not yet been resolved.</p>

<h3 id="comparison-of-internal-and-external-evaluators">Comparison of Internal and External Evaluators</h3>

<p>There are two primary ways evaluators relate to organizations needing evaluation services: (1) evaluators can work for the organization itself and evaluate a variety of programs in that setting, or (2) they can work for a research firm, a university, or a government agency to evaluate a specific program. In this book, the terms <em>internal evaluator</em> and <em>external evaluator</em> refer to these two types of evaluators, respectively. Although there are great similarities in what all evaluators do, the affiliation of an evaluator has implications for the manner in which evaluations are done.</p>

<p><em>Factors Related to Competence</em></p>

<p>In terms of knowledge about a program, internal evaluators have an advantage since they have better access to program directors and to the administrators of the organization. A person who is physically present on a routine basis is likely to see programs in action, to know staff members, and to learn about the reputation of programs from other people in the organizations. Such information is unavailable to an external evaluator. The more that is known about the actual operation of programs, the easier it is to ask relevant questions during the planning and interpretation of evaluations.</p>

<p>The technical expertise of an evaluator is important. An internal evaluator often works with a small group  of two or three; some evaluators work alone. An external evaluator, however, can often draw upon the resources of a greater number of people, some of whom are very skilled in sampling, qualitative analyses, or statistics. Some independent evaluators, however, essentially work alone and do not have access to such resources.</p>

<p>A different facet of the question of technical expertise is suggested by the need to perform evaluations of programs in different areas of an organization. Moving from topic to topic is stimulating; however, there is  also a risk that inexperience with the services being evaluated could limit an internal evaluator’s insight into aspects of some programs. By selecting external evaluators with experience in the type of program being evaluated, organizations might avoid errors due to inexperience.</p>

<p><em>Personal Qualities</em></p>

<p>In addition to technical competence, an evaluator’s personal qualities are also important. Evaluators can do more effective work if they are objective and fair, trusted by administrators, and concerned that program improvements be encouraged. Being well known and having been found worthy of trust (Taut &amp; Alkin, 2003), internal evaluators usually find program directors and staff more willing to devote time to an evaluation, to admit problems, and to share confidences than they would be with an evaluator not affiliated  with the organization. Being trusted increases the likelihood that a competent evaluator can fill an organizational educator role. Since credibility can be easily lost, evaluators avoid acting in ways that might suggest that they have allowed personal biases to influence their conclusions.</p>

<p>An internal evaluator can also be expected to want to improve the organization sponsoring the program   and paying the evaluator’s salary. External evaluators, being less dependent on the organization sponsoring the program, might not have the same level of commitment to working for program improvements. On the other hand, being dependent on the organization could affect the objectivity of an evaluator. Knowing program directors and staff members might make it difficult for an internal evaluator to remain objective (Ray, 2006; Scriven, 1997a); it is not easy to criticize a friend. An external evaluator is less likely to experience conflicting pressures when an evaluation reveals problems in a program. Internal evaluators might find it easier to remain objective if they remember that their value to an organization depends on their work being credible. Developing a reputation for tackling sensitive issues is somewhat easier when evaluators emphasize that most deficiencies in programs are due to system problems rather than personal inadequacies. Deming (1986), an  early champion of the quality revolution in industry, insisted that 85% of problems in workplaces are due to limitations in organizational procedures or work setting designs and 15% due to staff problems. Since many people mistakenly believe that evaluations focus on finding bad apples, evaluators should often refer to Deming’s point.</p>

<p><em>Factors Related to the Purpose of an Evaluation</em></p>

<p>Earlier in this chapter the purposes of evaluation were discussed—formative, summative, and quality assurance. Both internal evaluators and external evaluators can perform any type of evaluations; however, internal evaluators may have an advantage in performing formative and quality assurance evaluations. This seems true because such evaluations cannot prompt a decision to end a program. In fact, the internal evaluator’s rapport with managers and staff may encourage nondefensive communications, which are essential if avenues to improve programs are to be found. If, by contrast, a summative evaluation is wanted because support for a program might be withdrawn, it may be better if an external evaluator is used instead of an internal evaluator. For example, if a small elementary school district knows that it must close one of three schools, it may be wise to have an external evaluator perform an evaluation of the schools and select the one to close. No matter which one is selected, some residents will object. It could be hard for an administrator to serve in the school district if a sizable portion of the residents were dissatisfied with the decision; however, the external evaluator need never set foot in the school district again.</p>

<p>As should be clear, internal and external evaluators have many complementary strengths and weaknesses. Depending on the specifics of a given evaluation, one or the other may be preferable. A growing possibility in recent years is to develop a partnership between internal and external evaluators that has the potential to make good use of the strengths from both (Le Menestrel, Walahoski, &amp; Mieke, 2014).</p>

<h3 id="evaluation-and-service">Evaluation and Service</h3>

<p>An evaluator’s job falls somewhere between the role of the social scientist concerned about theory, the design of research, and the analysis of data (but for the most part uninvolved with the delivery of services) and the  role of the practitioner dealing with people in need (but seldom deeply interested or extensively trained in the methods of data collection and analysis). Evaluators are able to use the language and tools of the research scientist; however, they must also be sensitive to the concerns and style of the service delivery staff. In  addition, evaluators are called upon to communicate with organization administrators who have different priorities, such as balancing a budget and weighing proposals for expansions of services. Since the role of program evaluator is still fairly new, it is likely that the evaluator will at times seem out of step with everyone.</p>

<p>Participating in such a new field has advantages and disadvantages. The advantages include the intellectual stimulation provided by exposure to people serving various roles in human service settings and the satisfaction of seeing research methods used in ways that can benefit people. The disadvantages include being viewed as intrusive and unnecessary by some service delivery personnel. Instead of concentrating on problems, sensitive evaluators make sure program managers and staff have opportunities to talk about what goes well and what is rewarding in their work (Preskill &amp; Coghlan, 2003). Yet to demonstrate the value and worth of the program, effective evaluators must ask challenging questions and tactfully insist that answers be supported by data.</p>

<p>Of course, at times even the most skilled evaluator will be in conflict with an organization. One of the early leaders in evaluation was concerned that the perceived threat of evaluation would make it impossible to collect data of sufficient validity to draw useful conclusions (Campbell, 1983). By contrast, other observers (Lincoln, 1990; Vermillion &amp; Pfeiffer, 1993) predict that staff members themselves will more frequently use evaluation methods to develop improvements in programs. When evaluation systems are developed in ways which clearly distinguish between information collected for (1) the purpose of program improvement and (2) the purpose of individual performance appraisals for salary increases, conflicts among evaluation, service delivery, and administration may be minimized. That happy day has not yet come, but when evaluation is built into program management and all agree that program improvement is both possible and desirable, the potential for conflict will be reduced (Donaldson, Gooler, &amp; Scriven, 2002).</p>

<h3 id="activities-often-confused-with-program-evaluation">Activities Often Confused with Program Evaluation</h3>

<p>Sometimes it is easier to understand a concept when one understands what the concept does not include. Program evaluation is often confused with basic research, individual assessment, and compliance audits. Although these activities are valuable, when program evaluation work is confused with one of these other activities, the evaluation becomes all the more difficult to carry out.</p>

<p>Basic research concerns questions of theoretical interest, without regard to the information needs of people or organizations. By contrast, program evaluators gather information to help people improve their effectiveness, to assist administrators to make program-level decisions, and to enable interested parties to examine program effectiveness. Evaluators, of course, are very interested in theories of why a service might help its participants. Understanding theories helps in planning programs and selecting variables to observe. However, contributing to the development of theories can only be a delightful side benefit of a program evaluation. Evaluation findings should be relevant to the immediate or short-term needs of managers and   must be timely. If program staff members believe that evaluators are collecting information primarily to serve research interests, cooperation may well be lost.</p>

<p>Human service staff members often confuse program evaluation with the assessments made by educational psychologists, personnel workers, and counseling psychologists who administer intelligence, aptitude, interest, achievement, and personality tests for the purpose of evaluating a person’s need for service or measuring qualifications for a job or a promotion. These activities are not part of the work of a program evaluator. In program evaluation, information about the level of job performance, educational achievement, or health may well be gathered; however, the purpose is not to diagnose people, determine eligibility for benefits, or choose whom to hire or promote. Instead, the purpose is to learn how well a program is helping people improve on those variables.</p>

<p>Last, the methods and objectives of program evaluation differ from those used by program auditors examining government-sponsored programs to verify that they operate in compliance with laws and regulations. When Congress supports a program, it is important that the funds be spent as Congress intends. If the appropriation was for the enrichment of elementary schools, spending the funds for high school laboratories constitutes fraud. If 10,000 students were to be served, documentation of services for close to that many children ought to be available. Program evaluators are concerned that the program serve the right number of children, but in addition, evaluators would be particularly interested in how the services have affected the children.</p>

<p>The work of program auditors is closely related to accounting, whereas program evaluators tend to identify with education and other social sciences. Auditing and evaluation have moved toward each other in recent  years because neither can give a complete picture of a program (Hatry, Wholey, &amp; Newcomer, 2004). The change of the name of the GAO from the “General Accounting Office” to the “Government Accountability Office” illustrates this change. Nevertheless, if a program evaluator seeking to help a program make improvements in its services is viewed as a program auditor seeking to verify compliance with laws, it is easy to imagine that the evaluator’s purpose could be misunderstood.</p>

<h3 id="evaluation-and-related-activities-of-organizations">Evaluation and Related Activities of Organizations</h3>

<p>Some agencies may combine evaluation with another activity of the agency. Five functions that sometimes include evaluation are research, education and staff development, auditing, planning, and human resources.</p>

<p><em>Research</em></p>

<p>Some human service organizations routinely sponsor research directly with operating funds or indirectly through grants. For example, a police department was awarded a grant to study the effect of providing emotional support to victims of crimes. The social scientists responsible for such research would be excellent colleagues for an evaluator. University-affiliated hospitals carry on research; researchers working with psychiatric problems or emotional issues related to medical care deal with research problems sufficiently similar to those faced by evaluators to serve as colleagues.</p>

<p><em>Education and Staff Development</em></p>

<p>Some organizations have joined the functions of education and evaluation. There is a long-standing precedent for this marriage in educational psychology. New study materials and curricula are evaluated before being widely used by school districts. Universities have combined faculty development and program evaluation into one office. Businesses have developed major educational programs to train employees to qualify for more responsible positions; such programs need monitoring.</p>

<p><em>Auditing</em></p>

<p>As mentioned earlier in this chapter, some states have combined the functions of program evaluation and program oversight often carried out by state auditors. There are important differences in the assumptions and roles of auditors and evaluators, but there is precedent for coordinating these efforts in the inspector general offices in the federal and state governments.</p>

<p><em>Planning</em></p>

<p>Planning is an activity in which people with skills in program evaluation are needed because many service providers are not data oriented, feel uncomfortable constructing surveys, and may be unable to analyze the information obtained. Evaluators can contribute to the task of estimating the level of community acceptance. Sometimes evaluators can help planners simply by posing probing questions about the assumptions implicit in the plan or about the steps involved in its implementation.</p>

<p><em>Human Resources</em></p>

<p>Large businesses have offices of human resources. The office oversees all hiring, compensation, and professional development of employees. Policy development, compensation plans, training programs, and management effectiveness seminars all require evaluation if an organization is to be as effective as possible.</p>

<blockquote>
  <p><strong>CASE STUDY 1</strong></p>

  <p>Open Arms</p>

  <p>In May 2014, REACH Evaluation presented its program evaluation report of the Open Arms Children’s Health Dental Service, a program at Home of the Innocents. The report is a good example of a comprehensive evaluation. Open Arms involved on-site dental services for special needs children supported with funds from the Social Innovation Fund through the Foundation for a Healthy Kentucky and in partnership with the University of Louisville School of Dentistry’s Pediatric Dental Program.</p>

  <p>Meeting Needs</p>

  <p>The extensive three-page section “Documentation of Need” included a summary of research that “linked poor oral health to illness, chronic disease and early mortality” (p. 9), research on the wide span of health needs among foster children, and the comparative point that Kentucky “has the highest percentage of edentate persons (those who have lost all their natural teeth due to tooth decay or gum disease) ages 18 to 64” (p. 9) along with multiple additional points of evidence regarding the need that Open Arms was intended to address.</p>

  <p>Implementation</p>

  <p>The issue of what was actually done in the program was addressed at several points in the report. For example, copies of medical notes for dental visits for the same child before and after the program was initiated show enormous differences in detail—about two pages of very specific notes after compared to one line on a card before. In the “Early Lessons Learned” section, the progression of staffing strategies during the program was described, detailing how initial plans were adapted.</p>

  <p>Stakeholders</p>

  <p>The evaluators had contact with clients, client family members, and permanent staff as well as dental students and dental residents who received training as a part of the program. As with many evaluation reports, the category of “stakeholders” was not listed explicitly, but the perspectives of multiple stakeholders were clearly considered as a part of the evaluation. For example, although programs generally focus on clients, and clients’ perspectives were included, so were accounts of some parents of clients, who are also important stakeholders.</p>

  <p>Side Effects</p>

  <p>As with stakeholders, the report did not explicitly note that it found side effects. But with the stated goal of improving children’s dental care, the report’s note that having dental services provided on site saved staff time in several ways was an implicit description of an unanticipated side effect. Specifically, reduced time was needed to transport children to dental services because the services were on site, and they found greater efficiency when scheduling visits because the person planning the times for the dental services had access to information about children’s other activities, and was thus able to avoid potential conflicts more easily. No negative side effects were noted.</p>

  <p>Improvement Focus</p>

  <p>Suggestions by staff and clients for ways to improve the program were requested and listed. In addition, the evaluators showed an implicit focus on improvement by suggesting strategies for addressing one of the central challenges for the program—how to continue it past the initial period of outside funding.</p>

  <p>Outcomes</p>

  <p>As the primary goals of the program were fairly straightforward—serving 880 children a year, serving special needs children, and serving 80 dental students and 6 pediatric residents, the report appropriately presented direct counts of the relevant groups as the primary outcomes. For two 12-month periods (overlapping, because the available data covered only 21 months) 920 and 1,411 children were served. The 1,475 special needs individuals served during the 21-month period were described with appropriate categories, such as “Medically Fragile” or “Foster Care.” And the planned number of students and residents was met. The results of inferential statistical analyses showed a significant improvement in treatment acceptance over time, while also noting that the changes were relatively modest, partly because the scores were very high to begin with (a ceiling effect). They also found very much greater improvement in four groups of children (such as Medically Fragile and Refugee) than in four other groups (such as Outpatient/Nonmedically Fragile). Several summaries of qualitative data addressed issues not analyzed quantitatively, such as the improvement in the medical notes indicated above.</p>

  <p>Nuances/Mechanisms</p>

  <p>Although no formal statistical analyses were conducted to determine mediators or active ingredients, it was clearly noted that the attitudes and behavior of the staff, such as being “very relaxed” and being very much more patient than was expected, played a key role in the success of the program. In addition, the “variety of behavioral aids and techniques” (p. 4), designed to help with the challenge of providing dental services for special needs children, appeared to be another important component.</p>

  <p>Summary</p>

  <p>As noted at the beginning, REACH’s evaluation is a good example of a comprehensive program evaluation report. The report itself is a high-quality, 50-page booklet with pictures, graphs, and appendices for the tables reporting statistical analyses typically skimmed at best by most program stakeholders. As you will see through the rest of this text, there are many quite good program evaluations that do not address all aspects of MISSION as thoroughly or as well as this evaluation, yet still provide substantial information about the program to those who want to know as well as important feedback for those who wish either to improve the original program or adapt it to other settings.</p>

  <p><em>Sources</em>: REACH Evaluation, <em>Open arms,</em> Louisville, Author, 2014; B. Birkby (personal communication, September 13, 2017).</p>
</blockquote>

<blockquote>
  <p><strong>EVALUATOR PROFILE 1</strong></p>

  <p><strong>Ben Birkby, PsyD—Not Part of the Original Plan</strong></p>

  <p>When Ben Birkby was applying to doctoral programs in Clinical Psychology nearly 25 years ago, he was focused on preparing for clinical work with children and families. He recalls now that he knew very little about program evaluation, let alone imagined that it might become an important part of his career. Events would turn out differently than he expected, however. He enrolled in the PsyD program at Spalding University in Louisville, Kentucky. Having indicated that he wanted to be considered for financial assistance, his record of involvement in undergraduate psychological research caught the notice of Dr. Bob Illback, a professor at Spalding, who had developed a Program Evaluation specialty at REACH of Louisville, a local agency also providing clinical services to children and adolescents.</p>

  <p>Ben started work as an assistant to Bob for program evaluations on his first day of graduate school, and looking back, he notes that learning program evaluation was interwoven into his entire graduate program. He quickly realized that, unlike his peers, he was gaining valuable experience in this less-well-known part of the field, finding a crucial complement to his clinical work, and building an important foundation for a component in his career. Following graduation, Ben went to work at REACH of Louisville where, for nearly 20 years, he has worked clinically with children, adolescents, and families while also playing a major role in REACH Evaluation—an organization integrated with REACH of Louisville that is dedicated to improving services and bringing about organizational and community change through program evaluation. Over time, he has had a hand in nearly 100 evaluations of varying sizes and scopes and now helps lead a team of 11 staff—a long way from knowing almost nothing about the area.</p>

  <p>Two important aspects of Ben’s approach to clinical work are developmental and systems perspectives—paying attention to expected patterns of change over time and the recognition of multiple levels of perspective. Especially with adolescents, attending to and intervening with not only the individual but also the family, the school, and more, is essential. Ben said, “Program evaluation is critical to   my clinical work—it allows me to live out what I say is my professional identify as a systems clinical psychologist.” In addition, as he likes doing many different things and learning about different things, program evaluation is a good fit for those interests. As he has to learn and   do a wide range of things in evaluations, both as a clinical psychologist and personally, he feels he is “living his dream.” Finally, he noted  that he sees evaluators serving a crucial role; they are “the people in the background working to insist on reason and quality data in our programs.”</p>

  <p><em>Source</em>: B. Birkby (personal communication, September 18, 2017).</p>
</blockquote>

<h2 id="summary-and-preview">SUMMARY AND PREVIEW</h2>

<p>This chapter outlines the major questions and purposes of program evaluation: assess unmet need, document implementation, consider as many stakeholders as possible, look for positive as well as negative side effects, consider how to improve the program when possible, measure outcomes, and examine what can be learned about why the outcomes occur. Such activities are undertaken to help plan and refine programs, to assess their worth, and to make corrections in ongoing operations. Some ways in which evaluators fit into the activities of agencies are described.</p>

<p><a href="#_bookmark2">Chapter 2</a> deals with steps in planning an evaluation. Working with program staff and sponsors will often reveal that some people fear program evaluations; when an evaluation must have the cooperation of staff, it is necessary to allay these fears before beginning.</p>

<h2 id="study-questions">STUDY QUESTIONS</h2>

<ol>
  <li>One of the interesting aspects of program evaluation is its relevance to a variety of activities and agencies. Gather some examples of activities related to program evaluation from newspapers or news magazines. You might find material on medical care, education, or public policy in any area. The material might concern advocacy of new proposals or criticisms of current activities. Consider what information might illuminate the debates.</li>
  <li>Some people have suggested that while people want to believe that something is being done about social problems, most people don’t really care whether programs have measurable impacts on the problems. In other words, social programs are symbols, not real efforts. Consider the validity of such comments in the context of the reports you found in response to the first question.</li>
  <li>Illustrate how program evaluation activities could be applied in an organization with which you are familiar.</li>
  <li>List the advantages and disadvantages of making program evaluation a part of human service delivery systems. Save your lists; when you are finished with the course, consider how you might change them.</li>
  <li>Compare your ideas on evaluating the Sexual Assault Prevention Program mentioned in the introductory paragraph of this chapter with those of other students. What reasons lie behind different choices of outcomes considered relevant?</li>
</ol>

<h2 id="additional-resource">ADDITIONAL RESOURCE</h2>

<p>W. K<a href="https://www.wkkf.org/resource-directory/resource/2010/w-k-kellogg-foundation-evaluation-handbook">. Kellogg Foundation. (2010). <em>Evaluation handbook</em>. Retrieved from https://www.wkkf.org/resource- directory/resource/2010/w-k-kellogg-foundation-evaluation-handbook.</a></p>

<p>This work provides a helpful overview and succinct details of the program evaluation process. In addition, it highlights some related points such as the way that values and assumptions affect evaluation work. Best of all, like many resources on the web, it is free.</p>

<h1 id="2planning-an-evaluation">2 Planning an Evaluation</h1>

<h2 id="overview">OVERVIEW</h2>

<p>Preparation precedes production; it is often harder to plan well than to follow a good plan. Experienced evaluators know that the time put into planning an evaluation is well spent. Program evaluations begin in a variety of ways. Formal Requests for Proposals are sometimes published to invite a plan for an evaluation. Alternatively, program personnel may initiate an evaluation, the central administration or a funding agency may seek an evaluation, or public dissatisfaction might prompt government officials to call for an evaluation.</p>

<p>Regardless of who initiates an evaluation, there are a number of important tasks to complete. Evaluators need to become familiar with the nature of the program, the people served, and the goals and structure of the program, and, above all, learn why an evaluation is being considered. Evaluators seek to meet with program personnel, program sponsors, and other stakeholders, some of whom may question the need for an evaluation. Then, in consultation with these groups, evaluators may suggest how best to evaluate the program. These consultations may well involve negotiating the timing of an evaluation, the manner of conducting it, and its costs. Points such as whether the evaluator is internal or external to the organization will affect some of these processes such as how one learns about stakeholder perspectives and building relationships with stakeholders. Additional elements such as the population and problem(s) addressed by the program, the goals for a given evaluation, and other characteristics also affect details of preparing for an evaluation. Further tasks include deciding on the elements of the program to be assessed and on what data to use. Perhaps sufficient data have already been collected, or perhaps new data will need to be gathered as part of the evaluation. The analysis strategy needs to be determined as well as how and when and to whom reports of the evaluation will be presented.</p>

<p>It is important to understand that these tasks cannot generally be completed in a simple linear fashion— the results of some of the elements listed later may require returning to earlier elements. For example, talking initially with staff, an evaluator might plan to use data from client records to determine outcomes. But examining the records may show problems with that strategy, such as there being too many cases of missing data or that the information is not in a form to answer the evaluation questions. The evaluator would then meet with staff again to find a better solution.</p>

<p>One aspect of planning an evaluation, the choice of the overall approach to the evaluation, called the model (or sometimes a combination of models), generally occurs after some of the initial steps such as learning about the program and meeting with stakeholders. But models are conceptual guides, and the decision of which one  or ones to use is informed by a number of other elements in the evaluation process. A description of models  and a summary of the main options is presented first to clarify these conceptual points. Then the basic steps of preparing for an evaluation are described, most of which will be elaborated in greater detail in the later chapters. This chapter’s case study, in addition to providing another real-life example of an evaluation, also illustrates how models are choices made by evaluators that affect important aspects of evaluations. In the summary at the end of the chapter, a simple checklist of the main elements when preparing for an evaluation   is provided as a guide, especially for new evaluators. As might be expected, there are many relevant resources available online to help with various aspects of planning an evaluation. See the eResources for an annotated  list with active links to websites helpful for planning program evaluations. Looking further ahead, Chapter 3 addresses program theory and the selection of criteria for the evaluation, while Chapter 4 provides a detailed discussion of choosing specific measures.</p>

<h2 id="evaluation-models">EVALUATION MODELS</h2>

<p>There are many different approaches to program evaluation; Stufflebeam and Coryn (2014) list 23 different models, and at least 11 more have been suggested (Patton, 2016; Posavac &amp; Carey, 2007; Pouw et al., 2017; Sturges &amp; Howley, 2017; Thomas &amp; Parsons, 2017). Some are used more commonly than others, but all approaches have something to recommend them, otherwise evaluators would not have used them. Although some of these models of evaluation seem to differ greatly, in fact, the different approaches are all similar in being ways to shed light on the value and worth of programs through a range of methods or from different perspectives. The models differ primarily in the initial approach of the evaluator in gathering evaluation- relevant information. As there are far too many models to cover adequately in this printed text, the essential features of models are described, with explanations of specific models that illustrate four general approaches and a table listing some features of those models. An extensive list with descriptions and a correspondingly much larger table summarizing important features of those models is available in the Chapter 2 folder of the eResources.</p>

<p>Much like different theoretical orientations or research strategies, the models or approaches reflect different assumptions about important aspects of evaluations. Along the lines of the relatively simple natural evaluations described in the first chapter, one rather simple model is the BLACK BOX. Those following this model make few assumptions and take an almost stereotyped “bottom line” approach, concentrating on outcomes without reference to much else. Such an evaluation considers whether program participants are better off following the program than they were before without examining anything else about the process such as why they are better off or why some participants improve more than others. To be fair, some Black Box evaluations at least specify how much better participants need to be in order to count as successes, and some evaluators might say they are using a Black Box model, but still address other matters such as problems with implementation if they are substantial enough. But the simplicity of this model means that it is most commonly seen as a minimum standard to be exceeded (Mead, 2016; Peck, 2016; Solmeyer &amp; Constance, 2015). In general, this text presents a sustained rationale not to settle for a Black Box model.</p>

<p>In contrast, in many educational settings, especially accreditation of a program, evaluators must start by learning what that particular institution intends to achieve by looking at the specific objectives noted in formal documents—using an OBJECTIVES-BASED model. The goal of the evaluation is then to determine the degree to which the program achieved those objectives. In those settings, it is inappropriate for evaluators to criticize established objectives—they are taken as givens. For example, although many colleges and universities include philosophy as an important element of liberal arts studies, some colleges do not consider it to be an appropriate part of their educational plan. An accreditation visit using an Objectives-Based evaluation model would not describe the lack of philosophy classes as a problem at such a school, whereas an evaluation of a college that intended to teach philosophy but failed to do so would have the deficiency noted. To be clear, attending to organizations’ objectives is not only seen in Objectives-Based evaluations—all good evaluators should be aware of the stated objectives of the program. But whereas other models indicate additional bases  for determining appropriate outcomes, Objectives-Based evaluations start and stay with the organization’s  own goals.</p>

<p>A THEORY-BASED model explicitly includes elements that broader research has identified as important components of programs like the one being evaluated. At a basic level, this means that the selection of outcome measures is informed by current theoretical understandings of the relevant field, not just the specific objectives of the organization. For example, a basic understanding of Major Depressive Disorder includes impairment of subjective affect (feeling depressed) and the presence of physical symptoms such as greatly increased sleep and change in appetite. An agency that requested a Theory-Based evaluation for a program treating depression but wanted evaluators only to measure clients’ subjective feelings, and not physical symptoms, would face a conflict.</p>

<p>Theory-Based models typically also consider other elements indicated by current research, including portions of the interventions that are believed to be some of the mechanisms for change. Recall the example at the beginning of Chapter 1 regarding the counseling center’s program to reduce sexual assault. One focus of expected mechanisms might be training for women and men in behaviors intended to reduce their risk of   being sexually assaulted as well as measures of how frequently trained students show those behaviors. Another focus based in current theory would look at training for bystanders to intervene in situations with the potential for sexual assault, such as when some partygoers show signs of decreased functioning as a result of drinking alcohol (Hahn, Morris, &amp; Jacobs, 2017; Koelsch, Brown, &amp; Boisen, 2016; Senn &amp; Forrest, 2016). The explicit point of attending to theoretical insights in this model is also a common contribution that evaluators  can make to programs—many programs continue to use somewhat dated approaches, and good evaluators can help program staff learn about newer insights.</p>

<p>A model emphasizing a different perspective is the EMPOWERMENT model, sometimes called a CAPACITY-BUILDING approach. Whereas the above models address assumptions about the appropriate outcomes and mechanisms to be measured, an Empowerment evaluation stresses important aspects about the process of the evaluation, especially the interactions among evaluators and various stakeholders. Another way to summarize the Empowerment or Capacity-Building approach is that instead of primarily providing the product of an evaluation of a program, these evaluators provide the service of training agency staff how to evaluate their own program while working on the product of the evaluation. In contrast to an Objectives-  Based approach that would roughly adopt agency goals and outcomes, and also in contrast to a Theory-Based approach that would roughly draw appropriate goals and outcomes from research in the appropriate field, Empowerment approaches emphasize collaboration—evaluators and stakeholders work together to determine the goals and outcomes of the evaluation.</p>

<p>Figure 2.1 notes some of the basic points of these four models, which provide a window into a much larger set of models. For example, other models focus on the method of gathering information. One common technique is to use established assessments such as symptom checklists for treatment of mental disorders or knowledge as revealed on standardized educational tests. But other settings and evaluators consider that the stories clients tell will convey rich information about many relevant issues that cannot be reduced to numbers along a single dimension such as distress. In those cases, evaluators typically collect qualitative data. Several models are based on different emphases regarding data collection.</p>

<p><em>Figure 2.1</em> Characteristics of some basic evaluation models.</p>

<p>There are a couple of general points to make here. First, although a few models are incompatible with certain other models, most provide an emphasis that can be combined with one or more other approaches. To use the analogy of tools, many tasks require using a set of tools rather than just one. As valuable as a hammer may be for one part of a job, a screwdriver may also be needed for another part, and neither tool can substitute well for the other. Skilled evaluators are able to adjust to different settings or demands by employing a  different model or combination of models, even if they have their typical approach, just as people repairing things adjust to the specific needs by picking the appropriate tool or tools. Second, the choice of the model or models depends on a number of factors. The particular agency and program being evaluated, the reason for   the evaluation, the general skill set and approach of the particular evaluator, and other elements such as time frame and resources available may all affect the choice of the model. Hopefully it is clear that because so many elements are relevant, consideration of the desired model needs to be a part of the very early stages of any evaluation.</p>

<h3 id="an-improvement-focused-approach">An Improvement-Focused Approach</h3>

<p>Regardless of the other models chosen, however, this text presents an approach to evaluation in which program improvement is the overarching attitude. Improvements can be made in programs when discrepancies are noted between what is observed and what was planned, projected, or needed. Evaluators help program staff to discover discrepancies between program objectives and the unmet needs of the target population, between program implementation and program plans, between expectations of the target population and the services actually delivered, or between outcomes achieved and outcomes projected. Finding discrepancies is not a perverse trait of evaluators. If the point of evaluation is to improve the program—which  is nearly always the point—then discrepancies provide a place to seek to effect improvements. To learn how to improve a program, staff members need to discover what has occurred as planned and what has not. Do the clients have unmet needs that were anticipated or different ones? Do the staff members have the needed skills or has it been difficult to teach them the needed skills? Has the promised support materialized? Does the operation of the program suggest that the conceptual basis of the program is sound or that it can be improved? To deal with such questions, objective information is needed, but qualitative information can often help with interpretation. Personal observations provide direction in selecting what to measure and in forming an integrated understanding of the program and its effects.</p>

<p>The improvement-focused approach, it is argued, best meets the criteria necessary for effective evaluation: serving the needs of stakeholders, providing valid information, and offering an alternative point of view to those doing the really hard work of serving program participants. To carry this off without threatening the staff is one of the most challenging aspects of program evaluation. Further, this approach is not to be interpreted as implying that evaluators are not to celebrate successes. After all, evaluators are to look for merit and worth in programs. Managers of programs are going to pay close attention to evaluation reports when evaluators demonstrate that they recognize the strengths of programs, not just the ways in which the program has fallen short and can be improved.</p>

<p>As noted above, sufficient description of even most of the major models requires more space than available in this textbook. See the eResources for extended explanations and a summary table of models, their main characteristics, and the most obvious strengths and weaknesses. Further, new models introduced to the field since this edition was published will be added on at least a yearly basis.</p>

<p>The practical steps of planning a program evaluation are described next. Some of the steps will be different depending on specifics such as whether it is an internal or an external evaluation, while other steps will be similar across most settings.</p>

<h3 id="steps-in-preparing-to-conduct-an-evaluation">Steps in Preparing to Conduct an Evaluation</h3>

<p>In considering how to begin preparing for an evaluation, Sarason’s (1972) points about “before the beginning” apply both to the evaluation itself and to the program being evaluated. Interested readers will find Sarason’s chapter by that name still highly relevant. In brief, he cautioned against ignoring the context of any enterprise. Programs were created in specific settings because of particular forces and people. And although many of the motivations to create programs may have been based on increasing services for those in need, at least sometimes programs are created in reaction or opposition to other programs. Likewise, evaluations are not requested only because all stakeholders want to gain comprehensive insights to make services better. The point here is not to create suspicious evaluators looking for ulterior motives, but as a reminder that there may be important history to consider with regard to programs and evaluations.</p>

<p>With that point in mind, while planning an evaluation in an applied setting focused on emotionally sensitive issues—such as health, justice, and work success—it is necessary that the communication be clear  and agreements made be explicit. Oral agreements between evaluators and administrators should be followed up with memos outlining the discussions and the evaluator’s understanding of the agreements. Depending on memory is very unwise; evaluators need to recognize that since program administrators deal with many problems during each day, the evaluation merits the administrator’s attention during only brief moments during a week. Putting agreements on paper serves to remind all parties about the plans and is a record if disagreements occur later. Furthermore, reviewing developing plans described on paper can suggest implications that neither evaluators nor administrators had considered previously. A common reality late in   the process of an evaluation is regret that some details were not specified in writing earlier.</p>

<h3 id="identify-the-program-and-its-stakeholders">Identify the Program and Its Stakeholders</h3>

<p><em>Obtain a Complete Program Description</em></p>

<p>The first thing effective evaluators do is to obtain descriptions of the program. It makes quite a difference whether an evaluation is proposed for a new program or a well-established one; whether it is locally managed or offered at many sites; whether people seek to participate voluntarily or are assigned to complete it; whether participants are functional or suffer cognitive, emotional, or physical problems; whether it serves 25 or 25,000 people; and whether the program theory is well-articulated or based on common sense.</p>

<p><em>Meet with Stakeholders</em></p>

<p>The second thing effective evaluators do is to identify the stakeholders. Stakeholders are those people who are personally involved with the program, derive income from it, sponsor it, or are clients or potential recipients of the program’s services (Bryk, 1983; Sieber, 1998). Depending on the setting, people who live near program locations may be stakeholders.</p>

<p>Program personnel are usually more personally involved in the program than either the sponsors or the clients. The program director is a key person with whom the evaluators relate during the entire project. It   helps to learn as much as possible about the director’s training, program philosophy, vision for the program, and reputation. The people who deliver the services need to be involved as well. Such involvement should begin early so that they can add insights that only they can provide because only they know what happens   hour by hour. A consensus between the director and the staff about the purposes and methodology is also critical. Such cooperation increases the chances that all groups will cooperate in planning the evaluation, collecting data, and finally using the findings.</p>

<p>Program sponsors should be considered. At times program personnel are the sponsors; in other situations sponsors are foundations, government agencies, or central administrators of the institution housing the program. Often specific individuals are responsible for the evaluation. For example, a member of the school board or the vice president of a hospital might be the person who must be satisfied with the evaluation. It is important that evaluators meet with sponsors early during the planning phase to answer questions about the evaluation; however, such people will seldom be involved in detailed planning. As the evaluation proceeds, sponsors appreciate progress reports. Suggestions for keeping sponsors informed and helping sponsors use the information gained through the evaluation are given in Chapters 13 and 14.</p>

<p>The clients or program participants also need to be identified. The type of contact with clients will vary  with different types of evaluations and programs. Evaluations of services directed to whole communities might require contact with just a small sample of eligible residents, those of school-based programs might depend on work with parents, and evaluations of small programs might involve all participants in some way. Having a good understanding of the needs of participants is necessary because, after all, it is for their welfare that the program is being provided. Although these categories cover most common groups of stakeholders, there may  be others, depending on the specific program.</p>

<h3 id="become-familiar-with-information-needs">Become Familiar with Information Needs</h3>

<p>After learning about the program and meeting stakeholders, evaluators should ask: (1) Who wants an evaluation? (2) What should be the focus of the evaluation? (3) Why is an evaluation wanted? (4) When is an evaluation wanted? (5) What resources are available to support an evaluation?</p>

<p><em>Who Wants an Evaluation?</em></p>

<p>Ideally, both program sponsors and program personnel want the program evaluated. In such situations evaluators usually interact with cooperative people secure in their professional roles who want to verify that the program meets their expectations and who wish to improve or extend the program.</p>

<p>When the sponsors initiate an evaluation without explicit agreement by personnel, evaluators are faced  with helping the program personnel become comfortable with the goals and methodology of the evaluation before data collection begins. If this effort fails, evaluators face the possibility of active opposition or passive resistance; either way, they will not gain the cooperation essential to carry out the project (Donaldson, Gooler, &amp; Scriven, 2002). When program personnel treat an evaluation as a means of improving the effectiveness of their work, they are more likely to give the evaluators assistance in data collection and to volunteer valuable insights into the interpretation of the data.</p>

<p>Alternatively, if the program personnel initiate an evaluation, evaluators need to be sure that sponsors are convinced of its usefulness. Sponsors who are disinterested in the evaluation during the planning stages are  not likely to pay attention to the findings or to support improvements that might be recommended.</p>

<p><em>What Should Be the Focus of the Evaluation?</em></p>

<p>During meetings with the sponsors and personnel, evaluators often learn that the term <em>program evaluation</em>  does not have the same meaning for everyone. Although it would be better for them to think in terms of formative evaluations that would help them to retain positive features of the program and modify or improve other aspects of their work, sometimes program personnel merely seek affirmation of their current efforts. Program sponsors may want a summative evaluation if they are under pressure to divert resources to another program. Others might confuse program evaluation data collection instruments with measurements of individual accomplishments similar to employee appraisals (Bauer &amp; Toms, 1989).</p>

<p>At this point, evaluators help stakeholders to learn what types of evaluations best meet their needs and resources. The choice is seldom between one type of evaluation or another, but rather a matter of emphasis of two or more types. Incorporating some elements of various types of evaluations is often desirable, depending on the complexity of the goals of the program and the resources available for the project. A number of issues can be addressed in the overall evaluation such as the degree to which the target audience is reached, a program achieves the planned objectives, the achieved outcomes match the unmet needs that led to the program, and the benefits exceed costs. In addition, the specific aspects of the program that fostered achievement of the outcomes and the value of program outcomes can be examined as well.</p>

<p><em>Why Is an Evaluation Wanted?</em></p>

<p>Closely tied to the previous question is the issue of why evaluation is wanted. People seldom commission an evaluation without a specific reason. Effective evaluators put a high priority on identifying the reasons why an evaluation is wanted. Are there some groups in the organization who question the need for the program? Is   the foundation that provided the funds expecting an evaluation? What level of commitment is there to use the results of an evaluation to improve decision making? Ideally, program personnel seek answers to pressing questions about the program’s future. How can it be improved? Are we serving the right people? Should we expand? Evaluators expect that different stakeholder groups have differing priorities. Some want a smoothly functioning program, others want tips on making their work more effective, and still others want to see services expanded. One aspect of an evaluator’s role is to help both the sponsors and program personnel arrive at a consensus about the purposes of an evaluation.</p>

<p>Some reasons for evaluating programs are undesirable. For example, an administrator may use program evaluation as a ploy to postpone or avoid making a decision. Evaluations are also inappropriate when administrators know what decision they will make but commission a program evaluation solely to give their decision legitimacy. Effective evaluators attempt to help administrators develop better reasons, when possible.</p>

<p><em>When Is an Evaluation Wanted?</em></p>

<p>Stakeholders often want an evaluation completed quickly. Part of the planning process involves agreeing on a balance between the preferences of the stakeholders and the time needed for carrying out the best evaluation possible. When setting a viable completion date, evaluators must consider the time it takes to prepare a proposal, access records, arrange meetings, develop measures of the criteria for intermediate and long-term outcomes, find and observe a reasonable number of program participants and sites, and draw together the observations and prepare written and oral reports. Internal evaluators are often restricted in how many observations they can make because schools, hospitals, and businesses include only a limited number of students of a certain age, patients of a given diagnosis, or employees in a specific job. Furthermore, the type of evaluation requested leads to different constraints on the time needed to plan and complete an evaluation.</p>

<p>There is no formula to determine how long a project will take. With experience, however, evaluators develop a sense of the relationship between the scope of a project and the time needed for its completion. A good technique is to break the overall project down into small steps and then estimate how much time each  step in the project needs. Using the list of considerations in the previous paragraph might help in estimating  the time needed to complete an evaluation. On the other hand, the selection of a completion date might be specified in a request for proposals; in that case, evaluators adjust their proposed timetables to this date.</p>

<p><em>What Resources Are Available?</em></p>

<p>Besides time, another factor that can limit an evaluation is the availability of resources. Grants typically  include a specified amount for an evaluation. Of course, internal evaluators cannot be reckless with resources either. The assistance of program personnel can hold down the expense of data collection. Even if no formal contract is to be signed, listing in writing what is to be done as part of the evaluation is advisable.</p>

<p><em>Assess the Evaluability of the Program</em></p>

<p>After coming to understand the program and learning what information the stakeholders need, evaluators   must consider the resources available for meeting those needs. This process, called an evaluability assessment (Wholey, 2004), is intended to produce a reasoned basis for proceeding with an evaluation. It is best if the stakeholders agree on both the objectives of the program and the criteria that would reveal when successful outcomes have been achieved. If stakeholders cannot agree on what makes the program a success, evaluators are faced with a particularly difficult situation because they would need to provide a greater variety of outcome data and may be unable to draw clear conclusions.</p>

<p>A program is also not ready for a thorough evaluation until its conceptual basis has been developed. One review showed that less than 30% of published evaluations described programs with theoretical formulations linking the program to the desired outcomes (Lipsey, Crosse, Dunkle, Pollard, &amp; Stobart, 1985). Others agree that many programs do not have an explicit statement of how the program is expected to affect people (see Bickman, 2000; Cook, 2000; Leeuw, 2003). Upon questioning, it may turn out that some implicit program theories are no more complicated than “we tell them what to do, and they do it” (see reviews by Posavac, 1995; Posavac, Sinacore, Brotherton, Helford, &amp; Turpin, 1985). But knowledge about the health effects of regular exercise and a balanced diet low in fat, refined sugar, and salt is not sufficient to motivate most people. Many steps intervene between the provision of knowledge and actions; such steps can include social support, reinforcing appropriate behaviors, reminders, skill in applying knowledge, belief in the personal applicability of information, and so forth. Discussions with evaluators can reveal that planners had not developed a clear rationale on which to base a program or do not have the freedom to make changes in  response to an evaluation. Recognizing such situations even before gathering any data is a contribution that evaluators can make.</p>

<p>When deciding whether a program has a useful theoretical basis, it is helpful to construct an impact model showing how the elements of the program lead to the expected changes in the program participants (Lipsey, 1993; Renger &amp; Titcomb, 2002). Impact models are mentioned at several points in this text. In brief, an impact model is a diagram listing the activities making up a program on one side of a page and the outcomes the program is expected to produce on the other, with additional elements depending on the style of impact model. See  Figure 2.2 for an impact model of a hypothetical program to improve the lives of children in high- risk neighborhoods. The program is based on the idea that the children are not learning good conflict resolution skills and show limited respect for authority. Different assumptions would be reflected in a different impact model. The program components or activities are at the top. What happens during the program sessions, outputs, is listed next. The intermediate outcomes are given first; it is believed that they must occur in order for the ultimate outcomes to be achieved. Finally, at the bottom are the long-term, desired impacts.</p>

<p>Impact models, such as the one shown in Figure 2.2, help the evaluator to decide what to observe and measure. The best evaluations involve assessing the quality of the program’s activities (i.e., faithful and high- quality implementation), the degree to which the intermediate outcomes were achieved, and the extent to which the students achieved the outcomes at the end of high school. If one focused only on the objectives for students (the points in the lower section) and did not find improvements compared to the children who did not participate in the social learning program, one would not know what went wrong.  Figure 2.2 is a very simple impact model, but it still is valuable because it prompts the stakeholders to look at the elements of the program and trace the process by which the outcomes are expected to be obtained. It also tells the evaluator what to consider measuring. (See the eResources for other examples and the W. K. Kellogg Foundation Logic Model Development Guide, 2004 for a very helpful resource.)</p>

<h3 id="plan-the-evaluation">Plan the Evaluation</h3>

<p>Once it is decided to conduct an evaluation, planning begins in earnest. Examining published research and evaluations of similar programs, determining the best methodology, and preparing a written proposal complete the preparation phase.</p>

<p><em>Examine the Literature</em></p>

<p>When evaluators work in an area that is new to them, making a careful search of the literature before designing or developing new instruments is important. Evaluators can learn from the successes and failures of others and get a picture of the methodological, political, and practical difficulties that must be overcome. Searching published materials is much easier than in the past. The research literature can be accessed through Internet searches in libraries and even from home. For medical, mental health, and educational program evaluations, MEDLINE, PsycINFO, and ERIC are readily available. Many other specialized systems have   also been developed that have taken much of the drudgery out of literature searching. Once some reports are found, the bibliographies of these articles provide additional references.</p>

<p><em>Figure 2.2</em> Simple impact model for an after-school program for middle school children.</p>

<p>While reading reports of previous work, evaluators keep several key questions in mind. In what ways is the program evaluated similar to the program whose evaluation is being planned? What research designs were used? Can some of the measures of the outcome criteria be adapted? How reliable and valid were the measures? What statistical analyses were used? Is there a consensus among the reports? If there are conflicting findings, are these conflicts due to different community characteristics, compromises in selecting samples, variations in research design, or variations in how the programs were administered? What issues were not addressed?</p>

<p><em>Plan the Methodology</em></p>

<p>After reviewing the literature, the evaluators are ready to make decisions regarding sampling procedures, research design, data collection, and statistical analysis. Later chapters are devoted to some of these topics, but it is helpful to preview the main issues now to show how these decisions contribute to planning an evaluation.</p>

<p>Once the program’s target population has been identified, evaluators need to consider whether to use the entire population, a random sample, or a sample selected for other reasons. One argument for including the entire population is political: some people may be offended if they are not included. Evaluations within a large organization will be more readily accepted if all employees have an opportunity to participate rather than only  a random sample. If a state auditor’s office is evaluating a statewide program, gathering information from every county, or at least from every state representative’s district, would be wise.</p>

<p>When there are concerns about particular types of participants, it is wise to be sure that those types are represented in the sample. If a program is offered in different communities, the credibility of an evaluation is enhanced when evaluators take special care to be sure that participants from a variety of program locations are included. For example, an evaluation of a reading program in a large urban school system could be based on schools whose students represent different ethnic and economic backgrounds rather than just taking a random sample of students from all schools.</p>

<p>To minimize expenses and to complete an evaluation on time, it is often necessary to sample participants rather than observe an entire population. The strategy of selecting a representative sample and devoting resources to obtaining data from as many from that sample as possible produces a more representative sample than a halfhearted attempt to include the whole population would yield. For example, a 75% response from a random sample of 100 would provide more valid information than a 15% response from a sample of 1,000  even though the second sample would be twice as big as the first. Observers untrained in social science  research methods frequently emphasize a sample’s size rather than its representativeness; this misperception may require the evaluator to teach a little sampling theory to stakeholders.</p>

<p>No matter how sampling is to be done, evaluators know that some people will not cooperate and some cannot. Thus, it is advisable to seek a larger sample than is essential. Identifying the characteristics of nonparticipants and those who drop out at various points in time is important. Both the attrition rate and the reasons for attrition have a bearing on the conclusions the data can support.</p>

<p>Some evaluation models would not employ a representative sample, but only include those selected in accordance with the focus of the model. For example, in a formative evaluation, using a special sample of successful participants might permit one to develop insights into what pattern of program participation is needed to achieve success. It is critical to keep the purpose of the evaluation in mind when planning such details. Using such a selective sample is probably unsuitable for a summative or a quality assurance evaluation. Even when it makes sense to have a focused sample, unintended consequences need to be considered. Examining only those smoking cessation clients who attended all 10 of the workshops offered may fit one purpose of observing those who received a full dose of the intervention. But it also likely includes the most motivated clients, and their experiences are unlikely to be similar to all clients.</p>

<p>Another crucial step in selecting a methodology is choosing a research design. The choice depends on such constraints as the purpose of the evaluation, the preferences of the stakeholders, the deadline when the project must be completed, and the funds available. Some evaluations simply require a series of interviews conducted by a knowledgeable evaluator, others require a creative examination of existing records, and still others require a design similar to those used in basic research involving random assignment to program and control groups. Evaluators should remain flexible, selecting the methodology that matches the needs of stakeholders rather  than routinely using a favorite method. The evaluation project proceeds more smoothly when program managers, staff, and representatives of those providing financial support have participated in the selection of  the methodology and understand the reasons for the approach ultimately chosen.</p>

<p>Using multiple criteria from different sources yields the best information for an evaluation. The program participants themselves are one of the most important and accessible data sources. They can describe their reactions to the program and their satisfaction with it, and they can describe their current behavior. People close to the participants, such as family members, may sometimes provide information, especially when the participants are very young or incapacitated. Program personnel and records are often essential. For some programs, community-level variables are the most relevant. Last, evaluators must not overlook their own observations; this point is expanded in Chapter 8 on qualitative methods.</p>

<p>The day-to-day mechanics of data collection usually involve an on-site coordinator who keeps track of program clients, personnel, and other individuals supplying data for the evaluation. Surveys must be administered at appropriate times, addresses need to be updated, and relationships must be maintained with program staff. Because data collection is critical, yet sometimes tedious, a responsible person should handle  the task.</p>

<p>In quantitative evaluations, appropriate statistical analyses need to be completed. Using simple statistical procedures whenever possible is desirable because the findings will be presented to stakeholders who may not have a great deal of statistical expertise. Stakeholders ideally should be able to understand the interpretation of the results; they should not merely be impressed by statistical sophistication. One approach is to use statistical analyses that are as powerful and complex as necessary, but to illustrate the findings using percentages or— even better—graphs. Some ways in which analyses of evaluations differ from those that are used in basic research are mentioned at several points in later chapters.</p>

<p>During the planning phase, careful thought must be given to how an evaluation is to be reported to the stakeholders. Often evaluators have erred in focusing on a single report. Instead, a plan for reporting throughout the evaluation process serves the interests of the stakeholders and provides evaluators with constructive feedback as the project progresses. Chapter 13 contains an extended description of the variety of reporting avenues evaluators use to maintain support for the evaluation and to encourage the use of findings.</p>

<p><em>Present a Written Proposal</em></p>

<p>After reviewing the literature and thinking through the various methodological considerations outlined above, the evaluators are ready to prepare a written proposal. The acceptance of a proposal indicates that evaluators, program personnel, and perhaps the funding agency have agreed on the nature and goals of the program, the type of evaluation wanted, the criteria and measures to be used, and the readiness of the program for evaluation. It is psychologically and practically important for the program personnel to understand the evaluation process, to feel comfortable with it, and even to look forward to using the information that will be obtained.</p>

<p>Formal contracts are required for external evaluators. Such contracts will specify the tasks that the evaluator is to complete and any tasks for which the program staff is responsible. Items that went into the estimate of the costs should be specified. Internal evaluators cannot always insist on formal written agreements; however, listing agreements in memos with copies to program directors and central administrators helps in ensuring that the various stakeholders hold similar expectations for the evaluation. Expectations for an evaluation can be renegotiated if necessary. Using written communications ought not to   be interpreted as a lack of trust because honest misunderstandings occur when memories differ. Such differences have created problems after evaluations have been completed.</p>

<h2 id="dysfunctional-attitudes-making-an-evaluation-challenging">DYSFUNCTIONAL ATTITUDES MAKING AN EVALUATION CHALLENGING</h2>

<p>Political and emotional factors can undermine an evaluation project. Effective evaluators seek to identify these factors, discuss conflicts, and reassure stakeholders that the program evaluation is planned to serve their needs. Some concerns simply represent misunderstandings of program evaluation. Other concerns may reflect real conflicts within the organization that the evaluation has brought to the surface.</p>

<h3 id="our-program-is-exemplary">Our Program Is Exemplary</h3>

<p>Most people seem to think that they are better than average in lots of ways (Epley &amp; Dunning, 2000) even though on the average that cannot be. In a similar way program planners are generally enthusiastic and confident about the effects of their work; indeed, many expect their new program to have dramatic, positive results. Mendez and Warner (2000) showed that a goal published by the U.S. Public Health Service concerning the percentage reduction in adult smoking rates by 2010 was “virtually unattainable” because it was too ambitious. Program planners and staff might feel betrayed when evaluators find programs to be less than perfect. Difficulties are especially likely when a new program is expected to improve on a reasonably good existing program.</p>

<p>It may be necessary to assist the stakeholders in estimating the amount of improvement that is reasonable  to expect. For example, if elementary school children are reading at appropriate levels, a new reading program cannot raise reading levels very much. The new reading program might be considered successful if reading levels increase just a little or if students like reading better. Furthermore, a small improvement experienced by many people could be very valuable (Cook &amp; Shadish, 1994; Rosenthal, 1990).</p>

<p>There are other reasons why a dramatic effect seldom occurs. Whenever a group of people is compared  with a group of program participants, the comparison group will usually be receiving some kind of service. Program participants rarely can be compared with a group that receives no service at all. Similarly, when evaluators compare two versions of the same service, both of which are considered good, large differences between groups are unlikely. For example, if psychosomatic patients treated in a special unit are compared  with those being treated by their own physicians but without the benefit of the special program, both groups should improve because both are receiving treatment for their immediate problems.</p>

<h3 id="an-evaluation-will-offend-the-staff">An Evaluation Will Offend the Staff</h3>

<p>Evaluators recognize that no one likes to be graded, especially if those grades become public knowledge. In recent years, however, all sorts of published accountability information has been gathered and summarized. Even patient ratings of physicians are available on the Internet. It is critical that evaluators recognize that making program staff defensive will not contribute to a useful evaluation. Furthermore, evaluators need to remind people that program evaluation is not personnel evaluation. The evaluators focus on the <em>program</em>, not individuals who work in the program’s activities. Having made this assurance, it may be critical to note that  the idea of accountability and transparency has swept government and nonprofit organizations in the last decade.</p>

<h3 id="an-evaluation-will-inhibit-innovation">An Evaluation Will Inhibit Innovation</h3>

<p>Staff members may worry that evaluation will interfere with innovation by inhibiting them from experimenting with new techniques. In both process and outcome evaluations, the staff may believe that after beginning an evaluation no variation in the program is permitted until data collection is complete. This is partially true because major structural changes in the program could alter the essential goals or nature of the program. In such a case the evaluation would need to be revised. However, this does not mean that clinicians  or program personnel cannot be flexible in the day-to-day operation of the program within reasonable boundaries. Every program has some flexibility; evaluation will not limit that. However, it is wise not to attempt to conduct an outcome evaluation of a program that is just getting started; major changes can occur as staff members become clearer about their objectives.</p>

<h3 id="our-program-will-be-terminated">Our Program Will Be Terminated</h3>

<p>Although a negative evaluation seldom leads to a program’s cancellation (Cook &amp; Shadish, 1994), it is possible that an evaluation could result in the curtailment or elimination of a program when findings show that a given approach simply is not working as expected. However, before sponsors can eliminate a program designed to meet a specific problem, they are ordinarily under some pressure to decide what to put in its place. Therefore, an unfavorable evaluation will more likely result in the refinement of a program than its elimination.</p>

<p>Early in the planning phase, effective evaluators will try to have program personnel view them as partners who share the goal of providing quality education, service, or care. Evaluators can assist program personnel in meeting accountability standards that may be a condition for continued funding. One practice that allays some anxiety is to promise the program managers that they will see the final draft of the report and be asked for  their suggestions and clarifications (see Chapter 13). The comments of program administrators are appended  to many reports done by evaluation and audit offices of local and federal governments. Of course, evaluators cannot eliminate anxiety completely when a program does not have the full support of central administrators  or when there are valid concerns about the quality of the program.</p>

<h3 id="the-information-will-be-misused">The Information Will Be Misused</h3>

<p>Besides the fear that the program will be canceled, there might be some concern that information gained about the performance of the staff may be misused. Even competent clinicians, administrators, teachers, and other personnel are concerned about merit reviews, future promotions, and career advancement. It is absolutely necessary to keep formative program evaluations distinct from assessments conducted to reward the more effective staff members. This point has been emphasized by writers in education, medical care, and industry. However, staff members realize that they probably cannot control the information about the program that will be available to administrators of the organization housing the program.</p>

<h3 id="evaluation-methods-are-less-sensitive-than-perceptions-of-staff">Evaluation Methods Are Less Sensitive Than Perceptions of Staff</h3>

<p>Service personnel rightly feel that their observations are valuable sources of ideas both for improving the functioning of a program and for evaluating its effects. They may feel that the evaluators’ questionnaires, complicated research designs, and statistical techniques are less sensitive than their own observations and evaluations. At times they are right.</p>

<p>Although the subjective evaluations of the staff can be biased (see Dawes, 1994), the ideas of program personnel are a very valuable source of evaluation data. Evaluations can be improved by both quantitative and qualitative data gathered from many sources. The staff’s subjective observations will be very important when the data are being interpreted. The ideal strategy is to integrate the findings from both methodologies as described in Chapter 8.</p>

<p>Evaluators gain the confidence of managers and staff not only by recognizing this problem but also by articulating this awareness in such a manner that program staff members are reassured that the richness of human services has been appropriately respected (Taut &amp; Alkin, 2003). Early in the planning phase, program personnel can be assured that the evaluation will not begin until they have had the opportunity to review the evaluation proposal carefully and are confident that their concerns have been addressed properly.</p>

<h3 id="evaluation-drains-program-resources">Evaluation Drains Program Resources</h3>

<p>The concerns described so far are focused on various aspects of evaluation but not on the concept of evaluation itself. Some objections to evaluation strike at the very idea of program evaluation. The staff may charge that program evaluation drains money and energy away from direct service (Botcheva, White, &amp; Huffman, 2003). As the statement stands, it is true. However, a more critical issue is whether evaluation can improve service in the long run. The alternative to spending money on evaluation is to risk spending money on services that are  of unknown value. Today it would be hard to find a program funded either by government agencies or by private foundations that is not required to evaluate its services. Those who are not convinced by the accountability argument may be persuaded by a more pragmatic one: evaluations, if done well, may help in attracting more support and resources into the program. Furthermore, a favorable evaluation could lead to additional settings adopting an effective program.</p>

<h3 id="evaluations-have-little-impact">Evaluations Have Little Impact</h3>

<p>Some critics point out that evaluations frequently have had very little impact on programs. There is a good  deal of validity to this objection; evaluators have often been frustrated by seeing their reports set aside. However, evaluators should remember that when evaluations are used, their results will be only one of many factors behind decisions. Because evaluators work in a complex organizational context, the results of their  work must be timely and relevant to decision making. Well-designed and carefully executed studies are valuable only when they address the issues that are important to the organization. When evaluators show how evaluation is relevant to pending decisions, they raise the odds of the evaluation being used.</p>

<h3 id="the-effect-of-these-attitudes">The Effect of These Attitudes</h3>

<p>Stakeholders who fear evaluations of their work might attempt to determine the methods to be used, urging evaluators to use marginally valid methods that are likely to produce a desired result. Ideally, discussions among stakeholders and evaluators will reveal that better procedures are available and should be used. Open discussion ought to reduce or resolve unreasonable fears; however, evaluators ought not to ignore the threatening nature of evaluation. At times, an organizationally powerful stakeholder may seek to control an internal evaluation. One way for an internal evaluator to resist that influence is to seek outside support in the form of a review panel.</p>

<blockquote>
  <p><strong>CASE STUDY 2</strong></p>

  <p>Change 4 Good</p>

  <p>For over 10 years, Regina Langhout, PhD, a professor at the University of California at Santa Cruz, along with a number of her graduate students (including Jesica Fernández) and undergraduate students, have engaged in a Participatory Action Research (PAR) project called “Change 4 Good.” One aspect of the participatory nature of the project is that the goals and therefore the details were established by the various collaborators, the university researchers, and the fourth- and fifth-grade students in a nearby elementary school during the early stages—see especially the Meeting Needs section. In this case, “Students set a goal to increase the school’s connection to themselves and their families” (Langhout &amp; Fernandex, 2015, p. 198), with one major component being the creation of a mural that would present an image of the school that was better connected to students’ stories.</p>

  <p>This case particularly illustrates two relevant points. First, many aspects of evaluations are affected by the evaluation model or models chosen—in this case, the Empowerment and Capacity Building models. One clear example of the Empowerment model is that the project includes evaluations by the elementary students as well as by themselves. A number of other examples appear in the following sections. Second, along those lines, many good evaluations and their reports show important differences with other evaluation reports. For example, this evaluation included much more extensive comments about stakeholders, specifically using that term, than many other evaluations. Readers will likely notice other points as well.</p>

  <p>Meeting Needs</p>

  <p>A creative method, photovoice, was used to identify needs; this involved students taking pictures in response to a prompt about their hopes and then building on those pictures. The nature of the need identified fits the project and evaluation approaches—the students focused on a system problem, that “the school was not connected to them and their families, and therefore, they did not have a sense of belonging” (Langhout &amp; Fernandez, 2015, p. 198), rather than on an individual problem such as some students not being engaged enough in the school.</p>

  <p>Implementation</p>

  <p>The article documents many points in detail about the actual implementation. These elements illustrate aspects of a formative evaluation—insights are used to make adjustments to the program to address problems or weaknesses. For example, the project involved creation of a mural at the school, but when students evaluated the project, they “determined that their peers did not feel very connected to the first mural” (Langhout &amp; Fernandez, 2015, p. 203), so they decided to work on a second mural, making sure that students felt more connected to it.</p>

  <p>Stakeholders</p>

  <p>Consistent with the principles of empowerment evaluation, the report not only described the many relevant stakeholders, it also detailed multiple efforts to engage various groups of stakeholders in both the project and its evaluation.</p>

  <p>Side Effects</p>

  <p>In some ways, the broad nature of PAR and an Empowerment/Capacity Building evaluation means that most results fit more into the realm of desired outcomes than side effects. Still, a positive side effect was that involvement in the project provided the setting for children to demonstrate capabilities and performance that was less obvious than established academic assessment. A negative side effect was that some of the children found the difficulties of creating systemic change very discouraging.</p>

  <p>Outcomes</p>

  <p>Major points such as the creation of two murals, a book about one of the murals, and a documentary about the other are clearly documented outcomes. Fitting the Capacity Building purpose, they noted that the students adjusted their approach to collecting questionnaires and then were successful in reaching their target number, which illustrated the outcome of having the participants gain evaluation skills.</p>

  <p>Nuances</p>

  <p>Looking more at understanding various aspects of the events than simply identifying some mechanisms responsible for specific outcomes, the students looked at different responses to the mural by younger compared with older students, and students as compared with teachers. For example, they noted that the students liked elements of the mural that illustrated struggles more than teachers liked them, drawing the conclusion that students felt the images of struggles were particularly relevant to their own experiences.</p>

  <p><em>Sources:</em> Kohfeldt, D., Bowen, A. R., &amp; Langhout, R. D., <em>Revista Puertorriqueña de Psicología</em>, 27, 2016; R. Langhout (personal communication, September 21, 2017); Langhout, R. D., &amp; Fernandez, J. S., In D. M. Fetterman, S. J. Kaftarian, &amp; A. Wandersman [Eds.], <em>Empowerment evaluation: Knowledge and tools for self-assessment, evaluation capacity building, and accountability</em>, Thousand Oaks, Sage, 2015.</p>
</blockquote>

<blockquote>
  <p><strong>EVALUATOR PROFILE 2</strong></p>

  <p><strong>Regina Langhout, PhD—Scholar-Activist</strong></p>

  <p>Regina Langhout, PhD, is a community psychologist and social justice activist, or scholar-activist. A scholar-activist often works and advocates with those who are marginalized in various ways. As one example of her thoroughgoing and passionate commitment, she strives to work with all others in respectful, transparent, and engaging ways, from the elementary-school children in Change 4 Good to the school staff and administrators to her university students.</p>

  <p>Given that a primary goal for her project and for the evaluation was Empowerment, it is not surprising that one focus was the children’s growing ability to make sense of complex information and to act based on their understandings. An important part of that focus involved helping students learn not only how to run analyses such as t-tests with their data but also how to understand how the tests worked and what the results meant, along with taking responsibility to collect and manage the data. In keeping with the goals of Capacity Building, these elements meant that the students participated substantially in the evaluation rather than simply providing the raw data for her to analyze and use. The meta-message that she trusted the students would not have been lost on them, even if they might not have used such terms. These kinds of priorities are evident throughout her work and writings.</p>

  <p>But Regina has also been deliberate in working to become even more engaged with as many collaborators and stakeholders as possible, noting various challenges when balancing expectations and values of multiple stakeholders. Specifically, she notes that one approach to such authentic work with school staff can be described as being a “Critical Friend”—a way to describe the challenging balance of being supportive but also being as honest as possible about weaknesses as well as strengths. And her lab meetings with her graduate students include time to acknowledge and address students’ and her own life contexts from physical health to other stressors and celebrations as well as the focused business of research projects.</p>

  <p>Although she does not identify primarily as a program evaluator, evaluation is clearly one critically important tool and approach that serves her commitment to scientific rigor while working for justice. And those long established in the field as well as those just entering it will find inspiration in her example of paying attention to her heart and to the passions and the values that sustain us (see Langhout, 2015).</p>

  <p><em>Sources:</em> Langhout, R. D., <em>American Journal of Community Psychology</em>, 55, 2015; R. Langhout (personal communication, September 21, 2017).</p>
</blockquote>

<h2 id="summary-and-preview-1">SUMMARY AND PREVIEW</h2>

<p>Careful planning of an evaluation project serves to get an evaluation off to a good start. Note that the steps are suggested to help evaluators respond to the needs of the people most affected by the evaluation. New evaluators may find the basic checklist in  Figure 2.3 to be a helpful guide for their initial projects. Responsive evaluators who cover the areas comprehensively have fewer problems with the fears outlined in the second part of this chapter than do evaluators who seem to ignore stakeholder needs.</p>

<p>The next chapter focuses on a central concern—specifying the implementation and outcome criteria of wisely planned and successful programs. A thoughtless selection of criteria will negate efforts to conduct a useful evaluation.</p>

<p><em>Figure 2.3</em> Basic checklist of program evaluation steps.</p>

<h2 id="study-questions-1">STUDY QUESTIONS</h2>

<ol>
  <li>Apply the evaluation planning steps to the hypothetical sexual assault prevention program mentioned at the beginning of Chapter 1. Make a list of what needs to be done if you were to contemplate evaluating such a program.</li>
  <li>What models of evaluation would you think are most appropriate for the program to prevent sexual  assault on campus?</li>
  <li>One can think of a college course as a program. (In fact, a 3-hour class requires more time and energy  than many social service programs.) The elements of the program consist of (a) reading assignments, (b) class discussions, (c) lectures, and (d) projects such as term papers. The outcomes that university and future employers value include (a) the mastery of content; (b) the development of information management skills such as writing, analysis of arguments, and synthesis of information; and (c) the development of meta-skills such as organization, self-discipline, and maintaining good interpersonal relationships. What are some needed intermediate steps if the elements of the program are to lead to desired outcome skills?</li>
  <li>Imagine that you are part of an evaluation team in the institutional research office of a large university. The chairperson of the department of psychology asks you to evaluate a graduate program called “Community Psychology.” List the stakeholders. What are the first questions to ask the chairperson?  What models of evaluation seem most appropriate to such an evaluation?</li>
  <li>What models of evaluation are implied if an administrator asks an evaluation office to conduct an evaluation of a program specifying that the program manager and staff are not to be aware that the evaluation is being done?</li>
</ol>

<h2 id="additional-resource-1">ADDITIONAL RESOURCE</h2>

<p>Card, J. J., Greeno, C., &amp; Peterson, J. L. (1992). Planning an evaluation and estimating its cost. <em>Evaluation &amp; the Health Professions</em>, 15, 75–89.</p>

<p>When asked to estimate how much to charge for conducting a program evaluation, most new evaluators have little to go on. When pressed, most people will greatly underestimate how much time is necessary to complete even a very modest project. Although this is an older resource, these authors suggest how long it is likely to take to complete various steps in the process. Evaluators making a cost estimate need to remember that they ought to be paid for the initial discussion and the preparation of the proposal itself. There are many sites on the Internet referring to costs (just enter “program evaluation cost” into a search engine); however, it was hard to find one that provided estimates of the time needed as this source provides.</p>

<h1 id="3developing-and-using-a-theory-of-the-program">3 Developing and Using a Theory of the Program</h1>

<p>What is the best measure of a good intercollegiate athletic program? Some would say the proportion of games won; others would say the number of tickets sold; others, the devotion of the alumni to the team; others, the success of the student athletes after graduation; and still others, the favorable media attention attracted to the university through its teams. Some college presidents and faculty have argued that the criteria of a good sports program have shifted too much toward winning and away from the academic and life successes of the athletes after graduation. In fact, at some well-known schools, only small percentages of basketball and football players graduate. For the few who become professional athletes, failing to complete college may be a small matter, but for the others, not graduating limits their future vocational opportunities. Some people argue that this is irrelevant; basketball and football proceeds earn enough to support the entire athletic program, and, since no one compels the students to participate, the academic administration should not disrupt a good thing.</p>

<p>Differences about the desired criteria also lead to varied personal choices. If we test-drive a car, we use degree of comfort, ease of operation, clear vision of the road, perhaps what our friends think is cool, and other criteria, even though we have never written those standards down. When people disagree over the desirability of different automobiles, it is often because they are using different criteria to make their choices. For some people, an attractive exterior design is more important than a reputation for reliability; for parents with several children, a larger rear seat would be more important than it is for single people holding their first jobs.</p>

<p>Whether we think about them or not, we do have criteria that permit us to make distinctions and to form preferences as we make our choices of foods, friends, churches, automobiles, and politicians. When conducting an evaluation of a program or a product, we need to develop or select criteria and standards in a fashion that is far more explicit than we need for daily life, and those criteria are best when they are related to a theory appropriate to the program. Without the development of clear, appropriate criteria and specific standards, we might never be able to agree on the value of a counseling program for students who fear math, food stamps for poor families, or training for people who have lost jobs. As mentioned in Chapter 1, the traditional model of evaluation had been a subjective assessment by the people offering or funding services. Such informal assessments are no longer acceptable, especially when programs are paid for with public funds. This chapter describes the need to develop a program theory and then how to use that theory or a related model to choose the criteria that are to be measured and that will indicate successful implementation and desirable outcomes.</p>

<h2 id="developing-a-program-theory">DEVELOPING A PROGRAM THEORY</h2>

<p>Disagreements over criteria as illustrated above often may simply reveal disagreements over the purpose of a consumer choice or a program. Whenever people develop service or intervention programs, assumptions about the causes of the problems and the best ways to change problem behavior are made. Unfortunately, these assumptions are often implicit (Cook, 2000; DeFriese, 1990; Lipsey, 1993; Posavac, 1995; Posavac, Sinacore, Brotherton, Helford, &amp; Turpin, 1985). When there is no explicit statement of the theory behind the development of an intervention or when there is no conceptual framework linking the interventions to the projected outcomes, it is hard to agree on the criteria indicating program effectiveness, conduct an effective evaluation, or improve the intervention (Pion, Cordray, &amp; Anderson, 1993).</p>

<h3 id="why-a-program-theory-is-helpful">Why a Program Theory Is Helpful</h3>

<p>Initially, many evaluations were carried out without an explicit statement of the theory behind the program design. Nearly 30 years ago, DeFriese (1990) lamented that a sizable portion of the descriptions of proposed health-related treatment programs did not include credible descriptions of how the interventions are supposed to work. The value of articulating the logic of programs offered in community mental health agencies has been demonstrated (Yampolskaya, Nesman, Hernandez, &amp; Koch, 2004). Increasingly, theoretical foundations are expected in evaluations (Rog, 2015) as it has become clear that program design is less effective without theory and evaluations are less informative when carried out in a conceptual vacuum, although there is clearly room for further improvement in actual practice (Munter, Cobb, &amp; Shekell, 2016).</p>

<p>Specifying the theory behind the program provides assistance for planners, staff members, people responsible for obtaining funding, as well as for evaluators. In a comment on social welfare programs Etzioni noted “we know that it is extraordinarily difficult to change habits, personality traits, culture, and social institutions” (1994, p. 14). Yet governments and agencies frequently propose policies that are based on the assumption that interventions can motivate people to make major changes in their lifestyles in short periods of time. We act as though threatening shoplifters with jail will keep them from taking clothes from stores; telling patients with diabetes to lose weight will lead them to do so; showing teenagers how to recycle soda cans will stop them from discarding cans on the beach. Information is important; without knowledge of adaptive behavior people cannot change. Yet, there is ample evidence that such information alone is very weak compared to the major influences in life, such as peer pressure, family practices, media models, and simple convenience. Across many kinds of activities, it is clear that people differ in how ready they are to acknowledge problems and to take some action to change (Prochaska &amp; Norcross, 2001). Sometimes evaluators when working with a program planning committee can lead the planners to clarify their assumptions and implicit theories and perhaps refocus their plans. Perhaps the plans should focus on more realistic objectives rather than spreading program funds too thin by attempting too much.</p>

<p>A second value of thinking theoretically is that we can identify the intermediate results of a program rather than just the final outcome. We are also reminded that different people might respond to different forms of a program. Teenagers might respond if a music or sports figure endorsed a program; on the other hand, business managers might want to see a financial approach to a social concern. Before participating, most  people need to learn how to visualize themselves carrying out the recommended behaviors. A neighbor was overheard arguing with her husband about a newly announced community recycling program. Citizens were   to place cans, bottles, and newspapers into orange-colored containers on Friday mornings. She was not enthused and asked her husband, “Are you going to sort through the garbage every night?” For decades she  had put apple peels, the morning newspaper, coffee grounds, and empty soda cans into the kitchen garbage basket. Although keeping recyclable items separate from other garbage may have seemed to be a small matter to planners, the idea of separating items before putting them into the “garbage” was initially novel for some people. Publicity about a recycling program should include illustrations of how to participate in convenient ways. Describing the processes for people to follow in making changes in their lives can lead to more effective programs.</p>

<p>A third benefit of having a program theory is that theory helps us to know where to look in conducting an evaluation. Several decades ago, Lipsey and Pollard (1989) remarked that adopting “the basic two-step” in program planning and evaluation would improve common practice. Planners can specify (a) what should happen right after participation in a program as a step toward change and (b) a more ultimate outcome behavior that reflects a goal of the program. Evaluators would then develop methods to measure both. Learning about success or failure while the program is being provided is more informative than learning years later that the final outcome was or was not achieved. Particular attention would be paid to why the program did not lead to the intermediate result; perhaps additional resources were needed or the original theory was not valid. If the intermediate outcome was achieved but the final outcome was not, one possibility is that strong nonprogram influences were limiting the achievement of the desired ultimate outcomes, while another possibility is that the theory is wrong. Collecting information that could indicate which possibility is correct would follow from the theory and would be extremely useful to the program.</p>

<h3 id="how-to-develop-a-program-theory">How to Develop a Program Theory</h3>

<p>It is easier to agree that a program theory should be developed than to develop one. There are a number of approaches to developing a program theory and differences of opinion over the detail required in preparing a model of the program. Not only do some argue for more complicated and complete models (Renger &amp; Hurley, 2006), Gates and Dyson (2017) noted that recent approaches to understanding causality mean that evaluators have a higher bar to clear. Still, even simpler models are better than none (W. K. Kellogg Foundation, 2004).</p>

<p>In any case, good evaluators talk first with the staff of the program. At times the staff will have a fairly clear idea of how the activities making up the program affect the participants and how the intermediate stages lead to the final desired outcomes. Often, however, staff members are surprisingly vague. The rationale for some interventions may turn out to be as simple as “we have always done it this way,” and there may be no clear expectation of what initial steps lead to the final goals. In such cases, evaluators then turn to the research literature on similar programs. The theoretical underpinnings of similar programs that are producing good outcomes can be examined for relevant insights.</p>

<p>The research literature contains two kinds of material that might prove helpful. First, evaluations of similar programs might describe program theories. For example, peer leadership might lead to favorable outcomes more consistently in antismoking campaigns in junior high schools than do teacher-led classes (see Evans &amp; Raines, 1990). Since junior high children are especially sensitive to the attitudes of peers, program planners might try to use peers in any program that attempts to influence adolescents (Turner, 1999). In evaluations of energy conservation programs, evaluators have noted that electricity use is not easy for people to monitor; few people ever look at their electric meters and, when they do, it is hard to relate the spinning disk to energy use. On the basis of these observations, programs have been developed to provide new ways to give people feedback about their use of power in terms that are understandable (Seligman &amp; Finegan, 1990).</p>

<p>Examining basic research is a second way to develop program theory. Social support is often believed to be related to health and positive adjustment. Approaches to help people obtain social support are discussed in social and clinical psychology research studies (e.g., Glasgow et al., 1997). Some of those ideas might be appropriated to enrich the theory of some social service programs. Unfortunately, writers of basic research  often provide only brief descriptions of the independent variable, that is, the treatment, thus making it difficult to apply their work to other settings (“The trouble with dependent variables,” 1990).</p>

<p>Throughout this process evaluators must remember that they cannot just pick a theoretical model for the staff. Instead, evaluators can learn what model or models are implicit in the program and work with staff to examine the relevant elements. This is an iterative process and may take some time. It is extremely important that evaluators learn what is intended to happen to the program completers, how they are to change initially, what program experiences are to lead to those changes, and what activities the program must contain. Without specifying those steps, evaluators cannot choose what to observe and no one will know what the information gathered showed about the program.</p>

<p>A particularly good, classic example of the development of program theory was provided by Cook and Devine (1982), who described the expected processes whereby a psychoeducational intervention would help postsurgery patients recover more quickly and experience fewer side effects. Figure 3.1 has been adapted from Cook and Devine to illustrate some of the processes that connect the nurses’ teaching to the desired outcome, having patients able to leave the hospital sooner without ill effects. Note that there are numerous criteria that may be observed by someone evaluating the program; it is wise to include many intermediate variables, both those believed to be relevant and some likely alternatives, as well as the final outcome criteria of program success.</p>

<p><em>Figure 3.1</em> Illustration of an impact model showing the intermediate steps that are expected to be observed between the intervention and the desired outcomes. (Illustration prepared on the basis of the findings presented by Cook, T. D., &amp; Devine, E. C. Trying to discover explanatory processes through meta-analysis. Paper presented at the National Meeting of the American Educational Research Association. March, New York, 1982.)</p>

<h3 id="implausible-program-theories">Implausible Program Theories</h3>

<p>In the process of selecting the criteria of program success sometimes it becomes clear that program theories  are implausible. This is not to suggest that a great proportion of program theories are implausible, but some are, and wise evaluators don’t dismiss the idea that the conception of the program that they have been asked to evaluate might be based on implausible assumptions. Owners of swamp land were selling alligator hides to shoe manufacturers at a rate that led naturalists to worry about the disappearance of alligators in Florida. To protect alligators, the Florida state legislators passed a law forbidding the sale of alligator hides. This seemed like a straightforward way to permit alligators to thrive in Florida. Figure 3.2 is a diagram of the theoretical model underlying this law.</p>

<p>The outcome of the policy could have been evaluated by estimating the number of hides sold in spite of the ban. A comparison to the sales in years prior to the ban might well have shown that the new law successfully reduced the number of hides available. However, the real objective was not to have fewer hides sold; the real objective was to have alligators thriving in Florida. The legislators held an unstated assumption: it was assumed that landowners who were unable to sell alligator hides would maintain their land as alligator habitat. If that happened, the goal of the legislators would have been achieved; however, what happened was that landowners sought alternative uses of their land. By draining the land they were able to develop new farmland. When they did, the amount of habitat for alligators was reduced thereby negating the desired outcome of the legislation (Sieber, 1981). Detecting implausible program assumptions is more likely when theoretical models are developed and presented in a concise manner.</p>

<p><em>Figure 3.2</em> Sometimes programs are planned without a carefully thought-through impact model; when that happens there may be unstated assumptions in the model. At times unstated assumptions are implausible. When that happens the program can have an effect that was not imagined when the program was designed and implemented. (Model prepared from information in Sieber, S. D. <em>Fatal remedies: The ironies of social intervention</em>. Plenum, New York, 1981.)</p>

<h2 id="evaluation-questions-flow-from-program-theory">EVALUATION QUESTIONS FLOW FROM PROGRAM THEORY</h2>

<p>Since criteria and standards are chosen for specific programs, it is impossible to list the criteria that evaluators might use in conducting specific program evaluations. However, a number of general evaluation questions are central to program evaluation; most evaluations would include some of these questions. The value of dealing with these questions and the costs of poor choices of criteria are illustrated in the following sections.</p>

<h3 id="does-the-program-or-plan-match-the-values-of-the-stakeholders">Does the Program or Plan Match the Values of the Stakeholders?</h3>

<p>Educational and social service programs are designed to fulfill purposes that are laced with values. For example, what should be included in sex education classes for junior high students? Should the government provide funding for abortions? Should welfare recipients be required to perform public service work? These questions cannot be answered by listing facts.</p>

<p>Evaluators provide a valuable service when they help stakeholders clarify their values and assumptions. The changes to the U.S. federal income tax code at the end of 2017 provided a clear opportunity to see the range   of values and assumptions held by different people. Although most people would prefer they themselves pay less in taxes rather than more, most also believe that people with greater incomes should pay a somewhat greater percentage in taxes because they tend to use more resources than average. But what percentage should people at what income levels pay? The maximum rate was lowered in an attempt to prompt greater investment in businesses leading to more jobs. The effects of the changes in tax rates are hard to pinpoint, but the effort would be worthwhile to help in deciding whether the rate change was wise. The data will not affect fundamental value disagreements. However, identifying the value disagreements permits gathering data that speak to the central issues rather than to those merely peripheral to the arguments.</p>

<p>Turning to a different example, public housing programs are designed to make adequate housing available to poor people at affordable costs. Most people would agree that this goal matches the values of society. However, the way public housing policies had been implemented in large cities led to the clustering of poor people, often in high-rise buildings that neither foster a sense of community nor provide safe settings for residents (see “High-rise brought low at last,” 1998). An evaluation of public housing ideally would include information on the conflict between the values on which public housing policies were based and the results that have occurred. While a program evaluation cannot tell anyone what values to hold, the demonstration of the discrepancy between what occurred and what was hoped for could be an important motivator for improvement.</p>

<h3 id="does-the-program-or-plan-match-the-needs-of-the-people-to-be-served">Does the Program or Plan Match the Needs of the People to Be Served?</h3>

<p>The unmet needs of some segment of the population usually form the basis for the development of a program. The most useful program evaluations compare the unmet needs of people with the services available through the program. Unemployed people need skills that lead to employment. An excellent training program for a  skill that is not in demand would not fulfill a need. Programs cannot be evaluated in isolation from the community characteristics and the people being served.</p>

<p>In order to verify that a plan for a new program will meet the needs of the people to be served, evaluators conduct a needs assessment (Witkin &amp; Altschuld, 1995). An introduction to the methodology of the assessment of need is provided in Chapter 6. But some examination of need for the program is essential in  most if not all program evaluations. Since program resources are always limited, changes in the relevant needs for various programs over time may suggest changes in the resources devoted to those programs.</p>

<h3 id="does-the-program-as-implemented-fulfill-the-plans">Does the Program as Implemented Fulfill the Plans?</h3>

<p>Although the issue of monitoring implementation is the focus of Chapter 7, it should be noted here that there are at least two aspects of implementation. One is the match between the planned program and what is actually being done, and Chapter 7 provides important details about evaluating that match. The second aspect is more closely related to program theory, however. It is one thing to claim that a program is based on a given theory, but demonstrating that the actual practices follow the theory is a different matter. This section addresses various aspects of determining whether the program matches the plan.</p>

<p><em>Failure to Implement the Theory of the Program</em></p>

<p>Evaluators first examine a program plan to learn if the expected mechanism assumed by the theory is valid, because integrating theory and practice is much more complex than just applying one to the other (Leviton, 2014; Schwandt, 2014). Observers provide numerous examples of situations in which the theory failed. Antabuse is supposed to make alcoholics ill if they consume alcohol. One method is to implant Antabuse surgically. In an early evaluation of the procedure, blood tests of only 8 of 31 alcoholics who were to be getting Antabuse by surgical implant showed the presence of a therapeutic level of medication (Malcolm, Madden, &amp; Williams, 1974). A low-income population proved too hard to reach so a publicly funded family planning center served nearby college students (Rossi, 1978). Shopping mall cholesterol tests have been found to be exceedingly unreliable and, thus, of little use to people wanting to know whether they needed medical attention (<em>Cholesterol screening</em>, 1990). A quantitative measure of fidelity to program plans can be developed (Mowbray, Bybee, Holter, &amp; Lewandowski, 2006).</p>

<p><em>Programs That Cannot Be Implemented as Planned</em></p>

<p>A program cannot be offered as planned when the population to whom the program was directed rejects the service. The community mental health system was proposed to serve chronically ill psychiatric patients in community settings rather than keeping them confined to large state mental hospitals. It was believed that antipsychotic medication would permit such patients to live independently or with family members and that community mental health centers would provide supportive care and medications. Unfortunately, many discharged patients rejected the treatment. Since people could not be compelled to go to the centers and since funding was not as generous as expected, the program could not be implemented as envisioned. Instead, many mentally ill people ended up as homeless in the streets and under the bridges of urban areas.</p>

<p>Sometimes the theory underlying a program is not appropriate to conditions under which it is to be implemented. A curriculum to teach mathematics in elementary schools (dubbed the “New Math”) was well- planned, mathematically sophisticated, and evaluated favorably during development. However, the teachers who used it during development were better trained than the teachers who used it after widespread implementation. Accustomed to drilling children in multiplication tables, elementary teachers were expected to teach set theory. In addition, the new terminology made it impossible for most parents to offer their children any help. After a few frustrating years the new curriculum was discarded (Mashaal, 2006). An objectives-based evaluation that focused only on the degree to which the children learned the concepts covered could not detect a major reason why the New Math curriculum was ineffective: many elementary school teachers dislike and fear mathematics.</p>

<p><em>Resistance to the Program Can Make Thorough Implementation Impossible</em></p>

<p>People seeking to change the economic systems in Eastern Europe and the former USSR met with great resistance from people who were used to the older, centrally planned systems. Furthermore, many people preferred guaranteed employment and artificially maintained low food prices over the promises of better economic conditions in a more free economy that were uncertain. An example closer to home occurs when college deans ask faculty members to spend more time doing one-on-one academic counseling with undergraduates and to report on the counseling done. Faculty members who feel pressure to make progress  with their research or who may be more interested in graduate education have found ways to feign compliance with a dean’s request while not changing actual behavior, perhaps by describing brief hallway contacts as academic counseling.</p>

<h3 id="do-the-outcomes-achieved-measure-up-to-the-goals">Do the Outcomes Achieved Measure Up to the Goals?</h3>

<p>Evaluations of operating programs usually examine at least some results of the programs. Developers of innovative curricula would include measures of student achievement, and planners of an advertising campaign often depend on sales information. Even programmers of religious events deal with outcomes when they  report the number of people who say that important behavioral changes took place as a result of participation.</p>

<h3 id="seeking-information-about-the-levels-of-outcome-expected">Seeking Information about the Levels of Outcome Expected</h3>

<p>Some outcomes can be easily compared with what would be expected without the program. Education and health measures are among the most well developed for setting specific goals. Norms for academic skills are available for standardized achievement tests, and normal ranges are well known for many laboratory tests used by physicians. The outcome of on-the-job training programs could be specified in terms of the trainees’ skill   in carrying out the tasks needed by the employer. However, even tests with norms that have been validly developed can still be misused. Critics of the use of standardized achievement tests to evaluate school districts (Cannell, 1987; Shepard, 1990) suggest that when schools are repeatedly evaluated using standard tests, teachers come to tailor their teaching to the test and may even teach specific aspects of the test to their  students. Linn (2000) showed that when a new test is used in a school district, scores initially fall, but then   rise each following year as teachers learn about the test.</p>

<p>When programs are in areas for which extensive norms do not exist, evaluators are often tempted to permit program staff members to specify the objectives of the program. Although evaluators need the contributions of staff stakeholders in all phases of evaluations, experienced evaluators frequently discover that staff members are <em>too optimistic</em> about the level of success that will be observed. When one can find completed evaluations of similar programs, the outcomes achieved in those programs can provide information on what might be achieved by the program to be evaluated. The more closely an evaluated program matches the program to be evaluated, the more relevant the previous findings will be. Such information provides some rationale for the levels of outcome specified in the objectives.</p>

<p><em>Deciding on the Meaning of Outcome Achieved</em></p>

<p>Deciding whether the outcome observed is good or only marginal depends on an estimate of what should have been achieved and what specific outcomes mean. For example, after a year of reading instruction, almost all third graders can read better than when they started school in September; however, school boards and parents will not be satisfied with just any level of improvement. The improvement should be commensurate with nine months of instruction in third grade. Thus, when evaluators and stakeholders specify goals and objectives, attention must be paid to how much improvement they expect.</p>

<p>When evaluators first begin to work with stakeholders to develop statements of outcome objectives, sometimes they borrow the style of writing hypotheses that they learned in statistics classes. In statistical analyses the alternative hypothesis is often presented as the mean of the program group exceeding the mean of the control group, <em>Ha</em>: <em>Mp</em> &gt; <em>Mc.</em> In most social science research, however, the amount of difference between  the groups is seldom specified. In contrast, merely showing that changes occurred as in social science research is not sufficient in evaluation. It is more useful to list the actual level of achievement desired for students, patients, and trainees. If a product is to be evaluated, the performance needed is specified. If a medical information system is developed for a large medical practice, just saying that the new system would be <em>faster</em> than the old system is not very helpful. Instead, a minimum performance level should be specified. If the new system is not at least that fast, then the system is not working as well as planned. Evaluators trained in social science research methods often have difficulty committing themselves to a specific goal because basic research in the social sciences is not carried out that way. There are some alternative approaches to statistics for evaluators (see E. J. Posavac, 1998, and Chapter 13).</p>

<h3 id="evaluating-without-a-theoretical-model">Evaluating without a Theoretical Model</h3>

<p>The ease with which staff, government auditors, and the general public fall into the trap of skipping the step of model development suggest that the issue needs to be reemphasized. The Auditor General of Illinois criticized <em>Parents Too Soon</em>, a state-funded program whose objectives included reducing the rate of teenage pregnancy (Karwath, 1990). In defense, the program’s administrator described the difficulty of evaluating a prevention program, and mentioned the reduced number of live births to Illinois teenagers between 1983 and 1988. (By using these figures, note that the administrator had adopted a nontheory-based evaluation.) Since the number of Illinois teenagers dropped between those years and since the numbers of abortions were not known, the decreased birth rate was not seen as support for the program. Furthermore, there are so many influences on the rate of teenage pregnancy that skipping the development of a program model placed insurmountable limitations on the interpretation of any findings. If the program had specified how the program was to work, implementation goals and intermediate goals, and the final, bottom-line outcome goal, the administrator may well have been in a better position to respond to the criticism.</p>

<p>Although the idea of integrating theory and practice is a compelling and valuable goal, the human tendency to use established patterns called heuristics to process information and reach conclusions means that we tend to do things in the same way we always have. Recognizing these habits and working to increase the flexibility of our thinking is thus an important strategy to improve matters (Patton, 2014). Along those lines, empowerment evaluation approaches and the associated method of Getting to Outcomes provides a structured way for organizations to build more systematic and rigorous reflection on their ongoing program and results (Wandersman, Alia, Cook, Hsu, &amp; Ramaswamy, 2016). Reflecting systematically on one’s program and the theoretical reasons for its elements is one way to address some of these problems (Leviton, 2014).</p>

<h2 id="cautions-in-choosing-evaluation-criteria">CAUTIONS IN CHOOSING EVALUATION CRITERIA</h2>

<p>Once a program theory has been developed, it is possible to begin to specify the criteria of successful implementation and desired outcomes that will permit us to carry out useful program evaluations. Many evaluators have emphasized the importance of sound research design (as done in later chapters); however, just as a chain is only as strong as its weakest link, an unreflective selection of what to measure can lead to a failed evaluation just as surely as an inappropriate research design can. Ill-considered choices of standards could make it impossible to draw any conclusions from an evaluation even when the evaluation was well planned in other ways.</p>

<h3 id="criteria-must-reflect-a-programs-purposes">Criteria Must Reflect a Program’s Purposes</h3>

<p>Without very careful planning and a thorough understanding of a program, it is quite easy to select criteria that do not reflect its purposes. Many observers came to believe that the first evaluation of Head Start (Cicarelli, Cooper, &amp; Granger, 1969), a popular, federally funded preschool program for children of poor families, was mortally wounded by the decision to use measures of the improvement of intellectual skills as the primary criteria of success. Instead, Lazar (1981) argued that the most important aspect of Head Start was the involvement of parents in the education of their children. It was hoped that the involvement of low-income parents, many of whom typically participate only minimally in the education of their children, would continue as their children grew. Thus, Head Start was hoped to lead to increases in the likelihood that low-income parents would encourage the development of their children, help with doing homework, and make sure their children were in school. If more parents adopted such practices, such activities could have long-term positive effects on the children, effects far more important than the specific knowledge that the children gained in Head Start classes (Leik &amp; Chalkley, 1990).</p>

<p>What should the criteria for success be for a recycling program? Since it is necessary for residents to separate their recyclable materials from garbage, it might seem reasonable to measure the amount of recyclable material collected as an outcome criterion. But there have been reports of materials being collected, but then simply dumped in the regular garbage landfill. The real criterion of success is that the materials be reused in some way. A recycling program is actually quite complex. For such an effort to achieve desirable outcomes, the cooperation of residents is needed, along with an efficient collection system that meshes with sorting facilities that in turn distribute materials to appropriate manufacturers who find customers for their products. Criteria   of success may look valid yet not measure success in achieving the ultimate objectives of the program.</p>

<p>The time when an evaluation criterion is measured could be chosen in a way that fails to reflect the program’s purposes. A program could have positive immediate or short-term effects, but a marginal long-term one. A physical fitness program can be quite effective in improving muscle tone without having an effect on program participants’ physical condition or health 10 years later. On the other hand, some college alumni remark how helpful a faculty member was for their development even though the faculty member was perceived as critical and overly difficult when they were in school. Here, the long-term effect seems more favorable than the short-term one. Evaluators seek to make observations at times which correctly reflect the objectives of the program and the needs of the participants.</p>

<h3 id="criteria-must-be-under-the-staffs-control">Criteria Must Be under the Staff’s Control</h3>

<p>Evaluators can expect considerable resistance to an evaluation when program staff members feel that their program will be judged on criteria that they cannot affect (Torres, 1994). For example, employees in manufacturing firms may feel that their effectiveness is limited by the quality of the equipment they must use. Such limits need to be considered in evaluating whether they are doing as good a job as possible. Some college teachers object to the use of tests of student skills to evaluate teaching effectiveness because they fear that student achievement levels reflect student effort at least as much as faculty skills and effort. In addition, students enter college with different levels of developed skills; thus, the more well-prepared students graduate with better skills. Furthermore, if students must hold jobs while attending classes, their achievements will on the average be lower than that of others. Those whose success depends on the performance of others—as is   the case for teachers, counselors, coaches, or managers—face a more uncertain situation than do people who  are more directly in control of the results of their efforts. The challenge to evaluators is to find ways to identify measures and criteria that reflect the efforts and skills of the staff and are agreed to be central to the program’s purpose.</p>

<h3 id="stakeholders-must-participate-in-selecting-evaluation-criteria">Stakeholders Must Participate in Selecting Evaluation Criteria</h3>

<p>The criteria for a specific evaluation are selected in close consultation between the evaluator and the stakeholders involved with the project (Renger &amp; Hurley, 2006). If the stakeholders do not accept the criteria selected, even when they are appropriate, the evaluation cannot have the impact that it would otherwise have. This does not mean that evaluators simply use whatever criteria and standards stakeholders want. Stakeholders are seldom trained in evaluation methodology. When there is disagreement about the appropriate criteria and standards, evaluators spend many hours in dialogue with the stakeholders to assure that all parties agree on the standards discussed (McGarrell &amp; Sabath, 1994).</p>

<p>The selection of criteria and standards also varies depending on what aspect of the program development process is to be studied. Programs go through a number of phases: proposal preparation, planning, initial implementation, actual operation, expansion, contraction, and accreditation and reaccreditation. Criteria and standards to evaluate a plan for a program will differ markedly from those used in a reaccreditation.</p>

<p>In addition, the criteria differ according to the type of program being evaluated. Programs in education, health, criminal justice, marketing, and training differ in their emphases, the relative power of different stakeholder groups, the traditional types of information found useful, and the financial support available for evaluation studies. These differences play a part in the development of standards of program success and are taken into consideration in designing program evaluations.</p>

<h2 id="identifying-goals-and-objectives">IDENTIFYING GOALS AND OBJECTIVES</h2>

<p>In order to know how well we have achieved our goals, we need to know what we wanted to achieve. Some people set out on an automobile trip without a destination in mind. If driving is pleasurable and if simply a change of scenery is desired, having no destination is fine. But most people want to travel to some specific location. If stakeholders are unable to decide what a training or service program is supposed to achieve, there  is little reason to begin the program or to evaluate it. A number of issues related to goals need to be addressed. Are there clear ideas about objectives? Are there disagreements about objectives? If so, are these disagreements mutually incompatible or could they complement each other?</p>

<h3 id="how-much-agreement-on-goals-is-needed">How Much Agreement on Goals Is Needed?</h3>

<p>Sometimes people are very unclear about what might be accomplished through a program (Cook, 2000; Mathison, 1994). Community groups may sense that there is a problem and wish to provide some service, but simply saying that we want people to be educated, healthy, and happy is not enough to develop a program to assist them. In other words, stating goals only in the most abstract terms may attract supporters, but it will provide little assistance in designing a program or in evaluating a program once it is under way. Abstract goals may be appropriate in the U.S. Constitution or in the mission statements of organizations, but will not do   once people try to produce an operational program. There really is no reason to begin to plan if no one can describe goals any more specifically than saying that “we want to empower people,” “we want to provide excellent medical care,” or “we want excellence in education.”</p>

<p>When stakeholders can describe the specific goals they hold, even if the goals are dissimilar, progress has been made and negotiations may begin. Progress will be stalled if the goals conflict with each other to the extent that achieving one goal makes it impossible to achieve the other; a family cannot vacation on Baja beaches and in the Canadian Rockies at the same time. Similarly, cancer care cannot make patients as comfortable as possible while using aggressive treatment with experimental medications. Planners can work with stakeholder groups to find mutually agreeable alternatives or to define situations in which one would use one set of goals rather than another. There might be ways to define a patient’s condition that would determine when aggressive (but uncomfortable) treatment would be used versus situations when the goal would be simply to maintain comfort and to minimize pain while suspending intensive medical treatment.</p>

<p>Sometimes people have goals that are different but not incompatible. No college faculty is unified about what students are expected to achieve. Some professors expect detailed knowledge, others conceptual clarity. Some are concerned about the development of employable graduates; others are more concerned that students develop a sound philosophy of life. In most colleges, these different views exist side by side while students are exposed to a variety of professors and courses. In such a setting, mutual respect for differing points of view permits the organization to function.</p>

<h3 id="different-types-of-goals">Different Types of Goals</h3>

<p>Once a program has been planned and started, there are different types of goals whose achievement can be evaluated. It is important to verify that the program has gotten underway as planned and to learn whether short-term and long-term outcome goals have been achieved.</p>

<p><em>Implementation Goals</em></p>

<p>All programs involve some level of activity before it is possible to achieve any outcomes. For example, equipment must be purchased and staff members must be hired and trained before any service is given. If these activities do not occur, there is no point in seeking to learn if the desired results have been achieved. Figure 3.3 includes a number of goals that the evaluator would need to examine in an evaluation of the effectiveness of a new Internet-based mathematics teaching system for middle school children. Implementation goals refer to goals that focus on the availability of the hardware and the quality of teacher training.</p>

<p>Although it may seem obvious to verify that the program exists before seeking to evaluate its effectiveness, some stakeholders and evaluators have ignored this step. They seem to believe that if a program developer said that something would be done, then it is safe to assume that it has occurred as planned. One university vice president for research challenged the idea that the evaluation of implementation goals must be part of a program evaluation by asking, “Can’t we assume that these are honorable people? Can’t we assume that if they say they will do something, they will do it?” In fact the documentation of complete implementation is seldom a question verifying personal integrity. Instead, it is critical to learn if unforeseen problems made it impossible to carry out plans.</p>

<p><em>Figure 3.3</em> The objectives specified for a program should include more than just the ultimate goal of the program. Implementation, intermediate, and achievement goals or objectives of a hypothetical program using an Internet-based instructional system for mathematics education for middle school children are illustrated.</p>

<p>A graduate student team sought to evaluate the effectiveness of a program in which faculty members volunteered to invite freshman, nonresident students into their homes for dinner in an effort to encourage the students to identify more closely with the college and, thus, be less likely to transfer to a different college at some time. Each faculty volunteer was given the names and addresses of 10–12 new students. The evaluators learned that 40% of the volunteer faculty members did not invite any students to their homes. Of the students invited, only 60% showed up. This meant that, at best, only 36% of this phase of the program had been implemented. The faculty members’ intentions were not questioned. Scheduling problems may have come up; for some faculty members the task may have been more difficult than they had imagined; others may have misunderstood what they agreed to do; and perhaps some volunteered before checking with their spouses. Without including an implementation phase in this evaluation, the evaluators would have been unaware that the program was only one-third as strong as planned.</p>

<p>In Figure 3.3 access to the system and the training of teachers are the focus of the intermediate goals. If the achievement of students is to improve, students must be able to use the system and teachers must understand it.</p>

<p><em>Intermediate Goals</em></p>

<p>Intermediate goals refer to things that are expected to occur as a result of the proper implementation of the program, but that do not constitute the final goals intended for the program. For example, all students should have textbooks (an implementation goal) and they should read the assignments on time (an intermediate goal), but the actual criteria of success go beyond reading the assignments. Intermediate objectives in Figure 3.3 deal with student acceptance and use of the novel method of learning.</p>

<p><em>Outcome Goals</em></p>

<p>Even if the Internet system in Figure 3.3 is implemented and used, it is still necessary to learn whether math achievement improves. We cannot be satisfied with simply verifying that the teaching system is in place: we must also examine its outcome goals. The most ideal finding would be that the program was implemented as planned, that the intermediate goals were achieved, and that the outcomes were favorable. Even then we would still not have conclusive proof that the program <em>caused</em> the achievement of the valued outcomes. However, finding that good results followed faithful implementation is compatible with believing the program was responsible.</p>

<p>There are other possible patterns of findings. Suppose that the intermediate goals were met (the hardware was in place, the teachers knew how to use it, and the students spent time with the computers) but student achievement did not improve. That would support those who felt that the more interactive method for learning math did not serve these middle school children well. It should be noted that for this example it was assumed that the evidence was good that the hypothetical Internet-based learning system was effective. Evaluators must never forget that although some groups may assume or claim that a program has good outcomes, it might not actually be effective (Weiss, Murphy-Graham, Petrosini, &amp; Gandhi, 2008).</p>

<h3 id="evaluation-questions-that-apply-to-all-programs">Evaluation Questions That Apply to All Programs</h3>

<p>Critics argue that those who use objectives-based program evaluations limit the focus of evaluations to the explicitly stated goals of the program being evaluated, and devote little attention toward other factors (Scriven, 1991; Stufflebeam, 2001). If other aspects of a program were not examined, then objectives-based evaluation would have major limitations. One way to avoid this problem is to recognize that there are many goals that apply to all programs even though such goals are not listed during program planning. Program planners seldom need to state that people are to be treated with respect, that school children are not to be treated in   ways that foster dependence, or that people are not to be discriminated against on the basis of race or sex.  Since criteria such as these do not appear in lists of program goals, some evaluators have not treated these issues in program evaluations.</p>

<p>All programs also have the seldom-stated goal of no negative side effects or at least sufficiently minor ones. Evaluators should strive to become sufficiently familiar with the program and participants so that they can recognize any serious negative side effects. The negative side effects of Prozac, the antidepressant medication, became apparent (Glenmullen, 2000) in spite of enthusiastic advocacy of its widespread use (Kramer, 1993). At times evaluators have avoided personal exposure to the program in a misguided quest for objectivity, depending on surveys and records to provide evaluation data. Evaluators who do not become extremely familiar with the program, its staff, and its customers, clients, students, or patients run the risk of overlooking side effects.</p>

<h3 id="is-the-program-accepted">Is the Program Accepted?</h3>

<p>Good ideas are seldom adopted immediately. Mosteller (1981) described the astonishing lag in applying the sixteenth-century finding that citrus fruits (vitamin C, of course) would protect seamen from scurvy. On some long voyages, over 60% of the crew members died. One might have expected officials to immediately implement a policy that would have reduced this staggering loss of life, but 198 years elapsed between the first of several published descriptions of how to protect crews from scurvy and the adoption of the knowledge by the British Navy in 1795.</p>

<p>People for whom a program is designed may also reject it. There is a long line of research that clearly  shows that physically ill patients often reject or ignore the treatment prescribed for their conditions (Posavac   et al., 1985). Attempts to understand noncompliance with medical treatment suggest that better communication between physicians and patients promotes compliance. Some patients misunderstand the recommended treatment. It is especially hard for some patients with chronic illnesses such as diabetes to maintain the motivation to adopt permanent lifestyle changes. Involving supportive family members is often  an effective approach.</p>

<p>Many program managers request participants to complete surveys at the end of a service to measure the participants’ satisfaction with the program. We believe that such surveys have a role to play in program evaluation, but it is important to recognize that such surveys cannot provide a sufficient means to assess the degree to which the program was effective. Some people are very satisfied with worthless treatments because they feel respected and valued by the people offering the service. However, a program that is disliked and rejected may not have a chance to be effective because participants who dislike a program or its staff are unlikely to participate fully. Marketing research firms focus on learning the preferences of potential customers and then matching products to those preferences. Although human service fields cannot base service design only on what students, clients, and patients think they need, program staff members who are aware of client preferences have a better chance of designing and offering a service that will be accepted and utilized.</p>

<h3 id="are-the-resources-devoted-to-the-program-being-expended-appropriately">Are the Resources Devoted to the Program Being Expended Appropriately?</h3>

<p>There are many ways in which program costs can be used in program evaluations. Elementary procedures that relate cost to program outcomes are introduced in Chapter 11. The various evaluation questions that can be treated are reviewed here.</p>

<p><em>Using Program Costs in the Planning Phase</em></p>

<p>When a government program is developed to provide a service to all who qualify, it is difficult to project its ultimate cost because it is difficult to estimate how many people will apply. However, when a specific program such as a new curriculum or a counseling service for a set number of participants is being planned, the costs  can be fairly well estimated. Sometimes it may be possible to project the value of the results. For example, job training should result in better jobs at salaries that can be estimated. Then, the taxes those people will pay and the amount of welfare assistance they will not use can be estimated. One could ask whether that result justifies the use of the amount of government funds needed to provide the program. If such a program succeeded in providing job skills and leading people to be self-sufficient, there may be additional benefits to which dollar values cannot be assigned.</p>

<p>A second issue in the planning phase concerns whether alternative uses of the funds would provide a more desired result. Since resources are limited, it is important that critical stakeholders agree that the services to be supported are needed more than other services that could be supported instead. Such decisions are based partially, not totally, on the basis of costs and the value of benefits.</p>

<p><em>Is Offering the Program Fair to All Stakeholders?</em></p>

<p>The use of ethnic origin to target educational aid is an attempt to give minorities a boost to overcome past discrimination. Many people endorse this policy (Dawes, 1994); however, some writers argue that such programs primarily assist children of middle-class minority parents who do not need the assistance (McWhorter, 2000; Steele, 1990). Evaluations of some programs aimed at increasing the skill levels of minorities have found that the programs actually increased the difference in skills between the children from low-income families and those from the middle class (Cook, et al., 1975), possibly because middle-class  parents made special efforts to obtain those services for their children. If so, is it fair to continue to use public funds to support the program? A similar point can be made regarding “corporate welfare.”</p>

<p>A second illustration concerns support for public education. Because American public education has been funded largely through local property taxes, wealthy communities and communities with large numbers of businesses and industries can spend far more per pupil than can poor communities without industrial firms. Some critics then argue that it would be more fair if school funds were dispersed on the state level so that all school districts can offer school programs of a similar quality. The evaluator cannot answer these questions of fairness using research methods. However, the use of standards of social justice in evaluation has been encouraged by some writers (see Sirotnik, 1990) who argue that a completely objective evaluation methodology is neither possible nor desirable. It would be critical to be explicit about how such values are being used; evaluators with hidden agendas do not serve stakeholders well (Scriven, 1997a).</p>

<p><em>Is This the Way the Funds Are Supposed to Be Spent?</em></p>

<p>Another question about the use of program funding concerns whether the funds are being spent in a way that is compatible with the intention of the funding stakeholder. This question is related to the traditional accountability issue, which is designed to reduce the possibility of the misappropriation of funds. In the past, Congress and state legislators were often more interested in the question of whether funds had been spent as intended rather than the more difficult-to-answer questions of whether the funds had been spent wisely and whether the activities supported had met the needs that prompted the interest of the legislators.</p>

<p><em>Do the Outcomes Justify the Resources Spent?</em></p>

<p>Once the results of a program are known, it is possible to examine the program costs to determine whether   the outcomes were worth what it cost to attain them. If it is possible to place a dollar value on the outcome,  then we can ask if the return was worth more than the cost of the program. Business-based financial analysts  do this all the time. If the expected return on a company’s investment does not exceed the investment, then there is probably something better to do with the investment. If there really is nothing better to do with the money, then the business should be sold and the money distributed to the stockholders. When reasonable estimates of value are possible, this cost/benefit analysis is highly instructive. Although such analyses are becoming more common, some suggest that many outcomes require a more complex consideration of value  that cannot be given a price tag (King, 2017).</p>

<p>Appropriate time frames also need to be considered. Sometimes U.S. businesses are criticized for demanding a return on investments too soon; it may be that sizable returns require a long-range perspective.   In human service and educational fields it is quite hard to place a dollar value on the results of programs. In  this case, evaluators seek to compare programs that are designed to affect similar behaviors, such as developing employment or reading skills, and to ask which program is the most efficient. Unless there are other constraints, the most efficient would be the best program to offer.</p>

<p><em>Has the Evaluation Plan Allowed for the Development of Criteria That Are Sensitive to Undesirable Side Effects?</em></p>

<p>If it were possible to predict specific side effects, program plans would be changed to reduce the chance of these negative outcomes. Since evaluators and managers expect some unanticipated outcomes to occur, observation procedures are planned to permit side effects to become apparent. This means that evaluators will allot time for visiting program sites and talking with representatives of all stakeholder groups. An organizational consultant remarked that although the management of a company hired him, he accepted a project only after it was agreed that the union would codirect the project. There is no point in carrying out an evaluation if the evaluator only hears one view of the strengths and weaknesses of a program or an organization.</p>

<p>An improvement-focused approach to evaluation includes attention to unplanned results, whether positive or, as is often the case, negative. Overlooking side effects is more likely when evaluators plan an evaluation on the basis of official program descriptions (rather than the program as implemented), obtain information from only one stakeholder (usually the manager), or carry out the evaluation at arm’s length from the program. A professor of community nursing described how one evaluation team tried to evaluate a program designed to provide medical care to dispersed poor families in suburban areas. From the comfort and security of their offices they prepared a questionnaire based on the official objectives presented in the grant proposal and simply mailed the survey to a list of participants. These evaluators never visited any of the sign-up centers, never interviewed any participants, and never contacted any of the cooperating physicians. This mechanical application of objectives-based evaluation has contributed to negative reactions to evaluation in general and objectives-based evaluation in particular.</p>

<p>When evaluators believe that they have detected negative side effects, they will share their observations  with stakeholders to obtain confirmation of their views or additional information to explain the observations. More careful observation procedures can then be developed to examine the unanticipated problem. Finding negative side effects is not done to discredit programs, but to assist in improving programs. Perhaps the program theory can be improved (Fitzpatrick, 2002). Sometimes it helps to have an outside observer call attention to overlooked problems, but calling attention to a problem is merely the first step; for effective evaluators, a harder and more important step involves helping to develop plans for improvements.</p>

<h2 id="some-practical-limitations-in-selecting-evaluation-criteria">SOME PRACTICAL LIMITATIONS IN SELECTING EVALUATION CRITERIA</h2>

<p>Textbook authors have the freedom to describe ideal practices without being limited by the practical realities  of specific evaluations. Three important restrictions on the selection of criteria are the evaluation budget, time constraints, and the degree to which various criteria are accepted by the stakeholders.</p>

<h3 id="evaluation-budget">Evaluation Budget</h3>

<p>Evaluation is not free; evaluators and their staffs need to be paid, surveys need to be duplicated and mailed, costs for phones and computers add up quickly, and getting around to various program sites can be expensive. Evaluators estimate the amount of time they will need to do a credible job in light of the stakeholders’ needs.  It is usually necessary to negotiate the parameters of the evaluation since few stakeholders know how much time is involved in completing a valid evaluation. Since the funds for an evaluation will be fairly inflexible, the focus of the evaluation is often adjusted during these negotiations. It is better to carry out a modest evaluation whose findings can be trusted than to plan an ambitious project that can only be done poorly given the  resources available.</p>

<h3 id="time-available-for-the-project">Time Available for the Project</h3>

<p>Since evaluation is an applied discipline, the use of findings is often tied into budget cycles, academic years, or the meeting schedules of governmental bodies. Some years ago the federal government commissioned a study of an innovative approach to welfare. By the time the demonstration project and evaluation were completed,  the political climate had changed and Congress was no longer interested in the original idea or the evaluation. Often, projects are of a much shorter duration than expensive multisite demonstration projections costing tens of millions of dollars, but the principle is the same: There is a time when information can be used and a time after which it is no longer relevant and will not contribute to decisions (Sonnichsen, 2000). Before evaluators accept a project or begin to plan seriously, they ask about the project deadline. If the deadline and the stakeholders’ evaluation needs are incompatible, the evaluator must bow out or the project must be renegotiated. There simply is no reason to begin a project that cannot be completed when it is needed.</p>

<h3 id="criteria-that-are-credible-to-the-stakeholders">Criteria That Are Credible to the Stakeholders</h3>

<p>Evaluators seek to collect valid data in ways that permit valid interpretations; in addition, it is crucial that stakeholders accept those interpretations. While planning the evaluation, evaluators ascertain that the stakeholders have agreed that the jointly selected criteria fit the goals of the program and are appropriate for the participants in the program. We talk about stakeholders “buying into” the criteria: it is helpful to have agreements in writing. The stakeholders need not sign a statement of acceptance; however, the evaluator can keep careful notes during meetings as an evaluation is planned. After each meeting, copies of these notes can be distributed to critical stakeholders. At the beginning of each meeting, the agreements summarized in the notes can be reviewed to be sure that stakeholders still agree on the objectives of the program and the focus of the evaluation.</p>

<blockquote>
  <p><strong>CASE STUDY 3</strong></p>

  <p>“CU Flourish”: A Medical Resident Wellness Initiative Based on Positive Psychology “PERMA” Theory</p>

  <p>This case illustrates several important points. First, it provides another good example of the specific elements of a real evaluation. In this instance, it has the added benefit of being an evaluation in process, so more information will follow on a yearly basis in the eResources. Second, it illustrates how internal evaluators can be particularly appropriate (see especially the Improvement Focus section). Third, although many of the other cases were, in fact, conducted by more than one evaluator, this case explicitly notes the team effort that was involved. As you will see, the Evaluator Profile that follows presents the four primary team members rather than the story of just one.</p>

  <p>Meeting Needs</p>

  <p>Upon completion of medical school, physicians embark upon an intensive period of specialty internship and residency training that ranges in duration from three to more than 10 years. Training program requirements are demanding and are characterized by especially long days and weeks that require intensive periods of demonstration of clinical proficiency, academic achievement, and professional and personal stamina. Medical residency programs can take an appreciative toll on physician well-being. Taken together these demands are often reflected in high rates of burnout, depression, and in recent years, increased incidence of suicide (Dyrbye, et al., 2014). Accordingly, medical residency programs are now required to include wellness programs and/or initiatives in accredited residency training program curricula. The University of Colorado (CU) School of Medicine Resilience Program responded to these needs with the development and pilot implementation of a Resident Resilience curriculum entitled “CU Flourish.”</p>

  <p>Implementation</p>

  <p>The development and implementation of CU Flourish has been accomplished in three phases:</p>

  <p>Phase 1: Identification of theoretical and empirical underpinnings of curriculum components. The CU Flourish team adopted a unifying theory for the program’s goal of addressing the broad range of specific needs: Seligman’s (2011) Positive Psychology “PERMA” Theory of Well-Being, which is comprised of five building blocks that foster human flourishing—Positive Emotion (P), Engagement (E), Relationships (R), Meaning (M), and Accomplishment (A) (Seligman, 2011). They then developed four main program topics: “Finding Your Values Compass,” “Mindfulness for Healthcare Professionals,” “True(er) Grit,” and “Putting Joy and Gratitude to Work”; these collectively address the five building blocks of the PERMA theory.</p>

  <p>Phase 2: Initial program component pilot among varied constituent groups across the CU School of Medicine. Each of the topics was presented to colleagues and supervisors in the school, and the team reviewed the materials and feedback extensively in preparation for a two-day retreat format with CU Anesthesiology Residents.</p>

  <p>Phase 3: Pilot study and dissemination of evaluation findings. The retreat was conducted with an entering cohort of Anesthesiology Residents. Subsequently, a “Train the Trainer” session was held for other interested physicians, beginning the dissemination of a standardized program.</p>

  <p>Stakeholders</p>

  <p>The CU Flourish program team members are accountable to multiple and varied “horizontal” (colleagues in various departments/specialties) and “vertical” administrative stakeholders. The School of Medicine administrators also are interested in demonstrating to the medical residency accrediting body (ACGME) that the programs are actively addressing the wellness-related needs of residents of each specialty. The residents who benefitted from the program are, obviously, also important stakeholders.</p>

  <p>Side Effects</p>

  <p>The negative side effects most immediately felt by the team were time pressure, limited resources, and demand for program components taken out of context from the CU Flourish program. As is often the case, the promise of an effective program was met with considerable demand for the program and the “Train the Trainer” sessions quickly, straining available resources. In addition, other stakeholders wanted parts of the CU Flourish program without uniform agreement regarding maintaining program integrity, fidelity, or common modes of outcome assessment.</p>

  <p>Positive side effects included increased program visibility and enhanced cohesiveness and productivity of the evaluation team.</p>

  <p>Improvement Focus</p>

  <p>It is true that internal evaluators can potentially risk overstating good results partly because they themselves are stakeholders. In this case, the multiple examples of seeking feedback from colleagues and supervisors as well as the residents who were served by the program, and the repeated revisions to the materials in response to the feedback, demonstrate a high level of commitment to improve the program. In fact, this is the other side of the internal evaluator “coin”—being stakeholders also means that they not only care about improving the program, they have “skin in the game,” so their dedication to improvement can be particularly high. Further, this evaluation team demonstrated a meta-evaluative approach to improvement—not just examining the implementation and impact of the program, but considering both the results and the process of the evaluation itself, such as noting the problems of pressure on the evaluation team. Such reflective practices are particularly good examples for new evaluators to follow.</p>

  <p>Outcomes</p>

  <p>Outcome measures were administered via online surveys prior to and after the retreat along with more lengthy surveys conducted at the beginning and end of each academic year. Some key quantitative outcome indicators of the program are (1) positive-to-negative affect ratio (i.e., positivity or “flourishing” ratio), (2) mindfulness scores, (3) satisfaction in valued areas of living, and (4) burnout scores. At the time of this writing, only baseline (i.e., beginning of the academic year) data have been collected for each of three Anesthesiology Resident cohort groups. During the retreat, evaluation data were collected at the conclusion of each of the topic sections. Both objective ratings and subjective comments were collected. Notably, results suggested that the program and topics were well received and deemed to be useful, and there were significant increases in pre- to post-retreat flourishing ratios. More extensive outcome survey data will be analyzed when both the beginning and end of year time point surveys have been completed for all resident cohort groups and posted in the eResources.</p>

  <p>Nuances/Mechanisms</p>

  <p>The nuances/mechanisms of the program lie in the underlying PERMA theory. The premise is that if the PERMA components are found in the topic sessions, these will function as the active mechanism to elicit change in our key outcome measures. For example, the “Mindfulness for Healthcare Professionals” topic session was consistently rated by attendees to have Positive emotion (P), Engagement (E), and Meaning (M) components. Attendees noted that although the “Finding Your Values Compass” session did not necessarily tap into Positive emotion, it did elicit Engagement (E), Relationships (R), Meaning (M), and a sense of Accomplishment (A).</p>

  <p>Summary</p>

  <p>As the chapter emphasized, programs that have no explicit or even implicit theory make it challenging for evaluators to determine some of the appropriate measures, especially potential mechanisms. This case presents a good example of the benefits that accompany clear theoretical foundations.</p>

  <p><em>Sources:</em> A. O. Beacham (personal communication, February 2, 2018); Dyrbye et al. (2014).</p>
</blockquote>

<blockquote>
  <p><strong>EVALUATOR PROFILES 3</strong></p>

  <p><strong>Abbie O. Beacham, PhD; Alison Brainard, MD; Norah Janosy, MD; and Jennifer Reese, MD</strong></p>

  <p>The core CU Flourish team is comprised of three physicians and one clinical psychologist, who all contributed vital elements to the program. They asked to present profiles for each member because they have functioned in this project as one unit. These colleagues are each passionate about physician wellness and resilience, contributing countless hours in training, education, and research in this area. In addition, each has a unique history that has compelled her to pursue this work, and each one’s story and perspective makes this work compelling for others. The following “thumbnail” profiles provide an introduction to them.</p>

  <p><strong>Abbie O’Ferrell Beacham, PhD,</strong> is an Associate Professor in the Departments of Psychiatry and Family Medicine and an Associate</p>

  <p>Director of the Resilience Program at the University of Colorado School of Medicine. She completed her doctoral training at the University of Louisville, Louisville, Kentucky, and her predoctoral internship in Clinical Health Psychology at the University of Florida Health Sciences Center, Gainesville, Florida. Her post-doctoral fellowship research and clinical work was in psycho-oncology and orofacial pain at the University of Kentucky Colleges of Medicine and Dentistry. She has extensive clinical experience working and training students in medical settings including primary care, psycho-oncology, and chronic pain management. Dr. Beacham’s research and clinical work has integrated mindfulness and acceptance-based approaches to behavioral self-management in clinical medical and nonclinical populations and, most recently, resilience in healthcare professionals.</p>

  <p><strong>Norah Janosy, MD,</strong> is an Assistant Professor at the University of Colorado in the Department of Anesthesiology. She practices</p>

  <p>pediatric anesthesiology at the Children’s Hospital of Colorado. She attended medical school at the University at Buffalo State University of New York School of Medicine &amp; Biomedical Sciences, completed her residency at Oregon Health &amp; Science University in Anesthesiology, and completed a fellowship at Oregon Health &amp; Science University in Pediatric Anesthesiology. Dr. Janosy is the co- director of wellness for her department with her colleague, Dr. Alison Brainard. She is actively involved in the Resilience Council at the University of Colorado, as well as a founding committee member of the “Peer-to-Peer” Network. She has presented on physician wellness and resilience at numerous national conferences (Stanford Med-X, Colorado Review of Anesthesia for Surgicenters and Hospitals [CRASH], Society of Education in Anesthesia [SEA]). Physician wellness and resilience is a topic about which she is passionate, and she has devoted her academic career to this important topic.</p>

  <p><strong>Alison Brainard, MD,</strong> is an assistant professor at the University of Colorado School of Medicine Department of Anesthesiology. She</p>

  <p>attended medical school at Saint Louis University School of Medicine and completed her internship at the Thomas Jefferson University Program and her residency at the University of Pennsylvania Program where she was Chief Resident, Anesthesiology. Dr. Brainard is the co-director of wellness for her department with her colleague, Dr. Norah Janosy. She is actively involved in the University of Colorado Resilience Council, as well as a founding committee member of the “Peer-to-Peer” Network. She has presented on physician wellness and resilience at numerous national conferences (Stanford Med-X, Colorado Review of Anesthesia for Surgicenters and Hospitals [CRASH], Society of Education in Anesthesia [SEA]), and has been an invited guest speaker at Yale University. Physician wellness and resilience is a topic about which Dr. Brainard is passionate, and she has devoted her academic career to this important topic.</p>

  <p><strong>Jenny Reese, MD</strong> is an Associate Professor of Clinical Pediatrics, Section of Pediatric Hospital Medicine at the University of</p>

  <p>Colorado School of Medicine. She is the Interim Section Head and Medical Director of the Section of Pediatric Hospital Medicine. She also serves as the Inpatient Medical Director for Children’s Hospital Colorado. Her clinical time is spent as a pediatric hospitalist, and her administrative duties include quality and process improvement and clinical leadership, as well as developing and promoting programs that support wellness and resilience for health care providers. In 2015, Dr. Reese formed the University of Colorado School of Medicine Resilience Program for Faculty, Residents and Fellows, and she serves as the director of this program. Her undergraduate and medical school were completed at University of Washington in Seattle, Washington and her pediatric internship and residency were completed at the University of Colorado School of Medicine and Children’s Hospital Colorado.</p>
</blockquote>

<h2 id="summary-and-preview-2">SUMMARY AND PREVIEW</h2>

<p>The importance of developing a theory of a program and selecting evaluation criteria wisely is hard to overemphasize. The criteria are the windows through which users of the evaluation see the program. If the windows distort the view, the program would be misrepresented, either favorably or unfavorably. Ideally, evaluators observe more than just the final outcome of programs that require the cooperation and effort of the participants, as do programs in educational, psychological, criminal justice, and other settings. Evaluators and their clients benefit by examining the processes whereby a service is thought to lead to the expected outcomes. Even a very simple program theory that specifies program activities, expected intermediate outcomes, and  final outcomes greatly improves the evaluator’s ability to understand the program.</p>

<p>The next chapter focuses on the specific steps that one takes in measuring the criteria selected. Although a textbook separates the selection of criteria from a discussion of measurement issues, evaluators consider whether criteria can be measured validly and reliably at the same time they are making choices of criteria to measure.</p>

<h2 id="study-questions-2">STUDY QUESTIONS</h2>

<ol>
  <li>Consider a setting with which you are familiar: school, work, church, team, dormitory. Each has rules or procedures to meet certain objectives. Try to analyze the assumed impact model to learn how the procedures might lead to the objectives. For example, there are required courses or area requirements for most college curricula. How might these requirements lead to educated graduates? What intermediate outcomes need to occur? How much validity does your impact model seem to possess? Drawing an  impact model often helps to detect implausible assumptions. Try to find a few implausible assumptions underlying a public policy.</li>
  <li>This chapter suggested that different stakeholders have very different views of how intercollegiate athletes should be evaluated. Make a list of some public policies or programs that would be likely to be seen as having quite different purposes depending on the stakeholder group evaluating the policy. You might consider the views of different groups regarding traffic flow, casino gambling, or drinking laws.</li>
  <li>Show how an evaluator might select criteria of program success that are easily measured, but that miss the central point of the program. If you have trouble starting, think about the criteria of success for a  Little League baseball team or a volleyball coach in a middle school. Those should be easy; now work with the meaning of success for college teaching, counseling, law enforcement, or other important community activities.</li>
  <li>A local health care center participated in a program to teach volunteer community residents how to use computers to find health information posted by the center on its Internet site. Some stakeholders postulated that this effort would result in fewer cases of low birth weight babies in the community. Is this  a plausible result of such a program? What might be more plausible outcomes from such a program?</li>
  <li>What are some values and some limitations to the contributions that potential participants can make to specifying the criteria of successful programs from which they might receive services?</li>
</ol>

<h2 id="additional-resource-2">ADDITIONAL RESOURCE</h2>

<p>Mertens, D. M., &amp; Wilson, A. T. (2012). <em>Program evaluation theory and practice</em>, New York, NY: Guilford.</p>

<p>This comprehensive work provides detailed information, examples, and guidelines for those learning  the field as well as for seasoned evaluators. In particular, the second of four sections presents a framework of paradigms, theories, and branches to explain the range of models and approaches used by different evaluators as well as for different settings and needs.</p>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
