<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Program Evaluation Chapter 7-9 | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Program Evaluation Chapter 7-9" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="``` Sub CompareRangesAndCopy() Dim sourceRange As Range Dim comparisonRange As Range Dim output As Range" />
<meta property="og:description" content="``` Sub CompareRangesAndCopy() Dim sourceRange As Range Dim comparisonRange As Range Dim output As Range" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/15/Program-Evaluation-Chapter-7-9.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/15/Program-Evaluation-Chapter-7-9.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-15T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Program Evaluation Chapter 7-9" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-15T00:00:00+09:00","datePublished":"2023-08-15T00:00:00+09:00","description":"``` Sub CompareRangesAndCopy() Dim sourceRange As Range Dim comparisonRange As Range Dim output As Range","headline":"Program Evaluation Chapter 7-9","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/15/Program-Evaluation-Chapter-7-9.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/15/Program-Evaluation-Chapter-7-9.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=97b06ba2048c98509e06ce5ad3ed82faa3ff7a7f">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>15 August 2023</small>
<h1>Program Evaluation Chapter 7-9</h1>

<p class="view">by JAMES R. DUDLEY</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sub CompareRangesAndCopy()
    Dim sourceRange As Range
    Dim comparisonRange As Range
    Dim output As Range

    ' ソースデータ、比較データ、出力先のセル範囲を設定します
    Set sourceRange = Worksheets("Sheet1").Range("A1:I62")
    Set comparisonRange = Worksheets("Sheet2").Range("A1:I62")
    Set output = Worksheets("Sheet3").Range("A1:I62")
    
    ' ソースデータをコピーし、書式を出力範囲に貼り付けます
    sourceRange.Copy
    output.PasteSpecial Paste:=xlPasteFormats
    Application.CutCopyMode = False
    
    Dim i As Integer

    ' セルごとに値を比較し、出力範囲に値をコピーします
    For i = 1 To sourceRange.Cells.Count
        If sourceRange.Cells(i).Value = comparisonRange.Cells(i).Value Then
            ' 値が一致する場合、ソースデータの値を出力範囲にコピーします
            output.Cells(i).Value = sourceRange.Cells(i).Value
            output.Cells(i).Interior.Color = RGB(0, 255, 0) ' 绿色
        Else
            ' 値が異なる場合、比較データの値を出力範囲にコピーし、背景色を赤に設定します
            output.Cells(i).Value = comparisonRange.Cells(i).Value
            output.Cells(i).Interior.Color = RGB(255, 0, 0) ' 赤色
        End If
    Next i
End Sub

</code></pre></div></div>

<h1 id="7monitoring-the-implementation-and-the-operation-of-programs">7 Monitoring the Implementation and the Operation of Programs</h1>

<h2 id="the-structure-of-this-chapter">THE STRUCTURE OF THIS CHAPTER</h2>

<p>As you have already seen, this edition uses the many advantages of online materials as an important component of the textbook. You have occasionally been directed to the eResources for a few elements that, because of size or other characteristics, are better online than in print. In the case of one of the central elements of this chapter—information systems—the usual pattern of having online materials supplement print is reversed. Learning about ways to enter information into any current system, to manage that information, and then to use that information for any of a number of elements like monitoring the program, reporting on outcomes, and improving the program’s efficiency virtually requires a dynamic interaction with computer systems rather than just reading text or viewing small, static images.</p>

<p>So, this chapter begins in the usual way—describing the element of program evaluations known as monitoring the implementation, providing a good overview of the area and a general introduction to the elements of it. And the chapter concludes in the usual way, describing potential problems to avoid, before providing a relevant case study that especially illustrates the theme of this chapter, followed by a profile, summary, and questions. But for a substantial portion of the content of this chapter, you are directed to the eResources to learn about information systems with materials that are a better match to what is involved in using those systems than can be provided in a single chapter of a printed textbook. Among other things, a common request to program evaluators currently is for advice regarding what systems an agency might purchase to manage their data. Although the eResources module will not make you an information systems expert, you will gain enough exposure to the relevant points to be able to guide clients in a responsible way. Further, technology is changing so quickly that substantial developments are likely in such a short time that printed information will quickly become more and more dated, whereas the eResources will be updated on at least a yearly basis.</p>

<h2 id="introduction-to-monitoring">INTRODUCTION TO MONITORING</h2>

<p>A classic statement that program monitoring “is the least acknowledged but probably most practiced category of evaluation” (ERS Standards Committee, 1982, p. 10) is still relevant today. The most fundamental form of program evaluation is an examination of the program itself—its activities, the population it serves, how it functions, and the condition of its participants. Program monitoring includes an assessment of the resources devoted to the program and whether this effort is implemented as planned. It is a serious mistake to plan an evaluation that focuses only on the long-term results, ignoring its activities. Careful program monitoring yields “impressive results” (Lipsey, Crosse, Dunkle, Pollard, &amp; Stobart, 1985).</p>

<p>Monitoring is a basic activity of any organization. Businesses monitor their performance by  counting receipts at the end of every day and by performing inventories of stock. Human service managers monitor    their activities in some ways similar to those used by for-profit businesses. However, education, health care, criminal justice, and welfare agencies are designed to change the people served in some way, not simply to please them, a critical difference (Weiss, 2002). Consequently, the end products of human services—well- functioning people—cannot be measured as reliably, validly, or easily as store inventories or account balances. The place to begin is to ask whether a program has been implemented as planned. If messages showing why using cell phones while driving distracts drivers are not published or broadcast, there is no reason to evaluate the impact of the material. If drug abusers do not attend a drug rehabilitation program, then there is no need to ask whether the program reduced drug abuse.</p>

<p>Managers need basic information to facilitate their planning and decision making, to anticipate problems, to verify that program plans have been carried out, and to justify continued support for the program. If it is discovered that the people receiving the services of a program have unmet needs that are quite different from the needs a program was planned to meet, the program must be altered. An increase in the participant dropout rate would suggest problems. Without a program-monitoring system, managers might be slow in noticing the problem until it is severe and difficult to correct. The techniques described in this chapter apply especially to programs expected to remain in place. Many programs such as college counseling centers, community mental health centers, and organizations serving the needs of homeless people are expected to continue to offer services for years; these organizations need convenient access to data describing the service.</p>

<p>Evaluators have shown the value of monitoring the process of providing a product or service, not just evaluating the final outcome (Mowbray, Bybee, Holter, &amp; Lewandowski, 2006). A management information system provides information showing that the program is operating as planned. Keeping track of operations is similar to following progress during an automobile trip as we watch for route markers and familiar landmarks. While planning a trip, most drivers take note of intermediate locations in order to monitor their progress and to be sure they stay on the correct route. We don’t simply plan a trip, aim the car, drive for four hours, and ask, “Are we there?” The last point is so obvious that it is hard to understand why so many service providers, program planners, and evaluators fail to apply this principle to program evaluation. All too often an evaluation plan includes only an assessment of success in achieving the ultimate outcome, not the services actually provided or the intermediate steps needed to reach the desired outcome (Gehrs et al., 2004; Lipsey, Petrie, Weisburd, &amp; Gottfredson, 2006). Indeed, it remains true that some  program  sponsors  do  not  even  think about an evaluation until a program is well underway.</p>

<h2 id="monitoring-as-a-means-of-evaluating-programs">MONITORING AS A MEANS OF EVALUATING PROGRAMS</h2>

<p>Some of the information that evaluators summarize for program staff are fairly straightforward descriptions of the clients served and the services provided. At times, managers and staff seem to believe that such information is unnecessary because they think that they have an accurate and complete understanding of the service they provide. However, it is common knowledge among evaluators that managers and staff often hold opinions that are incorrect. Agency counselors overestimated the median number of treatment sessions clients came to (Posavac &amp; Hartung, 1977; Pulford, Adams, &amp; Sheridan, 2008). Solberg et al. (1997) described the need for better information systems in medical care, commenting that “… most of the defects in the current process of care for patients with diabetes or other chronic diseases are the result of an inconsistent, disorganized approach <em>that is the natural result</em> of relying almost entirely on the memories of individual clinicians” (1997, p. 586, italics in original). When information systems have been implemented, improvements in care have resulted (Ricciardi &amp; Tabb, 2006; Williams, Schmaltz, Morton, Koss, &amp; Loeb, 2005). Because it is so easy to be misled by our impressions, a summary of the client population and the services provided would be an appropriate first step to demonstrate effective program implementation and to facilitate effective management.</p>

<p>A description of clients and services can be done once as a snapshot of the agency or as a regular activity in order to track the agency over time. Managers of smaller and newer programs may benefit from summaries of information in the records; this process is time-consuming and sensitive because files contain far more material than would be needed to describe the agency’s clients and the services provided. Larger programs usually need periodic reports for accrediting and regulatory agencies, each with its unique information requirements. A procedure to gather information regularly is called a management information system. The system can be used at any time by staff. Evaluators can periodically summarize patterns and trends to track the activity of the agency. Further, many agencies have already instituted electronic medical records systems.</p>

<h2 id="what-to-summarize-with-information-systems">WHAT TO SUMMARIZE WITH INFORMATION SYSTEMS</h2>

<h3 id="relevant-information">Relevant Information</h3>

<p>The information gathered must be central to the purpose and mission of the program. The appropriate information can help set staff levels, satisfy accreditation standards, plan for space needs, and monitor quality. Effective evaluators gather only information that is important, not information that might be simply interesting. Because every piece of information recorded increases the costs of data collection and analysis, evaluators guard against including information that is not of use to the agency.</p>

<h3 id="actual-state-of-program">Actual State of Program</h3>

<p>An information system must describe the actual state of the current program. Evaluators are careful to distinguish between the program as described and the program as administered. Many programs that look very good on paper fail because staff did not or could not follow the design or procedure as planned. Reporters visited Chicago city parks during periods when activities were officially scheduled, only to find field houses locked or abandoned (Stein &amp; Recktenwald, 1990). If the reporters had limited their study to the official activity schedules, they would have assumed that the programs were being offered. There is no excuse for an evaluator to permit written plans, no matter how official, to substitute for on-site observations.</p>

<p>The information necessary to describe the services rendered includes (1) the type of services offered by the program (such as job training, group therapy, immunizations), (2) the extensiveness of the program (such as hours of training or therapy, skills to be learned, facilities), (3) the number of people participating in the service, especially the proportion of people completing education or therapeutic programs, and (4) indicators of quality. To be useful the system must permit convenient retrieval and summarization of information. Expected numbers of program participants are usually included in plans; such numbers could be used as benchmarks, providing a clear indication whether implementation faithfully reflected the plans.</p>

<h3 id="program-participants">Program Participants</h3>

<p>If a program is well planned, the population to be served would have been specified carefully and the program tailored to meet the unmet needs. An information system should document the identity and the needs of those using the service and compare these findings to the program mission. In other words, the fit between the program and the needs of those using it should be examined. If the fit is not close, changes in the program can be considered. If the agency had not defined the group to be served, a description of the people using the agency may be especially enlightening.</p>

<p>Program participants are routinely described by gender and age. The choice of additional information depends on the specific nature of the program. Such information may include where the people live, the major problems for which they are seeking assistance, the source of referral, and ethnic background. In some settings the barriers to obtaining services might be important to know. Transportation difficulties, childcare needs,      and length of time spent on the waiting list might be useful in describing a program and suggesting improvements.</p>

<h3 id="providers-of-services">Providers of Services</h3>

<p>The information system should include a description of who gives the services. In some settings, less-well- trained people work under someone else; a casual examination of the program might imply that the supervisor was the provider. For example, a licensed psychiatrist may sign insurance forms, but employ social workers to provide therapy. To understand a program knowing who actually works with the participants would  be essential. Staff may be underqualified or overqualified.</p>

<h2 id="program-records-and-information-systems">PROGRAM RECORDS AND INFORMATION SYSTEMS</h2>

<p>When appropriate information is gathered consistently, the types of reports illustrated in this chapter can be prepared without great cost.</p>

<h3 id="problems-with-agency-records">Problems with Agency Records</h3>

<p>It is impossible to monitor a program that does not keep adequate records (Vroom, Colombo, &amp; Nahan,     1994). However, it is no secret that the records of many human services are often abysmally incomplete.  Lurigio and Swartz (1994) described the difficulties they had as they tried to  work  with  the  inadequate records of jail-based drug treatment programs. Records are often in poor condition because recordkeeping is  dull and does take time. Individuals providing human services often could provide more services than they   have time to do and, thus, view keeping records as time taken from clients, students, or patients. Records were less important when impressionistic evaluations of success and effort were sufficient. However, impressionistic evaluations no longer satisfy the requirements of regulatory and accrediting agencies. It is critical to maintain and summarize agency records.</p>

<h3 id="increasing-the-usefulness-of-records">Increasing the Usefulness of Records</h3>

<p>Methods of summarizing information must be developed to increase the usefulness of  files  because  information that cannot be summarized is not helpful to evaluators or program stakeholders. Information systems permit the regular summary of participant descriptors, services provided, and quality  indicators  without manually searching through files.</p>

<p>When planning an information system, the program director, staff, and other information users develop a list of essential variables useful in managing and evaluating the program. A point repeatedly stressed in this book is that information is gathered only when it is useful. If there are legal requirements to obtain certain information, or if decisions are to be made on the basis of information, then it should be gathered. If material could reveal suspected negative side effects, it should be included as well. The challenge to information system developers is to summarize such information in helpful ways. Managers and staff need relevant and useful information, not just lots of details (Ackoff, 1986).</p>

<p>At this point, proceed with the “Information Systems” module on eResources, and then return to this point.</p>

<h2 id="avoiding-common-problems-in-implementing-an-information-system">AVOIDING COMMON PROBLEMS IN IMPLEMENTING AN INFORMATION SYSTEM</h2>

<p>Guiding staff on their use of an information system is a major effort; there are a number of possible pitfalls in the process.</p>

<h3 id="guard-against-the-misuse-of-the-information">Guard against the Misuse of the Information</h3>

<p>As with all information, a report comparing individuals with each other can be misused. Critics fear that providing summaries of information usually hidden in files allows such data to be used in a vindictive manner; however, most evaluators believe that providing more information reduces the opportunities for directors to behave autocratically. Most unfair managers restrict information, just as despotic rulers control the media in   the nations they control. Nevertheless, it  is crucial to remove fear  from an evaluation system. When people  fear that information will be used against them, they are motivated to hide their errors (Campbell, 1987; Deming, 1986; Edmondson, 1996, 2004; Hilkevitch, 2000) and to avoid asking for assistance.</p>

<p>Managers must also guard against the possibility that acceptable variations in style will appear as deficiencies. Some therapists may encourage clients to leave therapy sooner than others do. They may do this because of theoretical convictions, not because of inadequate therapeutic skills. In the table on page 16 of the eResources module for this chapter, this confusion was avoided by reporting separately the number of clients quitting after less than three counseling sessions. Even therapists who plan on short-term therapy expect to serve clients beyond two sessions. A related danger is the temptation to use the system in a misguided attempt to monitor staff work at such a detailed level that staff members simply report fictitious information.</p>

<h3 id="avoid-setting-arbitrary-standards">Avoid Setting Arbitrary Standards</h3>

<p>It is critical to avoid setting arbitrary standards for the staff to meet. The indexes developed using information systems can indeed reflect quality services, but they reflect only some aspects of the service. If pay and  promotions of staff are based on these partial views of quality, staff will find ways to meet the standards at the expense of other aspects of their work. Primary and secondary school teachers have learned how to structure lessons to raise standardized test scores that are thought to represent the outcomes of effective schools, and college teachers know that assigning higher grades increases the teaching ratings made by students (Gillmore &amp; Greenwald, 1999). Campbell warned that focusing on a particular variable would increase the probability that it would become invalid, indeed corrupted (see Shadish, Cook, &amp; Leviton, 1991, p. 141). Management information is used most effectively when combined with additional measures of quality.</p>

<h3 id="avoid-serving-the-needs-of-only-one-group">Avoid Serving the Needs of Only One Group</h3>

<p>People filling different roles in an organization have different informational needs  because  their  responsibilities differ. The needs of the accounting department differ from those of the billing department, which in turn contrast with those of the service delivery staff. By working closely with only one or two such groups, evaluators run the risk of providing information useful only to some groups. If that happens, the information system would be seen as irrelevant by those whose needs remain unmet by the system; such    people will resist providing information for the system, thus reducing its usefulness.</p>

<h3 id="avoid-duplicating-records">Avoid Duplicating Records</h3>

<p>Human service staff will not cooperate happily with an information system that duplicates information already reported and stored elsewhere. It is undesirable to simply impose an information system as an addition to existing recording procedures; integrating the information required by the old system with the automation of the new system is preferable. Staff members may question the accuracy of the new system. Because staff can seldom describe the population they serve in quantitative terms, they will sometimes be surprised at the summaries from a management information system. As the validity of the system is demonstrated, staff may come to appreciate the added knowledge provided by an information system.</p>

<h3 id="avoid-adding-to-the-work-of-the-staff">Avoid Adding to the Work of the Staff</h3>

<p>An integrated information system was developed in an agency providing family and marriage counseling (Savaya, 1998). Although the staff members were consulted in development of the system and were given training in its use, they were expected to complete assessment forms on their own time. Furthermore, the counseling staff was expected to change the way they assessed the success of their clients. The limited cooperation of the staff made it impossible to use the information system effectively. In contrast, to develop the most useful system, the users must be involved in its design. The information needed, the manner of summarizing it, the frequency of reports, and other issues should be determined in cooperation with those who are expected to use the information. The evaluator may be enthusiastic about getting the system operating; however, learning what is useful requires patient work with the staff and administrators.</p>

<h3 id="avoid-obsession-with-technology">Avoid Obsession with Technology</h3>

<p>Although almost no program can be run successfully in these times without electronic records at some level, it is important to find the balance between using technology as a very helpful set of tools and also maintaining the human connection that people desire. When the use of technology has a clear benefit of savings of time or money, it may be obvious to clients that they share in those benefits. For example, if a survey to document the level of symptoms before treatment is e-mailed to clients to save the time of completing a lengthy survey in the office, many clients will appreciate the convenience of being able to answer somewhat sensitive questions on their own schedule and in the privacy of their home. Further, if a comment indicates that those clients who want to complete a hard copy survey at the agency may do so, any who are not comfortable with the online version will not feel forced to complete it. But when the use of technology functions only to keep clients at a distance, any benefits are overshadowed by the obvious problems with such an approach. Although the line between these two areas certainly changes over time, the main point is that paying attention to the message conveyed in the use of technology—whether it helps the connection between clients and the agency or serves only to widen the gap between them—is an important element to consider.</p>

<p><a href="#bookmark1">Case Study 7</a> reports on the program evaluation of a federally funded program that focused on the health of mothers and infants. The program had been going on for several years and was in the second year of the grant cycle when Data Driven Solutions, LLC (DDS) was retained by a local governmental agency for the evaluation. As you will see, although the evaluators initially took the approach of a general program evaluation, issues of implementation quickly became central, and the section on implementation takes center stage in the case study. Some evaluations begin with a focus on implementation, whereas others develop it in response to the specific program.</p>

<h4 id="case-study-7">CASE STUDY 7</h4>

<p>Adapting to the Needs of the Program</p>

<p>The program addressed the health of mothers and infants through education on issues like healthy behaviors, breastfeeding, and safe sleep practices for infants; referrals; and support for prenatal and postpartum women.</p>

<p>Meeting Needs</p>

<p>Although everyone has a general need for good health, expectant and new mothers as well as infants have particular vulnerabilities to threats to their health, and these are substantially greater in vulnerable populations such as those with low income. In this particular case, the granter did not require extensive documentation of the details of these needs, so general recognition of these needs was sufficient.</p>

<p>Implementation</p>

<p>As part of the program, a large amount of data was collected on each of the participants of the program, including demographic data, background data, and data to address specific benchmarks of interest to the federal government, such as percentage of fatherhood involvement and percentage of participants with health insurance. When the program originated, a database was developed and data were entered from paper records of visits and meetings with the participants. The structure of the database supported the specific reports required by the federal government and those reports were easily generated.</p>

<p>The evaluators quickly discovered, however, that a series of changes, many of which were relatively minor and could be addressed individually, resulted in a cumulative effect that completely transformed the report process. First, additional fields had been added at various times to the database. In itself, this change was neither unexpected nor a problem. But next, some of the forms used to collect the data changed and the process of data entry also changed. Again, each individual change was not a major problem, but the series of shifts meant that the likelihood of errors and other problems began to increase more quickly. As readers might imagine, when paper forms and computer data fields are arranged in a similar layout, it is fairly easy to connect the right written information with the right computer entry. When new fields are added and then the forms are changed so both the words and the arrangement are different, it can be much easier to enter a number in the wrong place.</p>

<p>In many ways the “final straw” came when new benchmarks were established and yet another new set of forms were required. At this point, an entirely new database was needed to match the other elements in a way that supported efficient and accurate recordkeeping, rather than trying to add more layers of fixes to previous layers. Although one aspect of implementation, the nature of the actual services, had not been directly affected in a major way by these many changes, documentation of the services and the way that relevant data regarding those served were collected, a second aspect of implementation, was completely transformed.</p>

<p>To complicate the evaluation even more, however, a final report using data in the old database system was still required. At this point,       the results provided by the outdated automated formulas in the old system seemed blatantly inaccurate to program staff, and they asked           the evaluators to examine these findings. A detailed inspection of the old database found problems of missing data, obvious errors such as impossible values for given measures, and related problems. Some of the errors could be corrected by reviewing other records or tracing information entry backwards, whereas in other cases, it was clear that data had been lost, and those missing cases were ignored. The results calculated after this lengthy process appeared to both staff and funders as sufficiently accurate under the circumstances, which was a very successful outcome.</p>

<p>As noted in the text, one form of program evaluation is an “implementation evaluation” that is structured primarily to assess how a program has been initiated, to document what has actually been implemented, and to set the stage for a later accounting of outcomes. In this case, the original intent of a general evaluation was modified due to the overwhelming need to address the recordkeeping aspect of implementation, although some elements of a broader evaluation remained.</p>

<p>Stakeholders</p>

<p>Main stakeholders are noted as the mothers, the infants, the healthcare community, and the federal program that sponsored the services. In this case, adjusting the reporting system to the needs of the program while preventing the adjustments from interfering with continuing service to the clients was a substantial challenge.</p>

<p>Side Effects</p>

<p>Especially with the focus on the data system, no side effects were noted.</p>

<p>Improvement Focus</p>

<p>On a general scale, the focus on improvement should be fairly obvious—adapting details of the evaluation to fit the needs in an appropriate way and finding a process to facilitate changes in the organization to meet the needs of the funder. Hopefully it is clear that adjustments to an evaluation plan can be an excellent way to address broader evaluation needs by helping the program improve for the future. It is also true that some adjustments are inappropriate, such as changing the criteria for success to make poor results look better for staff. Understanding the difference between the two types of adjustments requires good judgment and integrity on the part of evaluators.</p>

<p>Outcomes</p>

<p>In this case, because the funding agency was most interested in reports about meeting benchmarks, the outcomes could be addressed appropriately even with data that were fewer and less complete than ideal. As with the above point about using good judgment to make decisions about adjusting an evaluation plan, knowing when one report in a series of reports can appropriately address a transition involves using good judgment.</p>

<p>Nuances</p>

<p>Nuances were not explicitly addressed.</p>

<p>Summary</p>

<p>As this case illustrates, the job of an evaluator may involve much more than determining whether a program is meeting its expected outputs and outcomes. In this situation, the evaluators worked on insuring all data points were included in the final results by developing an in-depth understanding of the database that contained all the raw data.</p>

<p><em>Source</em>: M. Dever (personal communication, January 25, 2018).</p>

<h2 id="summary-and-preview">SUMMARY AND PREVIEW</h2>

<p>Human service agencies can begin an evaluation with a systematic description of their programs, the amount and type of services provided, and the identity of those receiving the services. Such monitoring permits an agency to account for the funds expended and makes it easier to compare a program as implemented with the original plans. These sorts of comparisons can be invaluable in showing what aspect of a program needs additional effort or why a program never had the impact envisioned. Also, the need to complete accrediting agency and government surveys can be met more easily if monitoring for quality assurance is carried out regularly rather than only after requests for information are made.</p>

<p>Monitoring is a crucial aspect of program evaluation; it is essential to keep track of activities. However, stakeholders want to know how successful programs are in helping participants, students, or patients. The next chapters focus on methods that evaluators and stakeholders use to examine the degree to which outcomes are achieved. Such evaluations would be used with reports from the information system for a more complete understanding of the agency.</p>

<h2 id="study-questions">STUDY QUESTIONS</h2>

<p>1 How does program monitoring fit into the schematic diagram of the function of program evaluation given in Figure 1.1?</p>

<p>2 Explain how monitoring relates to formative and summative evaluations.</p>

<p>3 People sometimes object to an evaluation of effort on the grounds that monitoring activities do not focus directly on the quality of the services provided. Why does this criticism not negate the reasons for monitoring the activities making up the program?</p>

<p>4 Suppose that you managed a telephone sales office. Twenty people make telephone calls to teachers who use educational DVDs. If these potential buyers are interested in previewing a DVD, they are given a   code to permit them to preview it online for seven days to decide whether their school would buy it. How would you develop a monitoring system to keep track of what your employees are doing? How would you measure their success? What type of indexes of success would be important?</p>

<p>5 What types of program evaluation questions can be answered using an information system? What are</p>

<p>some questions that an information system cannot answer?</p>

<h2 id="additional-resource">ADDITIONAL RESOURCE</h2>

<p>Along the lines that information systems themselves are changing so rapidly that the eResources support timely updates, the best resources to supplement this chapter are also changing rapidly. See the recommendations in the eResources for print and other forms of materials for further reading and experiences.</p>

<h1 id="8qualitative-evaluation-methods">8 Qualitative Evaluation Methods</h1>

<p>All approaches to program evaluation noted in Chapter 2 and in the eResources have advantages and disadvantages. One of the major advantages of qualitative methods is its flexibility and appropriate use in innovative and novel settings. When stakeholders want information quickly or want to understand a complex program with ill-defined (even conflicting) objectives, highly structured approaches to program evaluation    may not be appropriate. Imagine that a new president of a private college has become concerned about the proportion of first-year students who, although in good academic standing, do not register for the sophomore year. The president responds to this problem by proposing the development of curriculum changes, enhancements to available student activities, dormitory reorganizations, increased access to  computer  terminals, and other steps to improve the experiences of first-year students. It is hoped that such measures  would lead more first-year students to identify with the college, continue their education there rather than transferring, and, in time, become alumni who participate in fundraising campaigns.</p>

<p>After  a  period  of  planning  and  negotiation  among  the  president’s  office,  student  life  administrators,  a skeptical faculty, admissions, and student organizations, plans are made and the program is implemented. The stakeholders would want timely information to help them adjust to these innovations. Some aspects probably need to be strengthened and others perhaps should be dropped. Since a lot of effort and funds are devoted to the program, it should be evaluated. But how?</p>

<p>The program needs to be evaluated, because without an informed evaluation, cynical stakeholders will claim evidence for failure, although others may see evidence for success. The question is how to evaluate it fairly and in a way that provides useful information for the stakeholders. The president’s office should have a central administrator keeping track of the activities sponsored by the innovation (the outputs), but more is needed because stakeholders want to know more than an information system can provide. Qualitative evaluators would supplement the numerical summaries from a relational database with direct observations of activities associated with the program; discussions with participants, with those who chose not to participate, and with staff; and examinations of program documents, other materials, and artifacts. Often this approach is called <em>qualitative evaluation</em> to distinguish it from quantitative measurement.</p>

<p>The term <em>qualitative data</em> is used in several ways. Some people think of questionnaire items referring to</p>

<p>ratings of impressions or subjective reactions as qualitative data; this is not how the term is used here. Such ratings are numerical and are treated as quantitative data. Others think of the characteristics of people (e.g., college major, on-campus vs. off-campus residence, religious affiliation, etc.) as qualitative variables, whereas most of these are still quantitative data, just at the categorical or nominal level. Other researchers use detailed coding of field notes or of unstructured conversations and counts of artifacts as the data for evaluations. Again, in these cases, the observations have been transformed into quantitative variables. Instead, qualitative evaluation is used here to refer to procedures that yield nonnumerical information that helps evaluators to understand  the  program  and  the  participants’  relation  to  the  program,  such  as  full  narratives  or  shorter explanations. To develop this understanding, it helps to learn how participants found the program, what they gained from it, and how they expect to use what they gained in the future. In other words, evaluators want to learn the participants’ stories as well as the staff members’ stories. Qualitative information helps us to interpret quantitative information and to recognize the unique aspects of different program settings (Corbin &amp; Strauss, 2008).</p>

<p>The first section of this chapter emphasizes data collection approaches particularly central to qualitative evaluation; however, many methodologists (Reichardt &amp; Rallis, 1994) urge that qualitative methods be used together with quantitative evaluation methods, as will be illustrated toward the end of the chapter.</p>

<h2 id="evaluation-settings-best-served-by-qualitative-evaluations">EVALUATION SETTINGS BEST SERVED BY QUALITATIVE EVALUATIONS</h2>

<p>The emphasis of this text is on evaluation methods that can be best used with programs having identifiable objectives that can be specified and measured quantitatively in order to learn the degree to which outcomes  have been attained. Furthermore, many evaluators believe that with enough care they can discover whether the influence of the program did or  did not cause changes in the participants. Given careful participant selection,    it is expected that successful programs would be effective if implemented at other locations.</p>

<p>Experienced evaluators often find that they are asked to conduct evaluations when these expectations cannot be met and indeed need not be. In order to develop support for a program, innovators may make their goals intentionally vague (Cook, Leviton, &amp; Shadish, 1985). Vague goals permit different stakeholders to read their own goals in the program’s objectives. Following the “letter of the law” of the suggestions in Chapter 5 might lead an evaluator to decline to conduct evaluations of such programs. However, high quality evaluations are still possible in many such settings. Before getting into specific methods, consider some other settings in which a quantitative-focused approach might be applied, but a more probing approach would provide more suggestions to improve the programs.</p>

<h3 id="an-after-school-program">An After-School Program</h3>

<p>Examine the logic model presented in Chapter 2 describing an after-school program for middle school children designed to increase their school achievement, provide skills to avoid some of the negative influences in their communities, and help them develop more respect for peers and adults. An evaluation of such a program could focus on classroom grades, high school graduation rates, and delinquency rates of those who have participated in the program compared to those who did not. However, those children coming to (or being sent to) such a program were probably doing better than other students even before the program started. Thus, a more in-depth examination of what actually happens during program times and learning what the program means to the children and their parents would be more informative. Further, understanding how the high school student leaders and the adult volunteers feel about the program would shed much light on the fidelity of the implementation. The high school students and volunteers could also tell the evaluators what the program means to other people in the community. Because teachers usually learn more than students, it is quite possible that the high school students being paid to lead might gain a great deal from this job.</p>

<h3 id="evaluating-a-political-campaign">Evaluating a Political Campaign</h3>

<p>Imagine  a  politician  running  for  a  state-level  office  who  is  dissatisfied  with  the  voters’  reactions  to  his campaign. Not surprisingly, he would not turn to a professional program evaluator for an experiment. A  political campaign is a diffuse, complicated endeavor. Although the ultimate goal of a political campaign is   very objective and quantitative, votes cast is not a useful index of campaign effectiveness if one wants to improve the campaign. An approach is needed that can provide recommendations that can be put to use   quickly.</p>

<p>The problem faced by this politician is similar to many other situations in which policy must be evaluated before the ultimate outcome index is available. Major, expensive evaluations have been criticized because it took too long for the results to become available; when the evaluations were completed, they were no longer relevant to the information needs of the policy makers. When, for example, an evaluation of a welfare reform plan (called the “negative income tax”) was completed, Congress and the White House were no longer in a mood to even consider the innovative plan, regardless of the findings of the evaluation (Cook et al., 1985).</p>

<p>These three hypothetical evaluation programs—a strengthened freshman year program, an after-school program, and a political campaign—are presented to illustrate situations that need evaluation approaches quite unlike monitoring with an information system or using the research designs to be presented in following chapters. Crucial characteristics of the situations calling for qualitative evaluations include (1) a longer cycle between program activities and outcomes than is expected for classroom lessons, medical treatments, or job training; (2) indexes of success that are based on the whole program rather than on measures of individuals   such as medical tests or ratings of improvement; (3) a need for results in a short  time;  (4)  multiple  stakeholders perhaps with conflicting values; (5) a request for suggestions for improvement rather than just an evaluation of outcomes; and (6) a concern that the social setting of the program or the program itself is unlike those found elsewhere. Most qualitative evaluations are conducted in settings with at least some of these characteristics.</p>

<p>Although this chapter highlights qualitative methods, issues that require qualitative approaches have been mentioned throughout this text. Almost all work with stakeholders, such as trying to understand what information is most important to them, is part of the qualitative approach to program evaluation. In previous chapters, the importance of verifying that a program has been implemented was stressed. Since it is impossible to know what will go wrong ahead of time, qualitative observations are important in the evaluation of implementation and in the detection of side effects.</p>

<h2 id="gathering-qualitative-information">GATHERING QUALITATIVE INFORMATION</h2>

<p>Before we examine observation and interview techniques, the central importance of the evaluator in making qualitative observations must be stressed.</p>

<h3 id="the-central-importance-of-the-observer">The Central Importance of the Observer</h3>

<p>The single most distinctive aspect of qualitative research is the personal involvement of the evaluator in the process of gathering data. In other forms of evaluation, measurement procedures are designed to dissociate an evaluator from data collection. This is attempted by using written measures of the achievement of goals or objective data such as income level or productivity. The criteria of program success are defined before the evaluation begins and measured with little involvement of the evaluation team except  to verify that  the data  are being gathered as planned with appropriate care. In this way, a form of objectivity is gained. Proponents of qualitative program evaluation, however, would argue that something is lost as well.</p>

<p>What is lost is the evaluator’s opportunity to understand the program from the inside rather than standing aloof on the outside (Fetterman &amp; Wandersman, 2004). The opportunity to respond to the information as it        is gathered is also lost. In a real sense, the qualitative evaluator is viewed as the measurement instrument. Qualitative evaluators are intimately involved in data collection so that they can react to the observations    made. Such reactions may involve adjusting the focus of an interview or even the evaluation itself. For example, it may become evident early in the process of observation that the staff’s expectations are incorrect or that the program sponsor’s objectives are different from what the program participants expect. An inflexible evaluation plan based on a misconception yields useless information.</p>

<p>Some evaluators object to qualitative evaluations due to fear that evaluations will become very subjective. The loss of the credibility of the evaluation process would be disastrous for evaluators who have striven to demonstrate that evaluations can be used in improving organizations. Qualitative evaluation can be rigorous; however, rigor means something different from what is needed for basic research. Although qualitative evaluators recognize the difficulty, even the impossibility, of finding one correct view of a program, this does not mean that all interpretations are equally legitimate; it is not true that anything goes.</p>

<p>How does one decide when a conclusion based on qualitative information is credible? Consider the tests of the validity of conclusions that are used in daily life. In a mystery story, readers can correctly conclude that   “the butler murdered the nanny” by integrating evidence from different sources into a credible, persuasive   story. Even when an eyewitness is not available, a defendant can be convicted when caught in a web of evidence. Many court decisions stand on qualitative syntheses of a great number of observations. The tests of validity of court decisions are similar to the tests of validity of qualitative evaluations. Such tests involve corroborating conclusions with evidence from multiple, independent sources; developing a sense of the correctness of the conclusions; and confirming the conclusions with people who know the program.</p>

<h3 id="observational-methods">Observational Methods</h3>

<p><em>Nonparticipant Observers</em></p>

<p>Since the goal of qualitative evaluation is to understand the program, procedure, or policy being studied, it is essential for the evaluator personally to observe the entity being evaluated. Nonparticipant observation means that observers are present in the program setting, but they serve no role in the administration or the delivery of the service. Nonparticipant observers would probably not be disruptive of the after-school program described in Chapter 2. They could develop a good understanding of the spirit of the leaders and the enthusiasm of the middle school children.</p>

<p>Qualitative evaluators seek to develop an understanding of how a program operates; they do not seek to answer predefined questions. It is impossible to specify beforehand the details that reveal how the program operates and that end up being important guides to understanding it. If it were possible to list all the things one would be looking for, qualitative evaluation would not be necessary; a checklist could be developed and a clerical assistant could be sent to make the observations. The evaluator must make the observations in order to detect what is important and discover how the details fit into the overall understanding of the program. Avoiding mental blinders and remaining open to many possible interpretations are important contributions of qualitative evaluations.</p>

<p>Using nonparticipant observers to gather information is practical when evaluators can be sure that their presence would not change the social system of the program. But the presence of observers can lead the program staff to act in guarded ways in an effort to control the impressions of an evaluator. Observers can also make staff members nervous and lower their effectiveness, as may occur if staff members had not been informed that evaluators were going to be present. When observations become part of normal operating procedures, staff members become surprisingly relaxed. Nonparticipant observations would be most feasible in settings that are relatively public, such as schools, libraries, and businesses.</p>

<p><em>Participant Observation</em></p>

<p>When the services of a program are too private to permit a nonparticipant observer to be present or when the staff members are so defensive that they would not be able to carry out their duties, it may be necessary to use a participant observer. A participant observer has a legitimate role in the program. An assistant in an emergency room would understand emergency room practices. A secretary in a personnel office or a dispatcher in a local police station could provide rich information about the effectiveness and the problems of the service being evaluated.</p>

<p>Using participant observation without the consent of the program staff would be  unethical  and  incompatible with the views of most evaluators. Approaching evaluation without the agreement of the people whose program is being evaluated violates the spirit of mutual trust that is important in effectively functioning agencies. A better approach would be to explain clearly the part the participant observer plays in  the  evaluation. Cooperation often can be obtained when evaluation is described as a way to learn about and represent the program to those who do not understand how services are delivered and what problems staff members face. Clearly there are less expensive ways to manage a program and one does not need to go to the trouble of conducting an evaluation if service improvement was not the goal of the  evaluation.  If  staff members understand that, cooperation may be obtained.</p>

<p>A variant of participant observation involves a pseudo-participant going through the system. Sometimes the experiences of pseudo-participants are indeed used to gauge the quality of normal treatment. Some stores assign staff (or  hire “mystery  shoppers”)  to act as  though they are customers  in their  own and competitors’ stores in order to assess the quality of services provided (see Stucker, 2004). People can approach public and private agencies as people would who are carrying out their own affairs.</p>

<p><em>Examining Traces and Documents</em></p>

<p>Traces refer to a wide variety of tangible items associated with living. Archeologists examine traces of societies that have disappeared. Running a program also produces many traces. The records mentioned in Chapter  4     are one form of trace, but there are numerous traces produced by any program. In schools, traces include   graded homework, teacher lesson plans, tests, litter in hallways, graffiti on walls, student club minutes, student newspapers and yearbooks, athletic trophies, and damage to desks and lockers. When evaluating a community development program, evaluators might count abandoned cars and discarded appliances and furniture in yards  or vacant lots, or look for litter in yards and alleys, as well as carefully maintained yards, flower beds, families in parks, and newly established small businesses. Pictures taken before and after the intervention might be used in evaluations.</p>

<p>Physical traces add a dimension to an evaluation that is hard to gain in other ways. Someone conducting an evaluation of a school program would gain considerable understanding of the school and community by systematically examining the traces suggested above. This understanding could not be gained without the personal presence of members of the evaluation team. Although evaluators may well consider the traces that might be observed, qualitative evaluators do not begin making observations with a prepared list of items to look for; whatever is important will be discovered through the observations.</p>

<p>The meaning of any one physical trace is nearly impossible to understand. It is the accumulation of evidence from many traces that leads to a conclusion. Furthermore, tentative interpretations are compared with information obtained through interviews and observations of behaviors. By seeking various forms of information from different sources, evaluators seek to draw conclusions that are collaborated in several ways. Qualitative evaluators borrowed the idea of triangulation from surveyors to describe this process. By triangulation qualitative evaluators mean that they use different approaches to learning about the program.</p>

<p>Documents refer to official and unofficial statements made by program  sponsors,  planners,  board  members, etc. Qualitative evaluators often learn how the program is intended to function  in  documents. Records kept by the staff and program managers are also documents of interest.</p>

<h3 id="interviewing-to-obtain-qualitative-information">Interviewing to Obtain Qualitative Information</h3>

<p>Qualitative interviewing is different from simply administering a written survey orally. A structured survey given orally remains a structured survey. Qualitative evaluators use open-ended, unstructured questions in   order to learn detailed information about programs. Interviewers want the  respondents  to  use  their  own words, thought patterns, and values when answering the questions (Patton, 1980).</p>

<p>Another method for gathering qualitative data—focus groups—has already been addressed in Chapter  6,    so that information will not be repeated here. Still, focus groups may be a good option when qualitative data is desired. Some evidence suggests that in-depth interviews provide the most cost-effective data collection (Namey, Guest, McKenna, &amp; Chen, 2016), but many evaluators find that focus groups provide important benefits. Others suggest that the method of dyadic interviews combines some of the  advantages  of  interviewing individuals with some of the benefits of focus groups (Morgan, Eliot, Lowe, &amp; Gorman, 2016).</p>

<p><em>Preparing for the Interview</em></p>

<p>Before conducting interviews, evaluators make sure that the interviewee understands the purpose of the interview and has consented to be interviewed. There is no point in trying to mislead the individual as to the purpose of the interview. Such unethical behavior may well create controversy and distract from the credibility of the evaluation report. It is a good idea to confirm the appointment for an interview a day or two beforehand. Interviewers should be on time, even early. One will have better rapport if interviewers dress according to the norms of the organization. One need not dress in a designer suit to interview a business executive, but do not show up wearing a baseball cap.</p>

<p><em>Developing Rapport</em></p>

<p>Interviewees usually are guarded as interviews begin. A personal relationship is needed to establish rapport    and trust between the interviewer and respondent. Rapport can be fostered by asking some  orientation  questions and showing accepting, friendly reactions to the answers. Starting out asking, “How did you become  a librarian?” gives a librarian a chance to talk and relax.</p>

<p><em>Asking Questions</em></p>

<p>One of the most crucial points in qualitative interviews is to avoid using questions that can be answered “Yes” or “No.” Patton (1980) provided several sets of questions that could produce useful information only if the interviewee spontaneously expanded upon answers to the actual questions. Figure 8.1 was inspired by Patton’s suggestions. As the questions in the figure show, qualitative interviewers use open-ended questions that encourage the respondent to talk and elaborate. The best questions begin with phrases such as “What is it like when … ?” “How would you describe the employees at … ?” “Please tell me how … ?” or “What happens if a client complains about … ?” Someone can refuse to answer questions completely or may provide misleading answers; however, the format of such questions encourages the informant to provide answers revealing important information.</p>

<p><em>Figure 8.1</em> Two possible sets of questions for a director of an applied social psychology graduate program. The contrasts between the two columns are the important points of this figure. Do note, however, that the qualitative interviewer would not have an inflexible set of questions ready to use. One would have the issues clearly in mind, but the actual questions asked would develop as the interview progressed.</p>

<p><em>Probing for More Information</em></p>

<p>Sometimes an interviewer senses that there is something more to be said, or the interviewee has not understood the point of the question. Conducting qualitative interviews gives evaluators the opportunity to tailor questions to the respondent. The purpose of probes is to gain more information. One behavior that will encourage respondents to answer more fully is easy: interviewers often nod their heads as the respondents speak. This sign of encouragement can often be seen during televised interviews as a journalist keeps nodding while a newsmaker talks. An interviewer is seeking information; consequently, it is important that the interviewer treat everyone in a nonjudgmental manner. One nods to encourage the speaker whether the interviewer agrees or disagrees with what is being said. A generic probe is silence. Instead of bringing up a new question when the respondent has stopped talking, the interviewer can simply remain quiet while looking at the respondent: being quiet for five seconds has a surprisingly strong impact.</p>

<p>Clearly either approach can be overdone and become artificial. More active probes are also used. The interviewer can probe by simply asking, “Can you tell me more about … ?” Interviewers may want to check on an interpretation of what was said by asking, “Let’s see, if I understand you, you said that …. If that is correct, can you tell me what happens next?” It is critical to avoid using directive or leading probes. An interviewer who interprets a tone of voice as anger would be using a directive probe if the follow-up question is, “Why does that make you angry?” A nondirective probe would be, “How did you feel when you answered that last question?” In this way the interviewer has responded to the appearance of emotion and can learn about it without running the risk of labeling it incorrectly.</p>

<p><em>Recording Answers</em></p>

<p>Although there was a time when the logistics of recording conversations were cumbersome and made too    many people uncomfortable (Lincoln &amp; Guba, 1985), nearly unobtrusive equipment and  the  pervasive  presence of other electronic devices like smartphones make it an excellent option now. Further, computer programs are available to organize the verbal material (Weitzman &amp; Miles, 1995) and voice recognition software to convert spoken language into text files has become readily accessible (e.g., Dragon Naturally Speaking, 2018).</p>

<p><em>Ending the Qualitative Interview</em></p>

<p>A structured interview ends with the last prepared question. By contrast, a qualitative interview could go on   and on. When the scheduled appointment is nearly over, when the participants are fatigued, or when the information being discussed has become redundant, it is time to quit. It is best for interviewers to summarize  the major points that have been made by saying, for example, “Here is how I understand your views. You have said that you enjoyed teaching the special freshman course, but that you received very little information from the department chair or the dean about how these courses were supposed to differ from regular sections of the same course. You said that you found that frustrating—is that correct?” The advantages  of  summarizing include getting a chance to verify interpretations before leaving and permitting respondents to expand on     some point. Finally, interviewers thank respondents for their attention and thoughtful answers. One might       ask if it would be possible to check back later for clarification if necessary.</p>

<h2 id="carrying-out-qualitative-evaluations">CARRYING OUT QUALITATIVE EVALUATIONS</h2>

<p>Although naturalistic evaluations require more involvement and creativity on the part of the evaluator than do traditional evaluation methods using surveys and checklists, the essential plan of a qualitative evaluation is  quite similar to how people go about learning anything. The steps to conducting a qualitative evaluation are given in discrete phases below; however, readers should not view these phases as a step-by-step recipe. The phases overlap and later phases provide feedback that could lead an evaluator to revise initial conclusions. Nevertheless, at different times during an evaluation, there are types of activities that are more important than   at other times.</p>

<h3 id="phase-i-making-unrestricted-observations">Phase I: Making Unrestricted Observations</h3>

<p>Qualitative evaluations begin with observations of the most crucial program events, activities, written materials, and settings. Qualitative evaluators do not seek to observe a random selection of activities or people. Often they seek out those who know more than others and can provide more information on how things work in that setting. At times, deliberately selecting extreme examples, such as some who demonstrate the most of some characteristic or behavior and others who show the least, may help accentuate relevant factors. Although there are no restrictions on the observations and interviews, evaluators try to direct the information-gathering process toward important elements of the program. Observations, impressions, and interviews are recorded in field notes. The chances of drawing erroneous conclusions increase when qualitative evaluations are based on samples of settings, people, or conditions that happen to be convenient or that are determined by program managers. Qualitative evaluators should have unrestricted access and should be permitted to gather information from all aspects of the program or agency.</p>

<h3 id="phase-ii-integrating-impressions">Phase II: Integrating Impressions</h3>

<p>The second phase, which actually begins with the first observations or interviews, is to integrate the impressions that are formed during this unrestricted observation period. From these impressions evaluators develop some specific ideas about how the program is run, how services are provided, how participants understand the purpose of the agency, and so forth. Then further observations and interviews are conducted in order to check on the accuracy of these impressions and to seek information on topics that were not explored thoroughly. With this additional qualitative information, evaluators refine their initial impressions. When additional observations no longer change the impressions, the major part of the information-gathering phase is completed. The field notes are to be saved so that others may see them. For a list of sources for online guidance and other assistance, along with active links, see the eResources.</p>

<h3 id="phase-iii-sharing-interpretations">Phase III: Sharing Interpretations</h3>

<p>As impressions are formed, qualitative evaluators share their views with stakeholders and other evaluators.    The qualitative approach has been misunderstood by some critics as subjectivity running wild. Describing tentative impressions to others who know the program and have a stake in it is a way to obtain feedback so    that impressions do not reflect the prejudgments of an evaluator. People familiar  with  the  program  can provide additional information to correct misunderstandings. Experienced, but uninvolved, evaluators can challenge interpretations that are not adequately supported.</p>

<h3 id="phase-iv-preparing-reports">Phase IV: Preparing Reports</h3>

<p>Once checks with stakeholders and colleagues verify the accuracy of the impressions that have been formed, an evaluator is able to present the descriptions of the program and to draw evaluative conclusions about the program. Reports are usually lengthy. One of the central goals of qualitative evaluations is to provide detailed descriptions of programs through the eyes of the stakeholders along with the insights of the evaluator. Reports thus typically include substantial chunks of what has been said verbatim. The job of the evaluator is then to integrate the views of many stakeholders so that everyone understands the program better than before. Although the reports contain details to communicate an understanding of the program, stakeholders who need information promptly can be given shorter, more focused reports.</p>

<p>The report is not presented as a definitive evaluation of the program since additional information can come to light later, conditions may change, and the membership of the stakeholder groups may change. The findings of qualitative evaluations can be applied in other locations to the extent that other settings are similar to the one evaluated. Since qualitative evaluators are sensitive to the many specific factors that can affect program success, generalizing findings to other settings is done only with extreme care.</p>

<h3 id="are-qualitative-evaluations-subjective">Are Qualitative Evaluations Subjective?</h3>

<p>It would not be surprising if many readers reacted to this presentation with skepticism; it just seems very subjective. Two comments are offered in response. First, some years ago the second author was discussing research methods with an experienced social psychologist. The conversation concerned the focus of this chapter—traditional quantitative research procedures versus qualitative personal observations. When asked, “If you wanted to learn about a group you knew nothing about, would you go there and live among them or would you send surveys and questionnaires?” the more senior professor responded, “Go and live there.” Although we both would have preferred direct involvement as the mode for personal learning, at that time neither of us was bold enough to supplement the quantitative techniques we used with qualitative observations. Qualitative methods permit the development of understandings that cannot be obtained with predefined surveys and checklists. When evaluators understand programs very thoroughly, their evaluations can contain reasonable recommendations that are likely to be used.</p>

<p>The second response to the concern about the subjectivity of qualitative evaluations centers on the question of how objective other evaluation approaches are. Subjectivity is not observed in the scoring of surveys or in the analyses of program records. However, qualitative evaluators are quick to argue that the choice of variables to measure and decisions about what is important to control and what does not need to be controlled can also affect the findings of evaluations (Guba &amp; Lincoln, 1989). Although the ways prejudgments can affect evaluations are different for quantitative and qualitative evaluations, evaluators must be equally wary of unexamined assumptions regardless of the form of evaluation (Reichardt &amp; Rallis, 1994). Let the methodologist who has no shortcomings cast the first stone.</p>

<h2 id="coordinating-qualitative-and-quantitative-methods">COORDINATING QUALITATIVE AND QUANTITATIVE METHODS</h2>

<p>For most evaluators, the question is not whether to use qualitative methods <em>or</em> quantitative methods; the question is how to use the methods so that they complement each other in the best ways possible (see Campbell, 1987; Maxwell, 1985; Reichardt &amp; Rallis, 1994; Rossman &amp; Wilson, 1985; Silverman, Ricci, &amp; Gunter, 1990). Depending on the evaluation questions to be answered, the relative emphasis on each approach varies. Light and Pillemer (1984, p. 143) write, “The pursuit of good science should transcend personal preferences for numbers or narrative.”</p>

<h3 id="bringing-numbers-to-life">Bringing Numbers to Life</h3>

<p>Although a political campaign requires primarily a qualitative evaluation and rapid feedback, the candidate would probably also like a careful accounting of the allotment of campaign funds and a quantitative analysis of opinion polls. The effectiveness of an experimental medication requires primarily quantitative measures indicating improved health and long-term follow-up of patients’ mortality and an enumeration of side effects. Medical research is improved when the focus on length of life is complemented with an examination of the quality of life (McSweeny &amp; Creer, 1995). Suppose that the after-school program mentioned as this chapter began did show an increase in rate of high school graduation for participants. All stakeholders would be  pleased, but to supplement that finding with some stories of how the program helped a student, even a story from a parent of a participant, would make that finding memorable and compelling in a  way  that  the  difference between two percentages cannot.</p>

<h3 id="getting-insights-from-the-most-successful-participants">Getting Insights from the Most Successful Participants</h3>

<p>The success case method was mentioned in Chapter 2. If the participants in a job training program wanted to learn how to make the best use of the program, they might learn from those who have completed the program and obtained a stable job. If the staff of such a program wanted insights in how to improve the program, they too might gain some ideas from those who have done well. Of course, it is true that some participants do well after a program for reasons that have nothing to do with their experiences in the program. A qualitative evaluator looking for successful participants would want to note that some participants did not really need the program. But among those who appear to be typical participants when they started, there are some who have done particularly well. Those former participants might well help staff to learn which activities are less helpful and which are more helpful. They can also tell the staff how they used the skills they were learning. How did they go beyond the activities defining the program? Although the staff has resources that the participants do not have, staff members cannot completely put themselves into the shoes of the participants. Only a qualitative evaluation approach can aspire to gaining insights using such an approach because the critical variables and insights cannot be anticipated and, therefore, cannot be discovered using a survey (see Brinkerhoff, 2003). Note that the strength of such an approach does not focus on the outcomes of the typical, or average, participant as so many evaluation methods do. This is not to say that average results are not important; they are. But creative ideas are seldom suggested by a focus on average outcomes. And although it is not always necessary, sometimes getting insights from the least successful participants may also provide important information.</p>

<h3 id="gaining-insights-from-what-works-well">Gaining Insights from What Works Well</h3>

<p>The qualitative evaluation method called appreciative inquiry (AI) was adopted from organizational development (Preskill &amp; Coghlan, 2003). One might begin by asking, for example, “What are you especially pleased with in the after-school program?” The assumption on which AI is based is that in every organization something works well. The best approach to improving an organization is to focus on what works well; a  critical job for the evaluator is to learn what works well and to help stakeholders recognize that and build upon it. Further stakeholders are asked what they would like to see in the future, what would be ideal. Other approaches seek to learn how the organization functions. This can be achieved by sampling stakeholders and seeking those who are well acquainted with the program; by contrast, AI evaluators seek to include as many stakeholders of the program as possible because for the program to develop positively, many people must catch the vision of possible futures that they indeed build. AI proponents fault many other approaches to evaluation as focused too much on problems, thus contributing to demoralizing stakeholders. More will be said on this topic later in Chapters 13 and 14 on Evaluation Reports and Encouraging Utilization.</p>

<h3 id="changing-emphases-as-understanding-expands">Changing Emphases as Understanding Expands</h3>

<p>An evaluation planned initially as a quantitative evaluation can be enriched by adding a qualitative component when unexpected, negative side effects are discovered. Often evaluators are slow to notice side effects because unexpected effects, good or bad, cannot be anticipated. Evaluation procedures also change when observations cannot be made as planned. In order to make any interpretations of the data it may be necessary to utilize a more qualitative approach.</p>

<p>In a similar fashion, a qualitative evaluation can become more quantitative as the evaluation questions become focused and clear. Once qualitative impressions are well developed, it might be possible to form hypotheses about the expected pattern of some quantitative measures of program outcome. In this way, qualitative evaluators can strengthen some aspects of their conclusions.</p>

<h3 id="the-evaluation-questions">The Evaluation Questions</h3>

<p>Evaluations have been classified as evaluations of need, process, outcome, and efficiency. Certain questions are more likely to require an emphasis on qualitative methods, whereas others will require quantitative methods. It is not possible to compare program costs with outcomes when using only qualitative information. However, most evaluations have multiple purposes. Implementation must occur before one can expect outcomes to be observed. Evaluation of implementation can be quantitative, such as determining the number of sessions, the number of participants, and the proportion of participants that met the official eligibility criteria. But it is also important to know how healthy the interactions between staff and participants are, and how participant families make the decision to participate. It is not unknown for intra-staff conflicts to develop or for staff members to defend what they consider to be their turf and thereby damage the program. In the areas of social and health services, such issues are very important in learning how to improve programs. Since so many evaluation questions are complex, it seems quite reasonable to expect that those involving both qualitative and quantitative approaches would be better than evaluations limited to one or the other (Datta, 1994).</p>

<h3 id="cost-of-evaluation">Cost of Evaluation</h3>

<p>One concern with qualitative methods is that the cost, especially for an evaluation of a major program with sites throughout the country, would be too great. Sending observers to the sites or recruiting and training individuals in many locations would be difficult and indeed quite expensive compared to sending surveys to and obtaining records from the sites. The expense is compounded by the open nature of qualitative evaluation. Someone commissioning a qualitative evaluation must trust that the work will produce useful information even before the evaluators know what observations will be made or how the observations will be used in reports (Lincoln &amp; Guba, 1985). One strategy to contain costs is to combine qualitative methods with quantitative methods and use the two methods together to verify particular issues such as an appropriate assessment of needs or the credible implementation of the program. In addition, some approaches to qualitative analysis that take advantage of newer technology reduce the costs of time and money substantially (Neal, Neal, VanDyke, &amp; Kornbluh, 2015). With smaller programs, the distinct advantage of richer data may be much more valuable than a slight increase in cost.</p>

<h2 id="philosophical-assumptions">PHILOSOPHICAL ASSUMPTIONS</h2>

<p>Some proponents of qualitative evaluation methods view the use of qualitative research procedures as flowing from a view of knowledge that is distinctly different from that held by evaluators who focus on quantitative approaches (Lincoln, 1990). At the most extreme, these writers adopt a constructivist  philosophy  that  considers reality to be only a function of social agreements rather than something outside of people that can be studied scientifically (see Hedrick, 1994). Guba and Lincoln (1989, p. 13) asserted that “there is no reality except that created by people.” On the other hand, they have also said, “ … there are external constraints that limit what can be agreed upon” (Lincoln &amp; Guba, 1985, p. 85). Many observers feel that in practice even the most partisan supporters of qualitative-only evaluations don’t limit themselves in ways that one might expect on the basis of the subjectivist philosophy they affirm in their more abstract writings (see Datta, 1994).</p>

<p>There is a second theme subject to less confusion that sets the most avid qualitative evaluators apart from the mainstream evaluation community. For most evaluators the purpose of evaluation is to improve the delivery of services to people in need by studying the implementation and impact of programs. Lincoln and Guba (1985) assert that quantitative evaluations are planned and managed in ways that bar some stakeholders from participating in the evaluation. Instead, they argue that evaluations should be consciously directed toward empowering the most disadvantaged groups in society. They call this “authentic evaluation.” A number of observers imply that empowering all stakeholders is not possible in many evaluations (Greene, 1987; Mark &amp; Shotland, 1987). There is an important tension between doing better to empower as many stakeholders as possible and recognizing the many realities that limit such empowerment. It may be that Lincoln and Guba are urging evaluators to do more than they are commissioned to do and indeed can do.</p>

<p>As demonstrated by the evaluators whom Reichardt and Rallis (1994) recruited for their edited volume,  there is a long and productive record of qualitative methods being used along with quantitative methods by evaluators who do not accept the philosophical assumptions held by some extreme advocates of qualitative evaluation methodologies (see Denzin &amp; Lincoln, 2005; Guba &amp; Lincoln, 1989). Shadish (1995) has shown   that the philosophical underpinnings sometimes presented for qualitative evaluation methods are “at best a distraction, and are at worst downright harmful” to efforts to resolve the debate over how to use qualitative methods in program evaluation.</p>

<h4 id="case-study-8">CASE STUDY 8</h4>

<p>Healing a Community</p>

<p>Many program evaluations are initiated through formal processes such as a call for proposals or a request directly presented to a <a href="#bookmark4">program evaluation organization. But sometimes the connection happens more spontaneously, as in this case. As described in Evaluator Profile 8, neither Dr. Steven Kniffley nor the police chief of a nearby city expected their attendance at a panel presentation on the</a> relationship between law enforcement and communities of color to lead to a program evaluation—but it did. In addition to illustrating how qualitative methods can be the appropriate approach in an evaluation, this case also demonstrates how good evaluators must be flexible as they learn about programs, so they can respond appropriately and helpfully to the situation. If anything, these points are especially relevant for evaluators early in their careers.</p>

<p>Meeting Needs</p>

<p>Recent conflicts between communities of color and law enforcement (in response to the deaths of young Black men) highlight the need to explore the assessment of cultural competency in law enforcement departments. Police departments face significant challenges as they adapt to engaging with an increasingly diverse community that has become more competent with higher expectations. These challenges are compounded by the homogeneity of most police departments (e.g., white and male) and a “glass ceiling” effect in the representation of persons of color in leadership. Furthermore, faced with limited numbers to engage in policing growing populations, the perceived least culturally competent group (e.g., younger officers via age and experience) are perhaps the most likely to engage communities of color during high crime times (e.g., evenings), which possibly contributes to more conflict.</p>

<p>Implementation</p>

<p>The  focus  of  the  request  was  to  evaluate  the  department’s  relationship  with  the  community  they  served—specifically  to  assess  the community’s perception of the officers and potential barriers to establishing an effective relationship that would empower both community members and law enforcement to collaborate on creating safer neighborhoods as well as to determine the officers’ reflective experience of multiculturalism, readiness for cultural contact, and the presence of prejudice towards the communities they served. This broad area, approached in an exploratory manner, makes it harder to separate elements of implementation, outcomes, and side effects from each other than in most cases. To adapt the details of this case to the MISSION framework, the elements examined are described in this implementation section, the method Dr. Kniffley used is addressed in the side effects section, and the findings are noted in the outcomes section.</p>

<p>Stakeholders</p>

<p>Obviously the police are stakeholders, as, implicitly, are local government officials. In this case, the whole community should be considered stakeholders, although some members of the community were much more aware of the details of the situation being evaluated. This case illustrates some inherent challenges to connecting with the full range of stakeholders in some evaluations, as well as a creative and productive approach to that challenge.</p>

<p>Side Effects</p>

<p>As noted, this section describes the qualitative data collection process. Among other things, although the exploratory nature of the evaluation did not label particular variables as “Outcomes” and “Side Effects,” the method deliberately sought information that would not have been listed as expected outcomes in a formal research design.</p>

<p>Steven conducted three focus groups with police officers in the department and community members. Two of the focus groups were with the police officers and one group was with neighborhood association presidents and community leaders. The police focus groups consisted of six participants each. The focus group with officers engaged them in a structured dialogue consisting of seven questions that were designed to elicit narratives around the following topics: (1) city culture, (2) multicultural issues that impact their jobs, (3) perception of skill and comfort engaging with culturally diverse individuals, and (4) their assessment of the cultural climate of the department. The community member focus group consisted of 12 participants and was structured around nine questions with a similar design to elicit narratives around the following topics: (1) city culture, (2) understanding of police policy and procedures, (3) cultural competency of officers, and (4) diversity training needs for police officers. Dr. Kniffley and a team of his graduate students transcribed and coded the recorded narratives. Using a common code book, individuals on the team coded the narratives for themes and then met together and reconciled any differences.</p>

<p>Improvement Focus</p>

<p>The findings and recommendations in particular should make it clear that Dr. Kniffley approached this evaluation as a way to help the department improve in the future, rather than as a way to criticize how they were not yet perfect. Although qualitative methods are not only useful in formative evaluations like this one, the specific methods of focus groups that evoked stories from important stakeholders were particularly appropriate for this improvement-focused evaluation.</p>

<p>Outcomes</p>

<p>For the police officer focus group, a number of themes emerged from their responses. First, the officers noted significant generational differences with the department with conflict being experienced between older and younger officers. Secondly, the officers highlighted the chronic disparities between certain sections of the city that impact their perceptions of criminal activity. Additionally, several of the officers discussed the role of parents and their influence on the issues/conflict between community members and police officers. Furthermore, the officers discussed their frustrations concerning the community belief that law enforcement’s only role is to arrest and the lack of awareness of the multiple roles officers have to fulfill in responding to emergency/nonemergency situations.</p>

<p>For the community members focus group five themes emerged from the analysis. First, the community members noted that differential perception of the cultural challenges facing the city existed between White and Black community members with segregation being seen as a significant issue. Community members also mentioned issues that they perceived to be impacting community/police relationships such as lack of jobs, skepticism towards officers and their motives, and the need to build better relationships with both children and adults.</p>

<p>A number of tangible recommendations based on the results were provided to the chief such as (1) increasing transparency through quarterly community conversations, and increasing access to the complaint process, (2) improving community/police relationships through the development of a community/law enforcement task force, increased law enforcement/youth programming initiatives, and a monthly officer spotlight, and (3) increasing access to diversity training opportunities such introductory Spanish.</p>

<p>Nuances</p>

<p>Although a case could be made that the qualitative approach gathered information about some potential mechanisms, the exploratory, formative nature of this evaluation really deferred such analysis to future follow-ups.</p>

<p>Summary</p>

<p>Not all evaluations take on such difficult, even potentially explosive, issues like this one. Effective evaluations in such settings hold not only the challenge of managing real tensions, but also the hope of making a valuable difference where it is most needed.</p>

<p><em>Source:</em> S. Kniffley (personal communication, January 31, 2018).</p>

<h2 id="summary-and-preview-1">SUMMARY AND PREVIEW</h2>

<p>There are settings in which evaluators cannot use the research methods based on the social science research methods model. Some complex programs requiring an evaluation in a fairly short time cannot be approached using only quantitative methods. Direct observations are usually required to understand programs fully. Furthermore, recommendations for program improvement are most likely to be applicable when evaluators   have a good understanding of programs and their stakeholders, especially the successful participants. In addition, evaluators who demonstrate a deep understanding of the strengths of programs find their recommendations viewed as more credible than reports based on numbers alone. Qualitative methods (talking, watching, and drawing conclusions) appear simple, but are quite complicated as evaluators seek to integrate    the information they gather. Nevertheless, the use of qualitative methods in conjunction with quantitative methods strengthens program evaluations.</p>

<p>When the objectives of programs can be specified and when an impact model has been developed, one can use the traditional research methods that have been developed in the social sciences. These forms of program evaluation methodology become the focus of the next chapters.</p>

<h2 id="study-questions-1">STUDY QUESTIONS</h2>

<ol>
  <li>Most college course evaluation forms are largely made up of survey items with five- or six-point rating scales. Here are some items taken from one form:
    <ul>
      <li>The goals of the course were clearly expressed at the beginning of the term.</li>
      <li>The course helped me to develop intellectual skills, such as critical thinking or problem solving. Rephrase these into open-ended questions that a qualitative evaluator might use in an <em>interview</em> to learn about these same issues. Then suggest some responses to these open-ended questions that could not be expressed by a numerical rating.</li>
    </ul>
  </li>
  <li>Look back to <a href="#bookmark0">Chapter 7</a>. Develop some suggestions of how qualitative methods could supplement and clarify the meaning of the summaries of the quantitative material the evaluators gathered. What interview issues could a qualitative evaluator address that would be very difficult to handle using a written survey? What are some questions that appreciative inquiry might suggest?</li>
  <li>Many observers worry that internal evaluators are under pressure—sometimes overt, sometimes subtle— to produce favorable evaluations. Discuss how such pressures might affect qualitative evaluators.</li>
  <li>Think back to the program described on the first page of this book. How would an evaluator  use qualitative evaluation techniques to gather information on the effectiveness of a sexual assault prevention program offered by a college counseling center?</li>
  <li>Suppose that a six-month parent education program was developed at an  urban  university  and  successfully implemented with mothers who gave birth at a public hospital. When the same program was offered to families qualifying for public aid in several rural areas, young mothers seldom returned for even the second or third session. Use the ideas in this chapter to develop some approaches to learn why the program was valued in one setting, but thoroughly rejected in the second setting.</li>
</ol>

<h2 id="additional-resource-1">ADDITIONAL RESOURCE</h2>

<p>Patton, M. Q. (2003). Qualitative evaluation checklist. Evaluation checklists project. Western Michigan University Evaluation Center. Retrieved from <a href="http://www.wmich.edu/sites/default/files/attachments/u350/2014/qualitativeevalchecklist.pdf">http://www.wmich.edu/sites/default/files/attachments/u350/2014/qualitativeevalchecklist.pdf</a>.</p>

<p>Patton provides a thorough overview of qualitative evaluation methods and procedures. This is a checklist, so points are simply made without explanation; however, it is a remarkable guide uncluttered with philosophical jargon. As noted in the eResources, the Western Michigan University Evaluation Center—<a href="http://www.wmich.edu/evaluation">http://www.wmich.edu/evaluation</a>—with specifics such as their Evaluation Checklist page</p>

<p>—<a href="http://www.wmich.edu/evaluation/checklists">http://www.wmich.edu/evaluation/checklists</a>—provides substantial resources for evaluators at all levels.</p>

<h1 id="9outcome-evaluations-with-one-group">9 Outcome Evaluations with One Group</h1>

<p>Third grade, a class for diabetic senior citizens, and a parenting skills workshop are programs that begin and end at specific times, are directed to a group of people with similar needs, and are planned to impart similar benefits to all participants. The most common approach to evaluation in such settings is to examine how well participants perform after completion of the program. This chapter presents the value as well as the weaknesses of such simple evaluations. Then, the following chapters illustrate how these basic approaches are augmented to improve the validity of interpretations. The reader should develop a sense of how the introduction of each level of control increases the power of the evaluation to demonstrate that experiences in the program helped the program participants to achieve the program’s objectives. The methods in this and the next two chapters are more similar to common research designs than approaches to evaluation discussed so far.</p>

<h2 id="one-group-evaluation-designs">ONE-GROUP EVALUATION DESIGNS</h2>

<p>There are two basic options with one-group designs—to assess various elements only after the program or to assess elements both before and after the program.</p>

<h3 id="observe-only-after-the-program">Observe Only after the Program</h3>

<p>When teachers, nurses, administrators, or judges inquire into the success of programs, they frequently use the simplest form of outcome evaluation because they want to know how the participants are faring after a service has been provided. Do members of a job-training group have jobs three months after receiving job skills training? How many of the people who attended a smoking cessation clinic are in fact not smoking one month after completing the program? The first step in deciding whether a program is useful is to learn how many of the participants finish the program with a level of achievement that matches the program’s goals. All that is needed is a set of systematic observations of participants at some specified time after completion of a program. Note, however, that such an approach to evaluation does not even show that the participants changed during the time they were in the program.</p>

<h3 id="observe-before-and-after-the-program">Observe before and after the Program</h3>

<p>The pretest–posttest design is used when stakeholders want to know whether the participants improved while being served by a program. For example, students should read better after a year of reading instruction, and cholesterol levels should be lower after patients have begun taking a statin pill each day. The program might have caused the improvement; however, information gathered from such a design does not permit such an interpretation because there are many alternative interpretations that remain viable even when a comparison between a posttest and a pretest reveals that change occurred. These alternative interpretations have been called “threats to internal validity” by Campbell and Stanley (1963); these threats will be discussed later in this chapter.</p>

<p>This text is based on the assumption that an evaluation should be designed around the questions to be answered and the complexity of the program. Evaluation is not basic research; evaluators do not assume that an evaluation must be designed to answer the same type of questions that basic research studies would address. When programs are relatively inexpensive, not harmful to participants, and fairly standard, rigorous evaluations are not always needed (Smith, 1989). It is important, however, for evaluators to recognize when a more ambitious design is appropriate.</p>

<p>In some situations the evaluation plan cannot be any more complex than either of these two designs. The Government Performance and Results Act (GRPA) of 1993 was passed to shift the focus of government decision making and accountability away from simply listing the activities that are undertaken by a government agency—such as grants made or inspectors employed—to the results of those activities, such as gains in employability, safety, responsiveness, or program quality. Under the Act, agencies are to develop multiyear plans, analyze the extent of the achievement of the goals, and report annually. Although GRPA was initially a requirement for federal agencies, many states in the United States have adopted similar laws. The results of the activities of government agencies are evaluated by examining what was achieved, but these results can only be found on one “group.” There is no possibility of, for example, comparing scientific activity related to the National Science Foundation (NSF) with the level of activity in the absence of the NSF. Similarly there is no possibility that Social Security payments would be suspended for a year to measure the level of poverty among the elderly without Social Security. The point is that the only way such national programs can be evaluated is through an examination of key variables after programs have been implemented. As you think about the limitations of such approaches to evaluation, bear in mind that the evaluations of many large-scale, costly government programs are, at best, posttest-only or pretest–posttest designs.</p>

<h2 id="uses-of-one-group-descriptive-designs">USES OF ONE-GROUP, DESCRIPTIVE DESIGNS</h2>

<h3 id="did-the-participants-meet-a-criterion">Did the Participants Meet a Criterion?</h3>

<p>In many job-training settings there are objectively defined skills that trainees must attain in order to do a specific job successfully; in such settings a posttest-only evaluation is satisfactory. A number of writers suggest that a criterion of successful psychological counseling is the point when measures of mental health cannot distinguish between treated people and people without diagnosed emotional problems (Jacobsen &amp; Truax, 1991). Compliance with a law can be measured using a posttest-only design. Manufacturing firms establish indicators of quality called benchmarks to track how well their firms are doing compared to firms thought to be the best performers in their industry. If there is a way to set a criterion of good performance and if performance can be measured validly and reliably, then comparing participants’ outcomes with benchmarks may be a perfectly satisfactory evaluation design.</p>

<h3 id="did-the-participants-improve">Did the Participants Improve?</h3>

<p>An important concern is whether the participants changed in the direction that the program was planned to encourage. Do rehabilitation patients speak and walk better after taking part in a rehabilitation program? Case Study 9 describes how a pretest–posttest design was used to evaluate a small program to address spiritual needs among homeless adults. Certainly, improvement is a fundamental objective of programs; however, discovering that change occurred during a program should not be confused with believing that the same change would not have occurred without the program. Some stakeholders appear to believe that a statistically significant difference between a pretest and a posttest provides evidence that the program caused the improvement. A statistically significant finding can only show that the change was unlikely to reflect only sampling error, not to reveal causality.</p>

<h3 id="did-the-participants-improve-enough">Did the Participants Improve Enough?</h3>

<p>If an improvement from pretest to posttest is statistically significant, evaluators face another issue: did participants improve <em>enough</em> to demonstrate a real effect in their daily lives? If the outcome of a school   program was to change attitudes on about civic responsibility, an increase of 3 points on a 92-point attitude    test does not seem to be much of a change even if statistically significant. It was probably not substantively significant. Clinical psychologists also advocate research on assessing meaningful change for clients (Meier, 2008).</p>

<p>There are a number of ways to assess change; some way to decide whether the change is enough must be found. No approach is definitive; all require thoughtful reflection. Two approaches will be discussed: (1) when an outcome variable itself is meaningful (e.g., successfully holding a job) and (2) when the outcome variable serves as a proxy for a variable that is too difficult or expensive to measure (e.g., improved attitudes about holding a job and improved skill in filling out a job application form rather than actual success on a job).</p>

<p><em>Outcomes That Have Meaning in Themselves</em></p>

<p>Imagine a smoking cessation program. Since people buy cigarettes in 20-cigarette packs, smokers can report fairly accurately how much they smoke per day. And they can report if they have quit. Quitting would yield the most benefit, but reductions in the rate of smoking are also valuable. A meaningful index of the outcome of a smoking cessation program would be the difference between the number of cigarettes smoked per day before the program and the number after the program.</p>

<p>where <em>Number smoked</em> is the number of cigarettes smoked per day. After such an index is calculated for each participant, the mean among all participants could be found. A second index would be the percentage of participants who are no longer smoking after the program. Comparing the reductions occurring during a smoking cessation program with the reductions of other similar programs would permit one to learn if a  program being evaluated produced a substantively significant outcome. And it is probably obvious that a reduction of a full pack a day on average is a substantial one, whereas a decrease from three packs plus one cigarette a day to three packs is not.</p>

<p>Fitness programs would also permit reporting program outcomes in meaningful units. Increases in maximum weight one can lift, reductions in time needed to complete a lap around a track, and lowered heart rate while doing some specified activity are three meaningful indexes of the results of a fitness program. To decide whether the change was large enough to be of value requires comparisons to norms for heart rate appropriate to the age of the participant.</p>

<p><em>Outcomes That Are Measured Using Proxy Variables</em></p>

<p>It is often the case that evaluators cannot measure what really is the desired outcome from program participation. College faculty want to train educated people, but we settle on calculating a grade point average or using a score on a standard test. Counseling programs are planned to encourage participants to develop into well-adjusted people, but we settle on learning whether former participants score lower on tests of anxiety, stress, or depression. Such variables serve as proxies for the real thing. Because such variables could be measured with a variety of tests that vary in the number of items that they contain, neither a single score nor the mean score for a group of participants can tell us anything precisely about the value of the program. We can, however, standardize the difference between the preprogram and the postprogram means to find a standardized effect size. If the program is designed to reduce an undesirable behavior or emotion, the standard formula for effect size is as follows:</p>

<p>In words, the difference between the mean prior to participating in the program and the mean after the program divided by the standard deviation of the variable measured (e.g., anxiety) is the effect size in Cohen’s d units (see Posavac, 1998; Zechmeister &amp; Posavac, 2003). Once standardized, we can compare the effect size of the program being evaluated to the results of other programs, and there are some general guidelines to interpreting effects sizes on their own. Here is an example. A counseling program was designed to reduce test anxiety among first-year students; assume these means:</p>

<p>If the pooled standard deviation was 6.4, the effect size would be</p>

<p>Effect sizes are best calculated so that a desired effect of the program produces a positive effect size. If an increase in the value of an outcome variable reflects a desired result, then the numerator of the effect size formula would be reversed.</p>

<p>Two questions should come to mind. First, where did _s_pooled come from? The assumption is made that the</p>

<p>true standard deviations of the pretest and the posttest are equal except for random variation. You may recall that the assumption of equal standard deviations is also made when using <em>t</em>-tests and analyses of variance. So, the best information we would have in this case would be to use a value halfway between the standard deviation of the pretest and that of the posttest. Suppose that the standard deviation of the pretest was 7.1 and the standard deviation of the posttest was 5.7; the mean of these two numbers is 6.4.<a href="#bookmark8">*</a></p>

<p>The second question should be “Is 0.88 good or bad?” To decide, an evaluator needs to know the effect psychotherapy has for treating test anxiety. If an evaluator finds some reports on the  effectiveness  of  treatments for test anxiety, the effect size can be calculated for each report to gain some perspective on the findings of the program being evaluated. In the absence of such information, evaluators can  examine  summaries of many evaluations. Lipsey and Wilson (1993) summarized more than 300 reviews of evaluations  in different areas. They found that the median effect size was 0.44 with the 20th percentile being 0.24 and the 80th percentile being 0.68. In the context of these norms, an effect size of 0.88 would be considered as a large one. As noted above, this form of effect size is known as Cohen’s d. Cohen suggested that effect sizes up to</p>

<p>0.3 are small effects, those from 0.4 to 0.6 are medium effects, and those above 0.7 are large effects.</p>

<p>The third question—one that might not have come to mind yet—is this: is it valid if I make my judgments using Lipsey and Wilson’s benchmarks? Well, not necessarily. You must retain your common sense. If a workshop was offered to teach how to use a new computer program, a pretest might show just about zero skill in using the program, but a posttest might show a sizeable increase. In this context, we would expect a very large effect size because most participants started at just about zero skill. By contrast, imagine a new medication for people who have had a heart attack that reduces mortality by just 1% among people 45–60 years old. A 1% increase in skill using a program would not indicate a valuable change, but a 1% reduction in death rate due to a heart attack among middle age people is more valuable.</p>

<p>It would be helpful to have a simple way to interpret an effect size; that is not possible. However, finding an effect size does permit one to compare change across similar programs which have used similar, even if not identical, measures of outcome with similar participants. That would have been impossible if effect size formula had not been developed.</p>

<p>Program evaluators who want their work to have an impact on agencies need to be sensitive to the issue of substantively significant change, not just statistically significant change. Determining what constitutes  sufficient change is a question for evaluators to address with program stakeholders. Interpretations are more valid when additional information is available: with detailed knowledge of the program, the participants, alternative programs, and the costs of the alternatives, it becomes possible to define meaningful change.</p>

<h3 id="relating-change-to-amount-of-service-and-participant-characteristics">Relating Change to Amount of Service and Participant Characteristics</h3>

<p>In educational and training programs, one sign of an effective program is that participants who receive more service change more than those who receive less. The more primary school grades that students complete, the better their reading skills. However, eating more food is related to better health only up to a point; then it leads to problems. The healthiest have found their optimal levels. For the most part, services offered to the poor provide less than the optimal level, although some argue that this is a complex issue (McKnight, 1995). Although correlating service received with outcome may be interpreted in different ways, it can be very useful in certain situations. For example, an important study in the history of medical research followed just this strategy. In 1835 Pierre Louis (Eisenberg, 1977) reported on his comparisons of the amount of blood drawn from patients with their progress in recovering from pneumonia (then called inflammation). Early nineteenth- century medical theory led to the expectation that the more blood drawn, the better the treatment and the more likely a recovery. Louis measured the volume of blood drawn from patients then followed their condition. He found that their condition after treatment was not related to the volume of blood removed. This finding was an important influence on the medical practice of the time and contributed to the eventual discrediting of bloodletting as a form of medical treatment. Louis was unable to conclude anything about the cause or proper treatment of pneumonia, but he did identify a useless treatment.</p>

<p>Another reason to examine programs even when one is unable to identify the cause of a change is to search for characteristics of the participants that might be related to achieving good outcomes. Do men experience better outcomes than women? Do members of minority groups complete a program as frequently as majority- group clients? These questions can be explored, tentatively to be sure, using a simple research design. When policy-relevant relationships are found between outcomes and characteristics of participants, those variables should be measured systematically in any future evaluations. The finding may even have an immediate impact  if a program appears to have a good effect on some segments of the target population but little effect on other segments.</p>

<p>Although it is quite easy to discuss the value of relating personal and service characteristics to success in a program, the statistical method of correlating improvement with other variables is often misunderstood. The most intuitive approach is to subtract pretest scores from posttest scores, call the differences “improvement,” and correlate the improvement with any other variables of interest. This seemingly obvious method is not endorsed by methodologists. Raw change scores—the differences between pretests and posttests—should not  be used because they are based on questionable statistical assumptions (Meier, 2008). Furthermore,  the  complex methods suggested to deal with change scores are hard to explain to clients and other  stakeholders   and are controversial, even among experts. Yet because the lengths of some services, such as psychotherapy,  are not standardized and because people receive differing amounts of service, it is of interest to relate outcome to the amount of service received. Fortunately, when evaluators are interested in relating change to characteristics of the program participants or amount of service, there is a valid technique available.</p>

<p>One good way to work with outcome and participant characteristics is to use advanced structural equation modeling (see Raykov, 1999), although most program evaluations do not have the number of cases or other resources to support this analysis. However, there are some analyses of change that are generally possible and that will inform stakeholders. A valid way to relate change to the amount of service received or to some characteristic of the program participants involves <em>partial correlation</em>, not raw change scores (Judd, Kenny, &amp; McClelland, 2008). Partial correlation may sound complicated but it is not. First, think about the problem. Those participants who finish the program at good levels are probably those who started at better levels relative to the other participants. For example, students who do well in course X are likely to have higher GPAs prior to the first day of class. The relationship is not perfect, but we think most students expect to find the principle to be true in general. Second, it would not be surprising to find that those with the higher pretest scores turn out to have participated in a program more regularly than those with lower pretests. In college, it is generally true that students with higher GPAs attend class more regularly than those with lower GPAs. In other words, if these principles are correct, we usually expect to find positive correlations among pretest scores, posttest scores, and participation levels. If we want to learn whether degree of participation predicts better outcomes, we need to control statistically for the differences in the pretest scores, that is, differences among participants before they even began, otherwise the correlation between participation and outcome would be large just because both variables correlate with the pretest score. We can control for pretest differences by using a partial correlation between the posttest scores and participation, holding pretest scores constant.</p>

<p>See Example 9.1 in the eResources for hypothetical data for 26 participants in a 20-session job-training program. Imagine that an evaluator wanted to learn how closely attendance was related to improvement in skill level of the trainees. Partial correlation, in effect, statistically removes the effect of pretest differences among program participants on the correlation of the posttest and the level of participation. Virtually no evaluator today would calculate correlation or partial correlation coefficients using hand calculators; instead, we use computers. To do a partial correlation, the variables to be correlated (in this case the posttest score with the level of participation) are identified and the variable that is to be controlled (in this case the pretest score) is specified. For the data in Example 9.1, the partial correlation is 0.40, indicating a fairly strong relationship between improvement in skill level and number of sessions attended. Training sessions seem valuable in skill level development; the better performance of those who came most frequently is not due to    the fact that those who came more frequently started at a higher level.</p>

<p>The partial correlation of 0.40 would be seen as “fairly strong” on the basis of the values of correlations that are found in the social science research literature. As noted above, Cohen (1987) found that 0.10 was small,</p>

<p>0.30 moderate, and 0.50 large. As mentioned above, these values are guides that need to be adjusted for the context in which an evaluator is working.</p>

<p>It is crucial to note that this internal analysis is appropriate only when there are fairly sizable differences among trainees in class attendance. If all trainees had come to 19 or 20 sessions, thus reducing the differences among attendance rates, the partial correlation is likely to be low, as correlations usually are when the range of one variable is small (Zechmeister &amp; Posavac, 2003). In that case, it might appear that attendance did not matter that much. However, the proper interpretation would be that it does not matter much whether trainees attend either 19 or 20 sessions; the low partial correlation would not have implied that 12 sessions would be just as helpful as 20.</p>

<p>Although a positive partial correlation between outcome and level of participation in a program supports the value of the program, it does not eliminate all nonprogram alternative interpretations that might explain why participants improved. The reasons for caution in concluding that a program caused improvements are described in the balance of this chapter.</p>

<h2 id="threats-to-internal-validity">THREATS TO INTERNAL VALIDITY</h2>

<p>Regardless of whether an evaluation shows that participants improved or did not improve, it is important to consider several plausible explanations for the findings. Evaluators want to develop valid conclusions; if some nonprogram influence could be responsible for the findings, we want to know that. The phrase “internal  validity” refers to the degree of certainty an evaluator may have concluding that a program caused participants to improve. Many evaluations use these single-group designs. Knowing why these designs fail internal validity criteria guards evaluators from claiming too much.</p>

<h3 id="actual-changes-in-the-participants-not-related-to-program-participation">Actual Changes in the Participants Not Related to Program Participation</h3>

<p>Two threats to internal validity refer to real changes that occur in participants due to influences that are not part of the program.</p>

<p><em>Maturation</em></p>

<p>Maturation refers to natural changes in people due solely to the passage of time. Children can perform more complex tasks as they get older; people get increasingly tired as they go without sleep; and there  are  predictable patterns to the development of adults. If an evaluation uses outcome variables that can be expected to change merely with the passage of time between a pretest and a posttest, maturation could be a plausible explanation. In other words, the evaluator may well have found that real changes have occurred during the course of the program; however, the reason for the changes could be that the program lasted six months and  thus the participants are six months older and more experienced—not that the participants gained anything   from the program. Sometimes people get better over time (e.g., most people with back pain experience improvement with time with or without treatment) and sometimes people get worse  over  time  (e.g.,  as children get older a greater proportion experiment with smoking and drugs). When  using  a  single-group design, an improvement such as seen with back pain can make a program look valuable over time, but a behavior like drug experimentation can make a prevention program look bad as time goes by.</p>

<p>Finding that maturation has occurred does not mean that it is the only explanation for the changes that were observed. Nor do plausible, alternative hypotheses mean that the program had no effect. Interpretations would be easier if the evaluator could learn how much of the change was due to maturation and how much was due to the program. Using the one-group pretest–posttest design, an evaluator cannot make this distinction. Methods to estimate the change due to maturation involve testing other groups of participants or potential participants as well as testing at a greater number of time periods, as explained in the next chapter.</p>

<p><em>History</em></p>

<p>History refers to events occurring between the pretest and the posttest that affect the participants. For example, an economic recession may make even the most well-designed program to help people find jobs look like a dud. On the other hand, an economic recovery would make a poorly run program look like a winner. These concurrent national economic changes are plausible alternative interpretations of any changes found among the participants of job-training programs.</p>

<p>Some of the same approaches used to account for maturational effects can help isolate historical effects:    test additional groups and test at additional times as described in the following chapter. However, history is    less predictable than maturation. Unexpected national events that affect the outcome criteria can occur at any time or may not occur at all during a given evaluation. In addition, Cook and Campbell (1979) pointed out     that history need not refer only to events affecting all participants; when something unusual happens in a particular program group, this effect is called <em>local history</em>. Events such as an argument among staff members, a particularly unusual individual in a therapy group, or a local community disaster cannot be accounted for by   any evaluation design except for designs that are replicated in a number of different locations.</p>

<h3 id="apparent-changes-dependent-on-who-was-observed">Apparent Changes Dependent on Who Was Observed</h3>

<p>Three threats to internal validity must be considered when participants are not a random or representative sample of the people who might benefit.</p>

<p><em>Selection</em></p>

<p>Participation in many human service programs is voluntary. Self-selected people are different from the typical members of the target population. In the posttest-only form of evaluation, the process of self-selection may mean that the participants were relatively well off when they began the program or that they were especially motivated. The fact that the posttest detects a desirable state reveals nothing about the effectiveness of the program. College teachers most likely to join faculty development programs are often already good teachers. After a faculty development program, most of these teachers will continue to be good teachers. Their competence tells us nothing about the quality of the program—these teachers were better than the typical  teacher from the beginning. Observations of their achievements before the program permit evaluators to estimate the misleading effects of self-selection.</p>

<p><em>Attrition</em></p>

<p>People differ in whether they will begin a program and they differ in whether they will complete a program they began. The posttest-only design is inadequate when too many participants leave before completing the program. Students drop courses they do not like, clients quit therapy when they learn that personal growth is hard, and medical patients sometimes die. The longer it takes to carry out an evaluation, the more attrition is likely to occur.</p>

<p>The level of achievement observed at the end of a program may indicate how well the program functioned, how good the people were who chose to participate, or how motivated the people were who stayed until the  end. As a general rule, those who stay seem to find the program to be more helpful or interesting than are    those who drop out. Failing students are more likely to drop a course than are those earning B’s and A’s. Patients who die were probably among the least likely to have benefitted from a treatment program. Without      a pretest, evaluators cannot accurately gauge the effectiveness of a program. A director of a small drug-abuse program remarked informally that his program had a 90% success rate. Upon inquiry, however, it was learned that only approximately 10% of those beginning remain for the full program. Having success with 9 out of the 100 who begin a program is markedly different from having a 90% success rate.</p>

<p>As with selection, the pretest–posttest design handles participant attrition fairly well. By pretesting, evaluators know who dropped out and how they differed from those who remained. The pretest–posttest design enables evaluators to know when preprogram achievement and participant dropout are not plausible explanations for the level of outcome observed at the end of the program.</p>

<p><em>Regression</em></p>

<p>The threat of regression toward the mean is one of the hardest to understand. However, as with basic statistics, many people already have an intuitive understanding of regression toward the mean even though they might be initially puzzled by the idea.</p>

<p>If you think carefully about the following example, you will understand what regression means and why its effects usually go undetected. It is said that after a particularly good landing, pilot trainers do not compliment    a trainee, because when such a compliment is given, the next landing is usually done less well. On the other hand, the trainers severely reprimand a trainee after a poor landing in order to elicit a better landing on the    next try. Empirically, it is true that complimented exceptionally good landings are often followed by less good ones, and reprimanded bad landings are often followed by better ones. However, let us see why the  compliments and the reprimands may have nothing to do with the quality of the next landing.</p>

<p>Imagine learning a complex task. Initially, quality of performance fluctuates—sometimes better, sometimes worse. What goes into the better performances? At least two things: the level of skill achieved and chance.   Pilot trainees know that they should touch the plane down with the nose up, but they do not at first sense the precise moment to lower the wing flaps and, at the same time, adjust  the  elevators to  obtain the  proper  landing angle (Caidin, 1960). Sometimes they make adjustments a little too soon and sometimes a little too   late. Because these errors are due partially to chance, the likelihood of a trainee doing everything at precisely  the correct instant two times in a row is low. Therefore, for trainees the probability is low for two consecutive good landings—or two consecutive bad ones. In other words, a trainer should not expect two consecutive     good landings by a novice regardless of what is said or not said.</p>

<p>The effects of regression toward the mean can be further illustrated with a silly but instructive example. Choose 20 pennies and flip each one six times, recording the number of heads for each penny. Select the  pennies that produced the most “excessive” heads—five or six heads can be considered excessive. Reprimand those pennies for producing too many heads. Then flip just those pennies six times again and see if the reprimand worked. If the penny yields fewer heads than it did during the first set, the penny is now behaving    in the way it was urged to behave. On the average, the reprimand will appear to have been effective 98% of the time if a penny originally produced six out of six heads, and 89% of the time if the first result had been five    out of six heads. The binomial distribution permits the calculation of these percentages (McClave, Benson, &amp; Sincich, 2008).</p>

<p>In summary, regression to the mean warns that whenever the value of a variable is extreme, the next  measure of that variable is likely to be less extreme. This principle applies to landing airplanes and flipping pennies as well as to emotional adjustment. If people who currently are the most depressed are followed for three months, it is likely that they will be less depressed as a group. This does not mean that they will be at a healthy level of adjustment. Most likely, they will still be more depressed than the general public. However, some of the transient events that contributed to the depression will have passed, and the most depressed, as a group, will be less depressed than before—with or without therapy.</p>

<p>Is regression a threat to the internal validity of the pretest–posttest evaluation design? Not necessarily. If all the members of a population are tested before and after a program, then regression is not a problem. For example, if all children in a school are given a special reading curriculum, regression will not be a threat to internal validity. However, if only the children reading at the lowest levels on the pretest use the special curriculum, regression will be a threat to the interpretation of the pretest–posttest change. Children score low on a test not only due to poor reading skills, but also due to such random things as breaking a pencil point, having a cold, misunderstanding the instructions, worrying about an ill sister, or planning recess games. A day, a week, or a semester later during a second test, the distracting events will not be experienced by exactly the same children. Generally, these retest scores will be higher than those from the first test for the children who previously scored the worst. This does not mean that all poor-scoring children will improve; it does mean that their average will go up. By contrast, the children who scored the best will on the average do less well.</p>

<p>Regression is often a plausible alternative hypothesis for pretest–posttest change when service programs are aimed at those people who are especially in need of help. Remedial programs are not prepared for everyone, but for those who have fallen behind: reading poorly, earning low wages, or feeling emotional distress. Sometimes a screening test to select people for a program is also used as the pretest in an evaluation plan. For example, students scoring the most poorly on a reading test are placed in a reading improvement program, and a second administration of the same test or a parallel form is compared with the pretest scores. This is a poor practice; regression is a very plausible explanation for at least part of the apparent improvement. On the other hand, if poor readers or troubled clients get worse when regression effects should have led to improvement, then the evaluation can be interpreted. The correct interpretation is that the program is ineffective.</p>

<h3 id="changes-related-to-methods-of-obtaining-observations">Changes Related to Methods of Obtaining Observations</h3>

<p>Two additional threats to internal validity, testing and instrumentation, are generated by the evaluators themselves and by their observation methods.</p>

<p><em>Testing</em></p>

<p>The effect of testing refers to changes in behavior due to being tested or observed. The results from repeated uses of the same observation technique may differ simply because respondents have become more familiar with the tool. Ability test scores increase reliably on the second administration for people initially unfamiliar with the test (Murphy &amp; Davidshofer, 2005). People interviewing for jobs gain from the experience and can present themselves better on subsequent interviews.</p>

<p>A second aspect of testing effects is called “reactivity.” People behave differently when they know they are being observed. This concept was discussed in Chapter 4; however, it is worth recalling. Clients, patients, prisoners, and schoolchildren will be affected when they know someone is observing their behaviors or asking them about their opinions or feelings. Observation techniques vary in how reactive they are.</p>

<p>The pretest–posttest design is clearly weak in the control of testing effects. If participants were unfamiliar with observation procedures, scores might change on the second test. The kind of change that should be expected due to repeated observation or testing does not seem clear except for ability and achievement tests; on these improvement is expected.</p>

<p><em>Instrumentation</em></p>

<p>Instrumentation refers to the use of measurement procedures themselves. Most college instructors know that they are not totally consistent when grading essay examinations. Standards can change as the  instructor becomes familiar with how the students answered a question. The standard may become higher or lower. If measures in an evaluation are not highly objective, it would be wise not to score the pretest until after the posttest is administered. Then the tests can be shuffled together and scored by someone who does not know which are pretests and which are posttests.</p>

<p>If the measures require observations that must be made before and after the program, the examiners may become more skilled as they gain experience. Thus, the posttests may go much more smoothly than the pretests. If so, a change in instrumentation becomes a viable alternative to any conclusion the evaluation might imply. In such situations, examiners who are highly experienced before the pretest is administered are the most effective.</p>

<h3 id="effects-of-interactions-of-these-threats">Effects of Interactions of These Threats</h3>

<p>In addition to these seven threats to internal validity, interpretations can be confused by the joint influence of two threats. This could occur if parents seek special educational opportunities for their children (an example     of self-selection) because their children are developing more rapidly compared with the children of parents   who do not seek special educational opportunities for their children (an example of different rates of maturation). This effect is called a <em>selection-by-maturation interaction</em>. It is generally especially hard to predict the direction and size of effect of interactions, however.</p>

<h3 id="internal-validity-threats-are-double-edged-swords">Internal Validity Threats Are Double-Edged Swords</h3>

<p>When examining a comparison between means, it is important to consider whether  an apparently  positive effect could have been caused by a threat to internal validity increasing the difference between the pretest and the posttest. Although attention is usually focused on these threats masquerading as program effects, it is also quite possible for these threats to hide a program effect. Therefore, the lack of a program effect, when the samples are large and the measures reliable, could be due to an ineffective program or  to  an  uncontrolled threat to internal validity serving to reduce the difference between pretest and posttest. Examples include an economic downturn, participants being tired at the posttest, the most successful participants being unavailable for the posttest, initially good-scoring participants regressing toward the mean, and more sensitive observation procedures detecting dysfunction that was missed by a less sensitive pretest.</p>

<p>In the next chapter, several design enhancements will be presented as ways to eliminate plausible explanations of change that are not related to the program. Only when such explanations are implausible can good outcomes be attributed unambiguously to the program. Likewise, good experimental control would provide better understanding of apparent program failure.</p>

<h2 id="construct-validity-in-pretestposttest-designs">CONSTRUCT VALIDITY IN PRETEST–POSTTEST DESIGNS</h2>

<p>Construct validity refers to the concept (or construct) that a method of observation actually measures. When evaluators use participant self-report surveys in pretest–posttest designs, it is necessary to examine whether the program might lead to changes in how participants label their own problems and strengths. Veterans suffering from posttraumatic stress reported that a program was very helpful, but described themselves as functioning less well and experiencing more symptoms than they did before completing the program (Spiro, Shalev, Solomon, &amp; Kotler, 1989). Was the program harmful? It appeared that the program led them to a more accurate recognition of their problems and a greater willingness to acknowledge them. The program was helpful, not harmful. Evaluators have distinguished different kinds of change; to differentiate among them they are called <em>alpha</em>, <em>beta</em>, and <em>gamma</em> changes (Arvey &amp; Cole, 1989; Millsap &amp; Hartog, 1988).</p>

<p>An <em>alpha</em> change is a real change in the behavior of interest. Identifying the cause of the change is a</p>

<p>question of internal validity, as discussed above. <em>Beta</em> changes occur when respondents change their understanding of the meaning of the variable. For example, exposure to a really good golfer may lead a person to rate her own skill at golf less favorably than when playing only with friends. People with better insight into what level of skill is possible may see themselves as functioning less well than they did when they completed a pretest; this change appears numerically as a loss, but the change may indicate a more accurate self-appraisal. <em>Gamma</em> change refers to differences between the pretest and the posttest that are due to a reconceptualization   of the meaning of the variable being measured. For example, a high school teacher might not see his physical symptoms as resulting from stress; once he does, his ratings of his own stress level may change because his understanding of the meaning of his symptoms has changed. Data taken from records may also be subject to these artifacts when official definitions change over time. Variables that  can be measured objectively would  not be subject to <em>beta</em> and <em>gamma</em> changes. When a variable could mean different things to  different  participants or different stakeholders, we say that the variable lacks construct validity.</p>

<p>For evaluations requiring a pretest that includes self-reported behaviors, attitudes, or intentions, several strategies can be used to minimize these threats to construct validity, including (1) distinguishing information gathered to evaluate programs from that needed to make treatment decisions, (2) implying that what  respondents report will be validated in some way, (3) using interviewers who are experienced with both the program being evaluated and the problems that the respondents have, (4) providing explicit reference groups   for respondents to compare themselves with (e.g., “How do your leadership skills compare with  other  managers in your division?” rather than “Rate your leadership skills from 1 [poor] to 10 [excellent]”), and (5) using behavioral anchors rather than evaluative terms (e.g., “I am so depressed that I cannot accomplish anything” rather than “Very depressed”). In some cases, retrospective pretests may be better than tests given before the program begins because after experiencing the program, participants have a better understanding of themselves (Robinson &amp; Doueck, 1994). However, there is evidence that sometimes participants inflate their own program success by reducing their preprogram rating of themselves (Taylor, Russ-Eft, &amp; Taylor, 2009).</p>

<h2 id="overinterpreting-the-results-of-one-group-designs">OVERINTERPRETING THE RESULTS OF ONE-GROUP DESIGNS</h2>

<p>When evaluators use a simple design, there may be a temptation to seek to compensate for the design’s weaknesses by measuring many variables. When this happens, the chances of a Type I statistical error increase. People not trained in research methods seldom appreciate the high probability that patterns of relationships will be observed among some variables just through sampling error. Many people grossly underestimate the probability of random coincidences, but instead think that causal connections have been observed (Paulos, 1988). Type I errors refer to statistically significant differences between pretests and posttests even when no change occurred. Type I errors occur for exactly the same reason that flipping 20 coins six times each will appear to identify some coins with a tendency to produce a surprisingly high proportion of heads. We can recognize that random variation was responsible simply by repeating the set of six flips. Unfortunately, when performing evaluations new observations cannot be obtained as easily as flipping coins several additional times.</p>

<p>A second reason why Type I errors are not discovered is related to hindsight bias. Hindsight bias is the tendency to believe we could have anticipated something <em>after</em> we are aware of what in fact happened (Bryant &amp; Guilbault, 2002). In program evaluation, hindsight bias refers to the tendency to make sense of almost any set of data after having examined it. Once a puzzled graduate student (actually, the second author of this text) presented a professor with findings that seemed markedly at variance with the professor’s research. The professor examined the columns of correlations and quickly interpreted the findings in a way that coincided nicely with the previous studies. In order to do that he developed novel, elaborate interpretations that had not been part of the theory. A week later the student discovered that the computer program had malfunctioned and the initial results had been mislabeled. An alternate program produced results that were quite compatible with the professor’s work. When faced with a page full of means for different measures or with a page of correlations, creative people can make sense of them—even when the variables are mislabeled and the results are in the wrong direction!</p>

<p>These problems, ignoring the possibility of Type I errors and falling prey to hindsight bias, are most likely to affect interpretations when many variables have been measured and when evaluation designs have few ways to control for threats to validity. Evaluators often have access to many demographic variables and can extract many process and outcome variables from agency records. Without a clear impact model indicating how the program processes are expected to work, it is easy to succumb to the temptation to correlate everything with everything else. Then, evaluators or their clients may read interpretations onto the results. Novel findings are best treated as very tentative, especially if they appear in the context of a large number of analyses that were not suggested by the theory underlying the program. All evaluation designs can be overinterpreted, not just single-group designs; however, the tendency seems more likely because this design is descriptive and does not support the inference that participation in the program causes desired outcomes.</p>

<h2 id="usefulness-of-one-group-designs-as-initial-approaches-to-program-evaluation">USEFULNESS OF ONE-GROUP DESIGNS AS INITIAL APPROACHES TO PROGRAM EVALUATION</h2>

<p>When there are previously specified desirable levels for the outcome variables (benchmarks), and when the participants do not drop out during the program, the pretest–posttest design may be sufficient to document the program’s success. However, even when benchmarks are not available, the reader should not gain the impression that these approaches do not have any legitimate uses. These single-group designs are less intrusive and less expensive and require far less effort to complete than more ambitious methods. Thus, these designs can  serve  important  functions  in  an  evaluator’s  toolbox  as  first  steps  in  planning  a  more  rigorous  program evaluation. When standards are not available, the purposes of single-group evaluations include (1) assessing the likely usefulness of a more rigorous evaluation, (2) searching for promising variables correlated with success in the program, and (3) preparing an agency for more controlled evaluations in the future.</p>

<h3 id="assessing-the-usefulness-of-further-evaluations">Assessing the Usefulness of Further Evaluations</h3>

<p>Before embarking on a rigorous evaluation that controls for many plausible alternative interpretations, a single-group design would serve to show whether there is any improvement to explain. If participants fail to get jobs for which they were trained, it is likely that no further study is needed. The planned outcomes were not achieved; no statistical analysis is needed. If the rehabilitation program participants finished at desirable levels of function, a pretest–posttest design might show that they improved while in the program. This finding may well satisfy some of the stakeholders. Moreover, finding improvements might justify a more complex evaluation.</p>

<h3 id="correlating-improvement-with-other-variables">Correlating Improvement with Other Variables</h3>

<p>The use of the pretest–posttest design and partial correlations permits relating improvement to the amount of service and to characteristics of the participants. The mean level of improvement could be acceptable, but if outcome measures show large standard deviations, it might be that some subgroups of participants benefit, while others benefit little. That question should be explored. On the other hand, a favorable outcome at the end of a program may really indicate a general improvement for many participants. Perhaps the improvement was due to the impact of the program; further work would be merited.</p>

<h3 id="preparing-the-facility-for-further-evaluation">Preparing the Facility for Further Evaluation</h3>

<p>A third reason for conducting an evaluation using only the participants is to help the staff understand the idea   of evaluation. As described in earlier chapters, service providers—from paraprofessional counselors to teachers to physicians—are beginning to recognize that their work can and will be evaluated, even though the quality    of service is hard to quantify. If evaluators begin their work with less threatening approaches, they have better chances of leading service providers to see the usefulness of evaluation and to value the contributions of evaluators.</p>

<p>The methods described in this chapter are among the least threatening. There is no possibility of learning that another program achieves the same level of success at a lower cost. When staff and managers fear that another program is more efficient, the resulting anxiety can lead to hostility and limited cooperation with an evaluator. Just this type of reaction occurred during an internal evaluation of a hospital rehabilitation program. The program director initiated the evaluation and showed remarkable openness to evaluation, making comments such as “Maybe we don’t help people as much as we think we do—we should know that” and “If a patient is institutionalized after leaving here, we have failed.” However, when the evaluators proposed increasing the internal validity of the evaluation by gathering observations from a neighboring hospital without a similar rehabilitation program, the director seemed to change his position. Such a comparison could conceivably have shown that similarly diagnosed patients without a program did as well as those in the expensive program. Then he said that the additional data were unnecessary since the value of this type of program was well known and fully documented. The expanded study went on because no controlled studies could be found supporting his assertion, a situation that is surprisingly familiar to those who carefully search for support for some widely accepted practices in medicine (see, for example, Eddy, 1990). The director mellowed when it was learned that the neighboring hospital had very few patients with the appropriate diagnoses and thus no information could be forthcoming that would question the need for program he directed. Cooperation and cordial relations returned.</p>

<p>Rational use of resources requires that ultimately such comparative studies be done. In medicine,  psychology, and social work much attention is being paid to “evidence-based” practices; however, many observers report that the evidence cited is too often equivocal (Raines, 2008; Weiss, Murphy-Graham,  Petrosini, &amp; Gandhi, 2008). Careful evaluations should not be done in a way that causes a staff to feel threatened. Internal evaluators who try to insist on methodologically rigorous evaluations may find that they  lose their influence with the staff. Beginning with nonthreatening methods permits evaluators to gain the confidence of staff members and managers. This strategy is not meant to imply that rigor is unnecessary; however, using less rigorous evaluations at first can be useful in earning the trust of the staff.</p>

<h4 id="case-study-9">CASE STUDY 9</h4>

<p>Spiritual Retreats with Urban Homeless Adults</p>

<p>Although spiritual needs would not be the first thing to come to mind for many people when considering how to serve homeless people with substance abuse problems, for about 20 years, the Ignatian Spirituality Project (ISP) has been conducting spiritual retreats for homeless adults in a number of U.S. cities. As one might imagine, this kind of program regularly has more needs than financial resources, so hiring a program evaluation firm was not a high priority. But Dn. Joe Ferrari, PhD, a St. Vincent de Paul Professor of Psychology at DePaul University in Chicago, was the right man in the right place at the right time to connect appropriate people and resources (see <a href="#bookmark7">Evaluator Profile 9</a> for more specifics). As is intended with all the case studies, this one illustrates important points from this chapter—in this example, particularly noting elements related to the evaluation of a single group as opposed to multiple groups that included a control condition. In addition, however, a major theme through Dr. Joe’s work with ISP illustrates a pervasive point about working with most stakeholders—evaluations require trust, and trust takes time.</p>

<p>Meeting Needs</p>

<p>The broader base of needs is very clear—it is estimated that there are up to 400,000 homeless adults on any given night in the United   States, many of whom have disabilities of some form and therefore would benefit from appropriate services. Although many may be in temporary circumstances, a substantial number are chronically homeless. The reality that spiritual needs are seldom considered as an appropriate focus for homeless men and women with substance abuse problems is a clear example of common societal attitudes and a       related part of homeless people’s experience—many people would prefer not to have to be aware of their existence, let alone their needs,       and homeless folks are generally well aware of this fact. Some research supports an awareness among homeless people of their spiritual    needs, but this point is not generally well known. Although not noted explicitly in the evaluation report, one implicit point about ISP’s  provision of spiritual retreats for homeless people is that taking the spiritual needs of homeless people seriously communicates something    very important about the value of those people both to the homeless themselves and to society more generally. Further, as will be seen in        the following, addressing spiritual needs may also have effects on other needs that are not necessarily spiritual.</p>

<p>Implementation</p>

<p>As the retreats are weekend events (Saturday morning to Sunday afternoon), the report describes what was observed by the evaluation team, whereas monitoring implementation for ongoing programs typically requires much more extensive processes. For example, the first event at 9:00 am on Saturday is an ice breaker followed by a prayer. Various individual and group activities follow with an obvious theme of considering God’s relevance to one’s life and taking time for silence and reflection. Meals and overnight accommodations are provided for these same-sex events. Toward the end of Sunday’s activities, a group discussion provides an opportunity for participants to summarize intentions or to reflect on their experience.</p>

<p>Stakeholders</p>

<p>Most evaluations include some stakeholders who are reluctant, even resistant, to some aspect of the evaluation. Dn. Joe is clear that the evaluation of ISP retreats took a long time to develop. The ISP director initially contacted Dn. Joe, based on a previous association, regarding some analysis of data that ISP had already collected. One step at a time, Dn. Joe and some of his students became more involved with the project—first with formal data analysis, then summarizing comments from a reflection session, then publishing the results of those summaries, and on to more structured evaluations, a result of growing trust over time. Although not stated explicitly, it would not be surprising that ISP staff were motivated to protect their retreatants from potential harm, given common attitudes and behavior toward the homeless. But clearly the staff and the participants, the other major category of stakeholders, grew to trust the evaluation team.</p>

<p>Side Effects</p>

<p>Especially in this case, the distinction between side effects and outcomes depends on one’s perspective. Although one could describe having a spiritual retreat result in decreased loneliness and increased hope as positive side effects, the evaluation team, focusing on ISP’s obvious intention for positive results, understood more broadly than narrowly, listed them as outcomes. No negative side effects were noted.</p>

<p>Improvement Focus</p>

<p>As with several aspects of this evaluation, the focus on improvement is somewhat subtle in the report—blending summary information about the participants and their experiences with gentle suggestions for ways to increase the scope of the evaluation and, therefore, the basis for feedback to ISP and eventual improvement of the program.</p>

<p>Outcomes</p>

<p>As noted above, the outcomes of decreased loneliness and increased hope over time, which were greater among women and African- Americans than in other groups, were a positive result documented by the evaluation team. Although there were no structural research design elements to support attributing the effects to the intervention of a spiritual retreat as opposed to alternative explanations like the participants being invited to a novel experience that was likely in a more pleasant setting than some shelters, the documentation does provide an important baseline for future evaluations as well as the clear suggestion that the retreat itself led to good outcomes. One important point about all evaluation results is that, although they are often not completely convincing evidence that the program must have caused the benefits, in the absence of clear evidence to the contrary they provide a reasonable hope that the program is resulting in good effects. Often that is enough to convince stakeholders to continue to invest time, money, and effort into the program.</p>

<p><strong>EVALUATOR PROFILE 9</strong></p>

<p><strong>Dn. Joseph Ferrari, PhD—Called and Formed</strong></p>

<p>Rev. Dr. Joe Ferrari, a Permanent Deacon in the Catholic faith, notes that he was “called and formed” to his current roles, which cover a very wide range. In addition to serving the people of the Diocese of Joliet, Illinois, he was the founding director of an MA/PhD program in Community Psychology at DePaul University in Chicago, IL. He is an extremely productive, multiple award-winning scholar, a St. Vincent de Paul Professor, and the Editor of the <em>Journal of Prevention and Intervention in the Community</em>. Still, he notes that his path to the present was anything but direct.</p>

<p>As he worked his way step by step toward a doctorate, he was particularly drawn to community psychology with his interests in applied matters that made a difference for individual people and communities like increasing blood donations and seat belt use. Yet he entered a doctoral program in social psychology because the Department Chair said, “Blood donations? That sounds like social psychology. Besides, we don’t have community psychology here.” After several legs of the journey in New York, he set off for Chicago for a big adjustment to a much larger program than when he himself comprised half the psychology department. At DePaul University, writing grants, publishing, teaching, and editing still did not keep him from his passion of offering students a chance to help people really make systemic changes in life. So an observer might think it is hardly surprising that one of his program evaluations examined the Ignatian Spirituality Project’s spiritual retreats for homeless—a particularly striking example of helping others.</p>

<p>Dn. Joe, as he likes to be known, is clear that the destination is not always known at the start, noting that “We never know where life will take us—the directions we are called, the events that form us.” With reference to the Source of how things develop in our lives, he explains, “PhD goes at the end of my name, because I earned that. But Dn. goes before my name, because that is what I am called to be.”</p>

<p><em>Source</em>: J. Ferrari (personal communication, October 24, 2017).</p>

<h2 id="summary-and-preview-2">SUMMARY AND PREVIEW</h2>

<p>This chapter emphasized two major points. First, a sympathetic understanding of the needs of the users of an evaluation may indicate that a simple research design serves quite well to meet the information needs at a particular time. Second, single-group designs cannot answer many questions that are critical to stakeholders because many interpretations of findings remain plausible. These plausible interpretations are called threats to internal validity.</p>

<p>The next chapter contains several approaches to clarify the interpretation of outcome evaluations. These procedures require making additional observations of (1) the program group, (2) nonprogram groups, or (3) variables not expected to be affected by the program.</p>

<h2 id="study-questions-2">STUDY QUESTIONS</h2>

<ol>
  <li>Not many years ago, when a child had frequent bouts of sore throat and earache, family doctors recommended removing the child’s tonsils. Typically, the child’s health improved afterwards. Parents and physicians attributed the improved health to the operation. What threats to  internal  validity  were  ignored?</li>
  <li>Some 50 elementary school children witnessed a brutal murder on their way to school in a suburb of a  large city. The school officials worked with a social worker, a school psychologist, and others in an  attempt to help the children deal with their emotions, to avoid long-term ill effects, and to help parents  with their children. A year after the tragedy, there appeared to be no serious aftereffects. What  assumptions are necessary to claim that those efforts were useful?</li>
  <li>Politicians are notorious for attributing any improvements in governmental and economic affairs to their own efforts and any deterioration to the policies of others. What threats to internal validity are most likely to make such interpretations invalid?</li>
  <li>Why can a smoking cessation clinic be evaluated using a one-group design?</li>
  <li>Prepare some examples of pretest–posttest evaluations that would appear favorable because of threats to internal validity. Also, prepare some hypothetical examples of evaluations that would appear ineffective because of threats to internal validity.
    <ul>
      <li>Technically, we pool variances, _s_2, and then calculate the standard deviation from the  pooled  variance.  When  finding  pooled  standard  deviations for a pretest–posttest design, the use of the average of the two standard deviations is sufficiently precise for most evaluations.</li>
    </ul>
  </li>
</ol>

<h2 id="additional-resource-2">ADDITIONAL RESOURCE</h2>

<p>Shadish, W. R., Cook, T. D., &amp; Campbell, D. T. (2002). <em>Experimental and quasi-experimental design for generalized causal inference</em>. Boston, MA: Houghton Mifflin.</p>

<p>Although an older text, this is the standard research design resource for program evaluators. It is the successor of the classic work by Campbell and Stanley (1963) and the more recent standard by Cook and Campbell (1979). The first two authors are highly respected methodologists whose writings are relevant to program evaluation. Shadish has been the president of the American Evaluation Association and has published widely on program evaluation. Cook is, perhaps, the most eminent living social research methodologist. The late Donald Campbell has been associated with more innovations in social research methods than anyone has tried to count. Threats to internal validity are discussed in detail in Chapter 2 and other chapters.</p>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
