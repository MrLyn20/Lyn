<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Program Evaluation Chapter 4-6 | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Program Evaluation Chapter 4-6" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="4 Developing Measures of Implementation and Outcomes" />
<meta property="og:description" content="4 Developing Measures of Implementation and Outcomes" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/14/Program-Evaluation-Chapter-4-6.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/14/Program-Evaluation-Chapter-4-6.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-14T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Program Evaluation Chapter 4-6" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-14T00:00:00+09:00","datePublished":"2023-08-14T00:00:00+09:00","description":"4 Developing Measures of Implementation and Outcomes","headline":"Program Evaluation Chapter 4-6","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/14/Program-Evaluation-Chapter-4-6.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/14/Program-Evaluation-Chapter-4-6.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=8d926da58a63a12e6c8f2ca86ddd0e59290944ea">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>14 August 2023</small>
<h1>Program Evaluation Chapter 4-6</h1>

<p class="view">by JAMES R. DUDLEY</p>

<h1 id="4developing-measures-of-implementation-and-outcomes">4 Developing Measures of Implementation and Outcomes</h1>

<p>After evaluators and stakeholders have agreed on criteria that would indicate successful implementation and outcome, evaluators face the task of developing methods to measure those criteria. Psychologists commonly gather data from individuals; sociologists gather data on a community or neighborhood level; and economists focus attention on still larger groups. These differences reflect the questions treated by these disciplines. Programs are designed to affect individuals (e.g., education or rehabilitation), communities (e.g., crime prevention, building improvement), or states or regions (e.g., clean air policies, economic stimulation). Depending on the program being evaluated, program evaluators gather data in ways similar to the methods used by psychologists, sociologists, business managers, or economists. Regardless of the focus of the program, each type of data has strengths and weaknesses; consequently, evaluators should use multiple sources and methods. One chapter cannot cover all approaches. Instead the most frequently used sources and methods as well as the characteristics of good measures of evaluation criteria are reviewed. Last, some measurement tools are presented to illustrate the principles.</p>

<h2 id="sources-of-data-for-evaluation">SOURCES OF DATA FOR EVALUATION</h2>

<p>A dilemma for evaluators is that the stakeholders who know the program best may be biased toward seeing program success, whereas those who have the least self-interest often know little about the program. This dilemma leads evaluators to seek data from a variety of sources using different types of measurement techniques.</p>

<h3 id="intended-beneficiaries-of-the-program">Intended Beneficiaries of the Program</h3>

<p>Educational, training, medical, and welfare programs that focus on individuals usually involve information  from individual participants. In contrast, programs in economic development, preventive medicine,  and  criminal justice focus on community-level variables rather than on specific, identifiable individuals. With such programs, all members of a community are, in a sense, recipients of the service; consequently, data that reflect the state of the community will be most relevant.</p>

<p><em>Program Participants</em></p>

<p>Program participants can provide information that is not available from other sources. In some situations participants spend more time with program staff than managers who are responsible for assessing the competence of the staff. College students, for example, often know more about the quality of a professor’s teaching than does a department head or a dean. Furthermore, only the program participants know what their reactions are to the program. For many programs, participants are the best source of information on their own condition. Interviews with deinstitutionalized chronically ill psychiatric patients showed that for many patients, discharge damaged their sense of well-being (Shadish, Orwin, Silber, &amp; Bootzin, 1985). An advantage of participant-provided information is that it is often relatively inexpensive to gather and is at least as accurate as other assessment approaches for a variety of behavioral and emotional dimensions (Shrauger &amp; Osberg, 1981). There are, however, surprising limitations to what people can report about themselves. For example, people have difficulty recalling what they ate the previous week and most people report eating medium-sized servings even though the definitions of “medium” varied by 200% in different versions of the same survey (“On a Diet?” 1989). Schwarz and Oyserman (2001) have shown how small differences in the phrasing of questions can affect what people report about themselves or their surroundings.</p>

<p>It is valuable to recognize that most participants can provide good data on many objective aspects of a program but not on other aspects. General hospital patients usually know whether rooms are clean, whether nurses and resident physicians treat them politely, and how long they have to wait in radiology (see Brown &amp; Adams, 1992). However, they cannot evaluate the choice of medication or the competence of their surgeons. Similarly, college students can report on whether a teacher returned tests promptly, held classes as scheduled, and lectured or led class discussions. There are few undergraduates who can evaluate the accuracy of information presented in lectures unless the professor is grossly incompetent.</p>

<p>Experienced evaluators do not forget that an evaluation is not a high priority for program participants because they have sought the services of the program in order to meet a need, not to provide data. Many people will share their views if they believe that their efforts can help improve services for others. However, few are so altruistic that they will struggle with poorly organized surveys. The participants who have the most favorable impressions of a program or facility are the most likely to cooperate with data collection. The 86 percent who responded to a lengthy survey on a chaplaincy internship were independently evaluated by their former supervisors as having performed better than the 14 percent who did not return the survey (Posavac, 1975). College alumni who returned a mailed survey had earned higher GPAs than those who did not (Hogan, 1985). The choice of measures can minimize some of these problems, although this limitation should also be noted in reports.</p>

<p>Participants will need to be assured that their answers to surveys or interviews will be treated confidentially. Many  people  do  not  understand  the  social  scientist’s  disinterest  in  facts  about  individuals,  nor  do  they understand the necessity of using group averages and proportions in evaluation reports. They are familiar with the case study approach often used in newspaper and television discussions of medical, correctional,  counseling, and educational programs. At times, however, details about an individual can play a role in an evaluation report by giving vivid illustrations of very good or very bad program performance. Using  case studies of teenagers, Love (1986) showed that troubled adolescents were unlikely to fit into the mental health and welfare systems of a large city. Although a case study cannot show the extent of a problem, it can show  how specific program failures occur and can help policy makers visualize a need for program development or change. Success stories can help planners as well because the way participants succeeded may suggest how to improve the program for others (Brinkerhoff, 2003; Kibel, 1999). To honor standards of confidentiality, any information about individuals should only be used in ways that do not permit the individuals to be identified.</p>

<p><em>Artifacts</em></p>

<p>In some settings, evaluators can use things that program participants or community members produce. In schools, students write essays, term papers, theses, school newspapers, and yearbooks. These artifacts reflect the quality of the curriculum, the teaching staff, and, indirectly, the school administration. In vocational settings, it might be useful to track the quality of items made and the amount of material wasted. The success of morale-boosting efforts might be revealed by a greater use of coffee cups and caps marked with a company or university logo. Turning to unwanted artifacts, the need for a vandalism prevention program might be demonstrated by high rates of broken windows, unkempt yards, and doubly or triply bolted doors. Success of a community program might be indicated by a lower rate of such artifacts.</p>

<p><em>Community Indexes</em></p>

<p>Some programs are designed to improve community-level variables. Examples include a crime prevention program to increase citizen participation in an effort to reduce theft and a citizen-developed  program  to monitor housing code violations to increase or maintain the quality of housing in a neighborhood. Since such programs are delivered to a community, the outcomes must be measured differently from the ways outcomes intended to affect individuals are measured. Community records, such as high school graduation rates, crime reports, fire losses, or immunization rates, may be summarized.</p>

<p>A major difficulty with community-level indexes is that there are many steps between the program and the desired outcomes (Cook, Leviton, &amp; Shadish, 1985). Many community events, government policies, and personal habits may have such strong effects on the behavior to be changed that even  a  well-conceived program may not create a detectable effect. Such influences are, of course, beyond the control of the program staff. An evaluator who measures community-level indexes for an outcome evaluation of, say, a media-based preventive health effort while ignoring the integrity of program implementation and intermediate outcomes   may be left quite in the dark about the reasons for apparent success or failure.</p>

<h3 id="providers-of-services">Providers of Services</h3>

<p>The staffs of programs have information that is crucial to program evaluations.  Programs  also  maintain records that, when summarized, can provide important clues concerning a program’s operation and the  outcomes achieved.</p>

<p><em>Program Staff</em></p>

<p>Staff members have been trained and are expected to assess participants’ needs and improvement. Also, staff members know how well a program is managed and run on a day-to-day basis. On the negative side, staff members expect to see improvement; after committing themselves to helping people improve their skills,  health, or adjustment, it becomes easy to overlook program problems.</p>

<p>Beyond a concern over the expectations of staff, evaluators need to recognize that evaluating a program is in  some  ways  an  evaluation  of  the  staff’s  performance.  Few  staff  members  unhesitatingly  welcome  an evaluation of the effectiveness of services they provide; in fact, most will have questions about how an evaluation is to be used. If they are worried, they may resist the evaluation. Some college professors try to bar students conducting course evaluations from entering classrooms, and some psychotherapists argue that checklists of symptoms and rating forms are not sensitive enough to detect subtle, yet crucial, changes in   clients that only therapists can sense. Although it may be hard to counter all concerns, program managers can assure staff that program evaluations are designed to evaluate programs, not individuals, and evaluators can assure staff of confidentiality. Furthermore, evaluators can include staff members in a thorough discussion of  the findings of the evaluation as part of the report preparation process; providing scheduled progress reports as well as a final report can assure staff members that they will receive feedback throughout  the evaluation  process (see Chapter 13).</p>

<p><em>Program Records</em></p>

<p>Evaluators often begin developing specific methods to assess the criteria by asking about a program’s records (Hatry, 1994). Records should include client characteristics, types and extent of services provided, costs of services, and workloads of staff members. When the variables are objective, these records can be very reliable. When the variables are less objective, as is the case with diagnoses or comments on progress, evaluators must exercise caution in using the information.</p>

<p>There are several important advantages of using the program’s archives: the measurement process cannot affect the program participant, there can be no participant loss due to refusal or inability to cooperate, and data collection is less costly than would be the case when gathering data in the field. However, the quality of records may be poor. For example, a jail-based drug rehabilitation treatment program had such poor records that attempts to match treatment information with other information about clients had to be abandoned (Lurigio &amp; Swartz, 1994). Hospital discharge records to be used in an evaluation contained many coding errors (Green &amp; Wintfeld, 1993). Furthermore, records can be deceptively altered in anticipation of an evaluation, as done by the Inspector General of the Pentagon (Margasak, 2001).</p>

<p>When working as an agent of the service provider, an evaluator has legal authority to use confidential records; however, the importance of maintaining confidentiality cannot be overstated. Program records often contain material which, if released, could harm a program participant or staff member. Even the accidental release of critical information could leave an evaluator open to legal challenge; if insufficient care is taken when handling program or agency records during an evaluation, the evaluator would gain  a  very  unprofessional image, would lose credibility, and could be denied further access to records.</p>

<h3 id="observers">Observers</h3>

<p>Observers who are not part of a program often provide perspectives that neither participants nor staff can. Observers could be (a) experts who are familiar with a wide range of similar programs or the needs the   program is designed to meet, (b) people especially trained to rate the variables that describe the process or outcomes of a program, (c) people with relationships with participants such as family members or work associates, or (d) the evaluators themselves.</p>

<p><em>Expert Observers</em></p>

<p>Very complicated programs with abstract goals are often evaluated by having a team of experts examine the program’s structure and products (Averch, 1994), which matches the Expert Opinion model, described in the Chapter 2 eResources. Accreditations of hospitals, rehabilitation facilities, and colleges are handled by experts who examine much information and talk with staff and participants before rendering a judgment. A basic research program must be evaluated before it can be provided with financial support. It is hard to think of a   way to evaluate a proposed research program that does not use expert opinion. Experts not involved in the program being evaluated may be the least biased sources of information. Expert opinions may be essential to clarify the meaning of quantitative information describing the program.</p>

<p><em>Trained Observers</em></p>

<p>Many programs can be evaluated using people who have been trained to rate the degree to which a program   was implemented as planned and well-defined criteria of program success. A municipal program to repair and clean parks might be evaluated by training a team of employees to examine parks using checklists with objectively defined levels of maintenance (Greiner, 1994). An evaluation of treatment services for severely mentally disabled clients may utilize ratings of psychological function by trained observers using a social adjustment scale (Green &amp; Jerrell, 1994).</p>

<p><em>Significant Others</em></p>

<p>If a program is designed to change behavior (as correction and counseling programs are) or to improve health   or work skills, people who have contact with the participants outside of program settings often can provide important information on improvements or problems (e.g., Katz &amp; Warren, 1998). With the exception of hospital patients or prisoners, staff members see participants for far fewer hours per week and in fewer settings compared to family members who see participants in natural situations. However, significant others also have biases; they want to see improvement in troubled family members. Evaluators seeking information from significant others would want to learn about specific changes in participants;  general  subjective  opinions would be less useful.</p>

<p>When requesting information about participants from significant others, it is necessary to have the participants’  permission  beforehand.  People  differ  in  their  willingness  to  have  spouses  or  others  provide personal information to an evaluation staff. Requests for cooperation can emphasize the potential to improve services for others. Care should be taken to avoid implying that good treatment for the participant is contingent on giving permission to contact a significant other or on providing information for an evaluation.</p>

<p><em>Evaluation Staff</em></p>

<p>In some situations information is gathered by people playing the role  of  clients  (Turner  &amp;  Zimmerman, 1994). The number of busy signals, time to answer telephone calls, and time on hold can indicate agency responsiveness to citizen needs (Jason &amp; Liotta, 1982). The City of Chicago Consumer Services Department placed calls for taxi service to learn whether companies were honoring a requirement to serve disabled riders; the department learned that 47% of such requests were ignored (Washburn, 2001). Some forms of qualitative evaluations are based on the thorough observations by an evaluator familiar with social science methods and intervention programs, but not with the specific program being evaluated. When a disinterested observer examines the program, traditional practices and assumptions made by the staff may be questioned (Guba &amp; Lincoln, 1989). Fresh insights into side effects of the program may be detected. Both the goal-free and the naturalistic models of evaluations depend on information gathered by the evaluators themselves. Evaluators must be free to observe the program and must remain on guard for attempts by agency  staff  to  guide  evaluators only to what managers want the evaluators to see. <a href="#_bookmark0">Figure 4.1</a> summarizes the primary strengths and weaknesses of the various sources of information just introduced.</p>

<p><em>Figure 4.1</em> Different sources of information used in program evaluations have different strengths and weaknesses. Using multiple sources is the best approach to obtain comprehensive information.</p>

<h3 id="which-sources-should-be-used">Which Sources Should Be Used?</h3>

<p>The choice of data sources depends on the type of decision to be made based on the evaluation, the size of the program, the time available to conduct the evaluation, and the cost of obtaining data. If an agency needs information to satisfy a state auditor’s office, it is likely that a summary of records, a sampling of participants’ attitudes, and a fiscal report will suffice. If a decision to expand (or eliminate) an expensive, controversial service is anticipated, a more thorough analysis of the needs and progress of the participants coupled with the judgments of outside experts may be essential. Such decisions must be made on the basis of an evaluation of considerable rigor and credibility.</p>

<p>An important kind of data source is the general area of social media—the online networks  such  as  Facebook and Twitter that have seen explosive growth in recent years. Several aspects of social media mean  that they are potentially transformative ways to gather information of many forms relevant to program evaluations. They not only can connect evaluators with substantially larger numbers of participants, but can target specific populations relevant to an evaluation, and costs are generally much lower than alternatives.    Even compared to just a few years ago, their use is becoming much more common (for example, Goldman &amp; Beaumont, 2017; Miguel et al., 2017; Price et al., 2017).</p>

<p>The advantages of greater numbers of people who fit specific populations that might have been difficult or even impossible to reach using more traditional methods is hard to overstate. As is well known, one important aspect of social media is the way they can facilitate connections among those with very specific commonalities even when there is great physical distance among members of these communities or when typical interactions would not help members identify one another. For example, most social media provide access to people throughout most of the world, and even groups that might avoid any public identification due to stigma, such    as those with particular diseases, conditions, or other aspects of identity that experience prejudice and discrimination, can still connect with one another, especially with the various levels of anonymity that online connections afford. Trying to recruit people to respond to surveys or other methods of data collection from  many of these groups previously involved enormous challenges, whereas the possibility of reaching substantial numbers of those with almost any particular focus is now greatly increased.</p>

<p>These advantages are not unmixed blessings, however. For one thing, although the potential to connect    with specific samples seems great, it is not clear how representative these groups are of the desired populations nor how online behaviors such as following links match established sampling methods that form the basis for established forms of analysis (Gabielkov, Ramachandran, Chaintreau, &amp; Legout, 2016). Another important   area of uncertainty is how established systems of ethics such as basic points of informed consent translate to social media. On the one hand, it is fairly clear to almost everyone that interacting online is much more     similar to actions in public than in private. To put it differently, people generally understand that they have no reasonable expectation of complete privacy when engaged online. But on the other hand, a number of  traditional areas of concern regarding protections in research raise important questions, such as obtaining consent from parents or guardians for those under the age of consent or what responsibilities researchers have when learning about illicit activities (Henderson, Johnson, &amp; Auld, 2013). Perhaps what is most clear at this time is that social media will continue to grow as an important avenue of data collection  for  program evaluation just as it is in other areas, but there are likely to be various new developments that are impossible to predict with certainty now.</p>

<p>Regardless of the reason for an evaluation, evaluators should strive to use multiple measures from more   than one source (Shadish, 1993). In choosing multiple measures, it is especially important to select measures that are not likely to share the same biases. For example, a client’s subjective assessment of the success of a program may be mirrored by the subjective feelings of the client’s spouse. If subjective measures of a program’s success are supplemented by objective measures, evaluators are less likely to obtain sets of data affected by the same biases. When a conclusion is supported from a variety of sources, it becomes more credible to stakeholders.</p>

<p>It is, however, possible that the different sources will not agree. Shipley (1976) conducted an evaluation of   a program in which college students served as volunteer companions to discharged psychiatric patients. The volunteers, patients, and hospital staff all viewed the program in quite glowing terms. However, more objective   measures   of   the   patients’   behavior   (staff   ratings   and   records   of   frequency   and   duration   of rehospitalizations) revealed highly variable outcomes. Some patients apparently benefitted; others did not.     The use of multiple measures led Shipley to a conclusion different from that drawn in evaluations of similar companion programs using only subjective ratings. In a similar vein, Sullivan and Snowden (1981) found that staff, clients, standard tests, and agency files did not agree about the nature of the clients’ problems. Massey and Wu (1994) showed that patients, family members, and case managers in a mental health facility assessed the  patients’  level  of  functioning  differently.  When  evaluating  publicly  funded  job-training  programs, subjective satisfaction might not be highly correlated with objective outcomes, such as income  level  (Hougland, 1987). Evaluators must therefore be sensitive to the possibility that focusing on a single source of information can distort their conclusions.</p>

<p>Since evaluators frequently work with people trained in service delivery techniques, not in research  methods, evaluators often are surprisingly free to choose the criteria of effectiveness and the analyses of those criteria. Some observers have gone so far as to say that evaluators can determine the findings of an evaluation before it is conducted through their choice of criteria and analytic methods (Berk, 1977; Berk &amp; Rossi, 1976; Zigler &amp; Trickett, 1978). Fairness to all stakeholders requires evaluators to examine their own attitudes as     they design evaluations. The following section describes the most important issues to consider when deciding how to gather information for evaluations.</p>

<h2 id="gathering-information-wisely">GATHERING INFORMATION WISELY</h2>

<p>When making choices among possible data sources and approaches to measurement, evaluators strive to use techniques that will contribute to valid, sensitive, and credible evaluations. Following the practices discussed below will help achieve this goal.</p>

<h3 id="use-nonreactive-measures">Use Nonreactive Measures</h3>

<p>Nonreactive measurement refers to procedures of gathering information that do not prompt the respondent to give particular answers. For example, whenever it seems obvious that certain answers are desired by interviewers, some respondents tend to give those answers. The previous section hinted at another problem of reactive measures: merely focusing attention on a variable can lead people to change their behavior. Even surveys can be reactive because they may prompt respondents to think about issues they may have never concerned themselves with before. Clearly when an evaluation leads to changes in the behavior of staff or clients, the evaluation does not accurately reflect the program in operation. Unless the evaluation depends   solely on existing records, totally nonreactive measurement is an ideal seldom achieved, although care in designing evaluations can reduce reactivity (Webb, Campbell, Schwartz, Sechrest, &amp; Grove, 1981).</p>

<h3 id="use-only-variables-relevant-to-the-evaluation">Use Only Variables Relevant to the Evaluation</h3>

<p>Because program evaluation is an applied discipline, the variables selected must be relevant to the specific informational needs of facility management, community representatives, those responsible for budgets, and other stakeholders. In planning an evaluation, evaluators seek to learn what decisions are pressing but not understood. Discussions with staff and managers often reveal the important variables. If no decisions can be affected by a variable, or if no program standards mention the variable, then it probably is not essential to the evaluation. Evaluators take care not to measure variables that some stakeholders might think would be “interesting” but that would not affect the operation of the program or agency; such variables raise the cost of  an evaluation but do not contribute to its value. Sometimes evaluators expect that a variable would be seen as important once summarized for agency leaders; at those times experienced evaluators act in the role  of educator. Furthermore, evaluators seek to be sure that they are sensitive to good and bad side effects that   cannot be anticipated.</p>

<h3 id="use-reliable-measures">Use Reliable Measures</h3>

<p>Reliability refers to the consistency of the information that a measure produces for the same person in the    same situation (Murphy &amp; Davidshofer, 2005). There are many ways to think about reliability. If different observers describing the same thing report similar levels of a variable, we say that the observation procedure is reliable. If different levels are reported, we say that the procedure is unreliable. Reliability is calculated using the correlation coefficient, <em>r</em>. If two observers agreed completely after making a number of observations, a correlation of the two sets of data would be 1.00, perfect reliability. To the extent that they do not agree with each other, the correlation would be smaller; 0.0 would indicate that knowing one observer’s reports would tell us nothing about the other observer’s reports. Generally, reliability values of 0.7 or  above  indicate good enough consistency.</p>

<p>If an outcome measure is a 50-item test of knowledge gained from, say, a job training program, there is another way to estimate reliability. The scores that a group of trainees earned on half of the items (say, the odd-numbered items) can be correlated with the scores for the other half of the items; such a correlation is  called a “split-half” reliability. Note that the focus of interest is still consistency just as in the first example concerning interobserver reliability. For split-half reliability the issue is interitem consistency, that is, the different items seem to be measuring the same thing. This form of reliability in general is called internal consistency and is typically reported as Cronbach’s alpha, which is the average of all possible ways to divide items for split-half reliability. Statistical analysis programs make such calculations simple.</p>

<p>Reliability is higher when the measurement procedure is minimally affected by the passing moods of the observer or the individuals observed. Like validity, reliability is likely to be higher when observations are based on objective behaviors rather than on inferences. Reliability is also higher when the measurement instrument contains more items. You may well have a sense of the higher reliability produced by using many questions if you think of a classroom test. A single item forms a larger portion of a short test than it would of a long test.     If you happened to forget the topic covered by a certain question, your score would be affected much more in the context of a short test than in the context of a long test. Because a single question can have a major effect, short tests are less reliable than long tests. The more reliable a measure is, the more likely it is to detect an   effect of a program (Lipsey, 1990).</p>

<p>Evaluators trained in disciplines in which the individual assessment is important are familiar with the reliability of classroom, achievement, or job selection tests. It is crucial that evaluators recognize that using   tests to evaluate individuals is markedly different from using tests to evaluate programs. Since evaluators focus attention on groups, not individuals, surveys and observational techniques that would not be reliable enough    for individual assessment may be acceptable to estimate the mean score of a group. By using the standard deviation of a measure and its reliability, it is possible to calculate a range of values likely to include a person’s true score. For example, assume a person scored 105 on an IQ test with a standard deviation of  15  and reliability of 0.95. The true scores of 68 percent of the people getting 105 will be between, or 101.6 and 108.4. The value added and subtracted to the person’s score is called  the standard error of measurement and applies when measuring an individual’s score (Murphy &amp; Davidshofer, 2005). This range is called a confidence interval.</p>

<p>By contrast, when estimating a group’s mean score, the standard error of the mean should be used to calculate confidence intervals instead of the standard error of measurement. Suppose a group of 100 students are selected at random from a school. Suppose further that they have an average of 105 on the same IQ test mentioned above. For a group, we work with the standard error of the mean, which, in this case, is, or 1.50. We can conclude that 68 percent of such groups have true mean scores between , or 104.7 and 105.3. Compare this confidence interval with the one found for an individual in the paragraph above. We can be much more certain of the likely value of a group mean than the score of any one of the individuals in the group. Even a measure with a reliability of only 0.60 would yield an estimate that would be useful for program evaluation purposes because evaluators are concerned about groups, not individuals. Even short scales are often sufficiently reliable for evaluating the effects of college experiences when samples are large (Walvoord, 2004).</p>

<p>When observer ratings are used, evaluators also seek high interobserver reliability. An instrument that    yields similar values when used by different people has more credibility than one that yields different values. Obviously, if the ratings are done in different settings—at home versus at school—a low interobserver  reliability index may be observed due to real differences in the person’s behavior between the settings themselves. Still, interobserver reliability can be increased through the development of a detailed instruction manual and careful training and periodic monitoring of observers.</p>

<h3 id="use-valid-measures">Use Valid Measures</h3>

<p>The instruments must validly measure the behaviors that the program is designed to change. These behaviors may be the ultimate criterion behaviors (such as a healthy life, developed skills, employment) or more immediate behaviors believed to be useful in achieving the planned long-term outcomes (such as more consistent adherence to prescribed medical treatment, higher quality work, or improved leadership practices).   In order to increase the credibility of the evaluation, evaluators make sure that staff members have approved   the measures. But because program staff members are often inexperienced in social science methodology and cannot judge how feasible it is to measure a particular variable, evaluators do not depend on them to suggest    all the measures and variables.</p>

<p>In general, the more a measurement tool focuses on objective behavior rather than on ill-defined or vague terms, the more likely it is to be valid. Objective behaviors (such as “late to work less often than once a week” or “speaks in groups”) can be more validly measured than traits (such as “punctual” and “assertive”), regardless of the specific measurement approach chosen (Mager, 1997).</p>

<p>Even when the variables seem quite objective, information taken from official records may not be as valid   as it first appears. Changing definitions and differing levels of care in preparing records can introduce changes in the meaning of variables that could influence an evaluation. At least twice Chicago’s crime records appeared to reveal an increase in crime when better recordkeeping was begun by the police. After a well-respected   person was installed as chief of police, thefts under $50 increased dramatically. Instead of ushering in a crime wave, the new chief simply insisted on better reporting procedures, and may have reclassified some crimes. Since homicides are usually recorded accurately and cannot be reclassified into a different category, homicide rates were used to determine that the crime rate did not increase when the new chief took over. In 1983 a   similar increase was observed when recording standards were again strengthened (“Good News,” 1983). After the renorming of the SAT, average SAT scores of college freshmen showed an increase between 1994 and  1995, a result solely due to a change in scaling by the Educational Testing Service (“Notebook,” 1994). The same pattern can be seen in many other cases.</p>

<p>One other aspect of validity particularly relevant to evaluations is important to note. Although validity has often been considered on a general scale, the concept of accurately measuring the intended target can be   applied in multiple ways. There has been growing recognition in recent years that just because a measure is  valid for one population does not mean it must be valid for all populations. An obvious example is that a measure written in one language is not valid for use with people who read a different language. In fact, even translating the measure into another language is not necessarily sufficient because words in two languages that seem to refer to the same thing may mean different things to those who speak those languages.</p>

<p>Even within the same language, there are important cultural differences that must be considered when attempting to measure elements validly. Currently, good evaluators take a culturally responsive approach (Bowen &amp; Tillman, 2015; Chon, Gero, &amp; Treichel, 2015; Faulkner, Ayers, &amp; Huq, 2015), which involves substantial efforts to ensure valid measurement with particular attention to cultural issues. Although a brief summary here cannot do justice to the complex process of culturally responsive evaluations, the main point is that evaluators must adapt their methods to fit the culture(s) of those involved in programs. Typically, this involves substantial collaboration with those who know the specific cultures and typically have life experience similar to those involved in the program. Among other things, the American Evaluation Association, in its public statement on cultural competence in evaluation (American Evaluation Association, 2011), has called     for careful attention to these issues.</p>

<h3 id="use-measures-that-can-detect-change">Use Measures That Can Detect Change</h3>

<p>Sensitivity to change is a very important point in evaluations. Before a course, few students are expected to do well on a test covering the course material. After the course, however, students who mastered the material will do well. Classroom tests are good if they are sensitive to changes that occur as the skills of the students  improve. Fairly stable characteristics of the students (such as intelligence) should not be the primary factors leading  to  differences  among  the  students’  classroom  test  scores.  By  contrast,  the  developers  of  intelligence tests seek to minimize the effects of specific learning experiences on intelligence scores and maximize the  effect of general skills developed over a long time period. Evaluation of such stable traits is nearly opposite to the approach evaluators take when seeking to detect the impact of a specific program (Meier, 2004).</p>

<p>Furthermore, care must be taken to be sure that measures of outcome are affected by the program, not by a great number of other influences. Community-level performance indicators were used in an evaluation of    crime prevention efforts in Victoria, Australia (van den Eynde, Veno, &amp; Hart, 2003). Unfortunately, the programs appeared totally ineffective because the outcome measures were not closely linked to program activities. For example, some programs (such as drama, wilderness experiences, training skills needed for circus performances) were designed to change youth culture. Even if effective, such an outcome would hardly affect emergency hospital visits a month or two after the program started; positive results might be seen over the course of a lifetime—obviously too long a time frame for any evaluation.</p>

<p>Several methods can be used to check on the expected sensitivity of a measure. Previous effective use of a measure with evaluations of similar programs would increase confidence that it is sufficiently sensitive. If the measure has been developed specifically for an evaluation, perhaps existing groups that should theoretically differ on the variable can be found. For example, a newly developed measure of scientific reasoning that   reveals differences between physics and humanities majors is a better measure of an outcome of an innovative science curriculum than a measure that cannot discriminate between such groups  (Lipsey,  1993).  In  estimating the sensitivity of a measure, evaluators must consider the typical score of the participants before a program begins. It is hard to detect improvements among program participants if most of them are already in     a desirable condition when the program begins. This problem is called a ceiling effect. For example, in a given week very large proportions of hospital patients are discharged alive and most automobile drivers are accident- free; consequently, even when medical care improves or drivers become more careful, it is hard to detect changes in such variables unless observations are made on many people. If appropriate, one way to increase statistical sensitivity to small changes is to observe the same people both before and after participation in a program. Repeated measures designs increase statistical sensitivity and should be considered when appropriate (Lipsey, 1990; Shaughnessy, Zechmeister, &amp; Zechmeister, 2008). Further, the best solution is to find other elements that show differences among people and predict the less frequent outcomes. For example, level of functioning and skill in driving would be more useful than only considering death and accidents.</p>

<h3 id="use-cost-effective-measures">Use Cost-Effective Measures</h3>

<p>In planning program evaluations, the cost of developing and producing data collection instruments must be considered. Several principles are important to remember. First, efforts devoted to making test material attractive and easy to use reap higher response rates; computers make this job rather easy. Second, funds devoted to obtaining copyright materials are small compared to the costs of gathering, analyzing, and interpreting data. Third, because interviews are a much more expensive form of data collection compared to surveys, extensive interviewing should not be routinely planned. Some evaluation questions, however, cannot be answered using records or written surveys. At times, program participants are unable to use written forms (e.g., children, elderly, or very ill patients), need prompting to obtain complete information (poorly educated participants), do not respond to written surveys (physicians), or might be defensive when answering written surveys (addicts). Last, the relationship between the instruments and participant loss needs to be considered. An instrument that is in itself inexpensive may not be cost-effective if evaluators must spend a considerable amount of time following up tardy potential respondents or if so few respond that the results do not represent any group.</p>

<h2 id="types-of-measures-of-evaluation-criteria">TYPES OF MEASURES OF EVALUATION CRITERIA</h2>

<p>It is impossible to catalog the wide variety of measures of unmet needs or the ways to measure implementation and outcomes of programs. In this section several of the strengths and weaknesses of the most widely used techniques are described. Some ways to speed your access to the literature are illustrated.</p>

<h3 id="written-surveys-and-interviews-with-program-participants">Written Surveys and Interviews with Program Participants</h3>

<p>One general point will come as no surprise to most readers—although the following sections address issues about a variety of ways to gather information from participants such as open-ended questions in contrast to forced-choice or Likert-type questions, one important choice when planning to collect information is whether   to use hard copies or online methods. Although there are situations that still require surveys printed on paper, increasingly, online options have substantial advantages that should not be dismissed quickly. Two obvious benefits of online surveys are the low cost of creating the surveys and making them available to participants   and the reduced time and labor as well as the increased accuracy of data entry. As just reading about online options is not a full experience, see the eResources for some examples of online surveys and a list with links to some of the platforms that can be used for online data collection.</p>

<p><em>Written Surveys</em></p>

<p>Probably the single most widely used method of gathering data for evaluation is the written  survey  administered to program participants. The written survey provides the most  information  for  the  cost  and effort required. Depending on the nature of the questions addressed, surveys differ in their reliability and validity. Asking clients of psychotherapy about the overall value of the service they received is not likely to yield particularly reliable or valid responses. However, the reliability of a survey that focuses on current specific behaviors is likely to be fairly high. Evaluators and stakeholders often draft surveys that are longer than they need to be and more complicated than participants are likely to complete. Gathering information that cannot     be used is a poor use of evaluation resources and an imposition on the potential respondents.</p>

<p>Using  surveys  developed  by  others  saves  evaluators’  time  in  writing  and  pretesting  items.  Furthermore, previous users of such surveys may have published the means and standard deviations that will provide comparisons to the program being evaluated, thus making the interpretation of findings more valid. When the program is indeed unique, evaluators are required to develop new surveys to measure attitudes about the program. Evaluators must be realistic about how much time they can afford to put into the construction of new instruments; like playing a guitar, writing survey items is easy to do badly.</p>

<p>Evaluators plan on reminding potential respondents to complete surveys. Reminder letters or telephone calls can be made soon after mailing a survey. Repeated reminders can be sent because with each reminder more surveys are returned. If too much time elapses before sending a reminder, many people who received the survey will have forgotten about it (Mangione, 1998). The proportion of people who will complete a survey varies with the effort required to complete it, the nature of the program, and the degree to which individuals identify with the program. Receiving less than 50% of the surveys back makes drawing most conclusions highly tentative in many cases. Even when participants respond at respectable rates, those who do not respond are probably different from those who do. In one study the 66% who returned a survey about their experience in counseling had participated in a mean of 20.2 sessions, but those who did not respond participated in a mean of only 9.5 sessions (Posavac &amp; Hartung, 1977). If all had responded, the average of the sample would have been 16.6, not 20.2.</p>

<p><em>Interviews</em></p>

<p>Interviews are used when it seems that the members of the target population are unlikely to respond to a written survey, when respondents may not answer difficult or sensitive questions unless an interviewer is at hand to encourage them, or when evaluators are not at all sure what is most important to potential respondents (Denzin &amp; Lincoln, 2005). The most compelling advantage that interviewing has over a written survey is the opportunity for the interviewer to follow up on the points made by the respondent. Interviewing is a difficult task requiring concentration and considerable familiarity with the interview questions and the program under study; it is not just chatting with program participants. Group interviews permit program participants or community residents to respond to the ideas of others; it is felt that such interaction yields richer and more well-thought-out ideas. Initial stages of needs assessments can often be profitably based on group interviews called focus groups (Krueger &amp; Casey, 2000). Sometimes focus groups are used to gain information that can be used to design written surveys (Mitra, 1994). Interviews used in qualitative evaluations are less formal than is described here (see Chapter 8).</p>

<p>An alternative to face-to-face interviews is interviewing over the telephone (Lavrakas, 1998). Because telephones are a necessity of life in developed countries, an interviewer can contact just about anyone. Furthermore, telephone interviews do not involve travel, so their cost is lower than in-person interviews. However, the interviewer may have to make repeated calls and must establish rapport quickly before the person who answers the phone classifies the caller as a salesperson and hangs up. A letter received before the telephone  call  explaining  the  need  for  the  study  and  requesting  cooperation  may  increase  an  interviewee’s cooperation. Since both face-to-face and telephone interviews are more expensive than written surveys, choosing among them should not be done without careful consideration of cost and the nature  of  the  evaluation being planned.</p>

<h3 id="checklists-tests-and-records">Checklists, Tests, and Records</h3>

<p><em>Checklists</em></p>

<p>A list of various aspects of a planned program could be used to create a checklist to verify that implementation has been thorough. A list of desired behaviors or things learned by participants can also be used as a checklist to measure program outcomes. A complex observational checklist approach to evaluating the effectiveness of residential mental health treatment facilities was developed by Paul (1986). By carefully defining key behaviors, lists of specific behaviors (such as pacing) that are objective and visible were developed. Such checklists can be reliable and can be used very efficiently once observers have been trained and know the definitions of the target behaviors thoroughly. Greiner (1994) described checklists used in the evaluation of municipal programs such as street and building maintenance.</p>

<p>Individuals can be asked to use checklists to describe less objective behaviors in themselves or others. For example, family members might be asked to indicate the degree to which discharged rehabilitation patients perform various behaviors thus indicating how well people are able to take care of themselves. Mental health staff  members  might  be  asked  to  describe  a  patient’s  social  functioning  by  checking  one  of  several  possible levels ranging from totally dependent on others to provide a supportive environment, through partially dependent (i.e., able to function with frequent therapeutic interventions), to totally independent (i.e., functioning well with no need to continue contact with a mental health center).</p>

<p><em>Tests</em></p>

<p>In many educational settings, measurements of cognitive achievement are frequently used to measure both  needs and outcomes. Published achievement tests with high reliability are well developed and widely accepted in educational settings (Murphy &amp; Davidshofer, 2005). As mentioned already, educational program evaluators should avoid choosing measures of aptitude or intelligence when looking for a measure of achievement. Evaluators in mental health settings can often find standard tests that reflect a patient’s stress, anxiety, and depression, emotional states that should be relieved by most therapies. The rapid  development  of  computerized databases has made the search for potential measures possible for an evaluator with access to a university library. A particularly useful resource for evaluators seeking measures related to medicine and  mental health is the Health and Psychosocial Instruments (HAPI) database. <a href="#_bookmark1">Figure 4.2</a> contains illustrative citations obtained from HAPI. Each of the citations listed was just one of many that appeared to be relevant      to the type of program listed. There are databases available relevant to many topics.</p>

<p><em>Agency Records</em></p>

<p>Detailed records are kept for employees of all businesses; medical care providers are required to  keep  incredibly detailed information on patients; in order to run manufacturing firms, orders, inventories, and completed products must be tracked accurately; and criminal justice agencies similarly keep detailed records. Many such records are in agency databases already. The outcomes of programs that are planned to improve health, decrease crime, raise productivity or quality, or increase skills among schoolchildren can often be   traced in records kept by the organization sponsoring the evaluation. Because most information is kept for the purposes of working with individuals, summarizing information on a program level can provide insights not previously available.</p>

<p><em>Figure 4.2</em> Illustration of information on scales obtained from the Health and Psychological Instruments database. This database of scales greatly reduces the effort required to search for measurement tools.</p>

<h2 id="preparing-special-surveys">PREPARING SPECIAL SURVEYS</h2>

<p>Surveys are used so widely in educational, psychological, and policy settings that a brief overview of the principles of survey design would be helpful.</p>

<h3 id="format-of-a-survey">Format of a Survey</h3>

<p>The format of a survey is an important factor in gaining the cooperation of respondents, analyzing the  responses, and interpreting the findings. If a survey is to be self-administered, the layout must be attractive, uncluttered, and easy to use. For many evaluations, a structured answer approach format is preferable to using free response (or open-ended) questions because analysis of narrative answers is very time consuming. When the structured approach is adopted, it is still possible to produce a visually cluttered survey if the questions    with different answer options are mixed together. For example, a mix of “Yes/No” questions, attitude  statements with which respondents are to agree or disagree, and questions about frequency of past behaviors creates difficulties for respondents because they must repeatedly readjust to the response format as they move from item to item. The more difficulty a respondent has answering, the less likely he/she will complete a   survey. Learning how to phrase a variety of questions in ways that permit them to be answered using the same answer format becomes easier with experience. The effort put into making the survey easy to use also helps evaluators when they seek to interpret and present the findings. The mean responses to items with the same answer format can be placed into a single table, thus making reports showing comparisons among items easier  to interpret.</p>

<p>At times the topic of interest in an evaluation is complex and requires narrative answers. When the range     of reactions to a program is not known or when having the respondents’ own words might be of value, surveys with open-ended questions are often useful. Patton (1980) used both a structured answer format and open-  ended questions when evaluating a performance appraisal system in an educational setting. Readers of Patton’s report had access to both the statistical summary of the survey and the personal, often impassioned, free responses. McKillip, Moirs, and Cervenka (1992) showed that the most useful information for program improvement was obtained from items that elicited specific comments, such as “Name one thing you liked     and one thing you did not like about [the program].” These open-ended, but directed, questions were more helpful than simply asking for comments. Evaluators planning an evaluation that makes extensive use of open- ended questions should consult materials on developing coding schemes for narrative material (e.g., Corbin &amp; Strauss, 2008; Weitzman &amp; Miles, 1995).</p>

<p>When preparing survey items, the most important principle is to remember who is expected to respond to the items. A statement that is clear to an evaluator might not be clear to someone reading it from a different perspective. Questions that have the best chance of being understood are written clearly, simply, and concisely. Such statements cannot be prepared in one sitting; they must be written, scrutinized, criticized, rewritten, pretested, and rewritten. Items written one week often do not seem acceptable the following week. Keep in mind that unless one is testing a skill, all respondents should be able to answer the questions. If the first draft cannot be improved, then the evaluation team has not learned to evaluate its own work.</p>

<p>There are several characteristics of clear survey items:</p>

<ol>
  <li>Good survey items are grammatically correct.</li>
  <li>Items should avoid negatives because negatively worded sentences take more effort to read and are misunderstood more frequently than positively worded items. Double negatives are especially likely to  be misunderstood.</li>
  <li>Well-written items use short, common words.</li>
  <li>Good survey items focus on one issue; an item such as “The training was informative and presented  well” combines two issues. In this case it is possible to interpret a positive answer but not possible to interpret a negative answer. What should the director of training do if many people respond negatively—have a workshop on teaching techniques or one on the content of training?</li>
</ol>

<p>Several practices can help detect survey items that need improving. First, the statements should be read aloud. Awkward phrasing is often easier to detect when read aloud than when reviewed silently. Second, imagine someone reading the item who dislikes you and may be looking for a reason to criticize your work. (No kidding, this will really help you to detect problems with any of your written material.) Third, have a colleague read the draft items. Fourth, think through how to interpret each possible answer to each item. As mentioned above, sometimes one answer can be interpreted but another cannot. Fifth, remaining ambiguities probably will be detected if the revised draft is administered as an interview with several people from the population to be sampled. Ask them to paraphrase each question; if they can, the question is clear.</p>

<p>Once survey items are prepared, they must be arranged into an order that is easy to use and appears logical to the respondent. The first questions should refer to interesting, but nonthreatening issues. The goal is to encourage the potential respondent to begin the survey. Questions dealing with controversial issues should be included later. If there is a concern that the order of items might affect the answers, consider making two different forms of the survey so that the possible effect of order can be examined. The demographic items (such as age, gender, or occupation) are best placed last since these are least likely to interest respondents. During an interview, however, demographic items should be addressed first to build rapport (Babbie, 2009). In all cases, the use to which the information is to be put should be explained.</p>

<h3 id="instructions-and-pretests">Instructions and Pretests</h3>

<p>The instructions accompanying a survey need to be clear since no interviewer is available to clear up difficulties. It is best to underestimate the skills of respondents and give more instructions and examples than might be necessary. Once the survey and instructions are prepared, it cannot be assumed that potential respondents will interpret instructions and items just as the evaluator intended. The best practice is to administer the survey to a small sample of people to learn how they understand the instructions and the items. After doing so, it is likely that the survey will need to be revised once more.</p>

<h4 id="case-study-4">CASE STUDY 4</h4>

<p>Primary Care Behavioral Health abused, neglected, and abandoned children. The outpatient clinic is an expansion of the commitment to underserved children in the community. Case Study 1 describes the evaluation of a later expansion to this clinic.</p>

<p>The Primary Care Behavioral Health (PCBH) pilot was grant funded and designed to provide feedback in order to improve the program as the grant cycle progressed. Evaluators met with the primary investigator regularly to report on patient satisfaction and the prescribed outcome measures to ensure that the program was meeting the needs of patients and families in a satisfactory way and meeting targets for increasing services.</p>

<p>Meeting Needs</p>

<p>The evaluation noted that the National Institute of Mental Health reports that one in five children present a “severely debilitating mental health disorder,” yet only 20% receive the mental health services they need. Further, the Kids Count Data Center estimates that 24% of Kentucky children aged 2 to 17 have one or more emotional, behavioral, or developmental conditions. Ensuring access to good- quality, integrated physical and behavioral health services for vulnerable children is ongoing and challenging. The PCBH pilot hoped to meet the needs of children by expanding behavioral health care access for children and their families, specifically by embedding a staff clinical psychologist 30 hours each week in the exam rooms suite of the clinic along with the primary care providers.</p>

<p>Implementation</p>

<p>Electronic medical records were audited for the year prior to the start date of the PCBH pilot and the number of mental health services provided before and after the pilot was implemented were compared, documenting the solid implementation. Additionally, the evaluators were able to assess how the data was entered, where it was stored in the record, and what difficulties there were with codes and other records of visits. The evaluators then met with staff to problem-solve procedural issues to address some of the identified challenges.</p>

<p>Stakeholders</p>

<p>The stakeholders who were included in data collection were child patients, their parents, and the primary care providers.</p>

<p>Side Effects</p>

<p>No negative side effects were detected. Positive side effects related to transportation issues and stigma. The intention of the program was, in part, to increase ease for families who might have difficulties with transportation to and from the clinic. This was only measured by administering the parent survey item regarding increasing accessibility to mental health at the same time as pediatric visits. It was also hoped that the introduction of behavioral health in a primary care setting would normalize behavioral health services and serve to reduce the stigma that often accompanies mental health issues. This was not measured; however, primary care providers gave feedback that patients and parents seemed to be comfortable with the combination of behavioral health in the primary care setting.</p>

<p>Improvement Focus</p>

<p>One particularly clear aspect of the evaluators’ approach that illustrates an improvement focus was their detailed examination of the electronic medical records and their subsequent collaboration with staff to find workable solutions to address identified issues. The goal clearly was to help the agency learn from its experience with the system and move forward in a better way.</p>

<p>Outcomes</p>

<p>Mental health outcomes were measured for child patients, and parents completed a survey after each encounter with the behavioral health psychologist to determine their satisfaction with the mental health services provided. Primary care providers also completed a brief pre- and six-month post-survey assessing satisfaction with the PCBH pilot. In addition, a semistructured interview was conducted at the end of the pilot year for more in-depth qualitative information.</p>

<p>Results demonstrated that significantly more children were served with the psychologist embedded in the clinic than before the PCBH pilot program began. Qualitative data showed that the stakeholders had favorable impressions of the model and were hopeful that the program would continue. Although this case is not unique in reporting both quantitative and qualitative data, it is a good illustration of an evaluation that used multiple formats and sources of data, rather than relying on a single source such as patient self-report. Specifically, observational data on patients, self-report from parents, and surveys from providers gave a much more well-rounded summary than is typically reported.</p>

<p>Nuances</p>

<p>The support staff and primary care providers’ attitudes about the PCBH were important but not measured. Perhaps the greatest unmeasured mediator is the rapport building and skills the psychologist was able to demonstrate with patients and providers, although data showing the psychologist was rated very highly by parents and other providers does suggest that the relationship may have been a crucial factor. Although many evaluations try to examine what factors may have been important mechanisms, it is quite common, as in this case, for the final report to acknowledge how potential explanations for the outcomes eluded measurement and analysis.</p>

<p>Summary</p>

<p>When single sources of data are the only basis for an evaluation, the possibility exists that the conclusions are the result of the choice of   that source rather than the actual workings of the program. It is true that sometimes using multiple sources and methods can yield</p>

<h2 id="summary-and-preview">SUMMARY AND PREVIEW</h2>

<p>The use of measures of different variables from multiple sources of information is one of the core qualities of valid and useful evaluations. When choosing measures of variables, evaluators use the criteria for good measurement instruments to evaluate the specific approaches being considered. Is something important being measured? Is the approach sensitive to small changes? Does the measure seem valid, reliable, and cost- effective? Could the measure itself create changes in the variable being measured? In choosing variables and information sources, evaluators seek data sources that are not subject to the same limitations;  instead,  evaluators seek variables that provide information from different perspectives.</p>

<p>The selection of appropriate criteria for evaluation studies and the use of good ways to measure criteria are marks not only of competent evaluators but also of ethically sound work. As social scientists whose findings can have practical impacts on others, evaluators face a greater number of ethical dilemmas than those who study only theoretical questions. Chapter 5 deals with some of these ethical issues.</p>

<h2 id="study-questions">STUDY QUESTIONS</h2>

<ol>
  <li>Consider a program setting with which you are familiar. You might think of the Financial Affairs Office of your college, a hospital laboratory, or a preschool center. Think of some survey items that the participants would be competent to answer and some that require specialized training to answer and thus should not be addressed to participants.</li>
  <li>In that same setting identify the groups who might be surveyed or interviewed to obtain information on he quality of the program. On what aspects of the program is each group especially able to provide    useful information?</li>
  <li>Consider the variables and information sources that might reflect the quality of the program you have been considering. Arrange variables in two columns—place variables that would be expected to put the program in a favorable light in one column and variables that would be expected to highlight program problems in the second column. Compare the columns. How do those two sets differ?</li>
  <li>Think of some evaluation issues that can only be answered using interviews and some that can be answered best by survey items with set answer options (i.e., agree strongly through disagree strongly).  How do these two sets of questions differ from each other?</li>
  <li>Suppose that an unethical evaluator wanted to stack the deck to guarantee a favorable evaluation of a program. What types of variables would one use to achieve this? By contrast, suppose that someone  wanted to guarantee an unfavorable evaluation. What types of variables would one choose to do that?</li>
</ol>

<h2 id="additional-resource">ADDITIONAL RESOURCE</h2>

<p>Fowler, F. J. Jr., &amp; Cosenza, C. (2009). Design and evaluation of survey questions. In L. Bickman &amp; D. J.     Rog (Eds.), <em>Handbook of applied social research methods</em>, rev. ed. Thousand Oaks, CA: Sage.</p>

<p>This chapter provides a very readable description of the development of surveys with many illustrative items, good and bad. Many stakeholders who request evaluations seem to believe that anyone can write survey questions. The problem is that the findings from poorly prepared surveys are seldom interpretable.</p>

<h1 id="5ethics-in-program-evaluation">5 Ethics in Program Evaluation</h1>

<h2 id="examples-of-the-problem">EXAMPLES OF THE PROBLEM</h2>

<p>Evaluators often find themselves in ethical conflicts that are seldom experienced by social scientists engaged in basic research. Although the following scenarios are hypothetical, experienced evaluators can identify with the problems described below.</p>

<h3 id="a-project-that-cannot-be-done-well">A Project That Cannot Be Done Well</h3>

<p>Evelyn Marshall works for a social science firm, Evaluation, Inc. The firm’s staff is interested in winning a contract to evaluate an early parole program recently approved by a state legislature. The central question to be answered is whether early parole leads to better outcomes (e.g., fewer rearrests) after release from prison  relative to the outcomes of prisoners who serve a greater portion of their sentences. The wording of the     request for proposals is clear: the legislature wants an evaluation that examines whether the program causes a lowered arrest rate after release. Further, the evaluation is needed within six months. Because early release is dependent on good behavior, Marshall knows that those who are released early are already viewed as less likely to be rearrested soon after release. A reasonable test of the outcome as defined should take at least several    years to collect meaningful data. In this situation Marshall knows that she does not have enough time to test    the effectiveness of the program. Should she and her colleagues prepare a proposal even though they know    that it is impossible to do what the legislators want?</p>

<h3 id="advocacy-versus-evaluation">Advocacy versus Evaluation</h3>

<p>Morris Franklin completed an evaluation of the Community Mental Health Center outreach program for high school students whose poor school performance is thought to be related to drug abuse. As typically occurs, Franklin found evidence that supports the program’s effectiveness (all stakeholders like the program), evidence that does not support the program (grades of the participants did not go up), and some ambivalent evidence that could be interpreted as favorable or unfavorable depending on one’s point of view (program students hold more part-time jobs than those not in the program). Franklin’s report was received favorably by the center’s director. Meetings were scheduled to consider the possibility of adding a tutoring phase to the program to help improve the participants’ grades. Franklin felt that he had done a good job. Later that week, however, the director asked him to write a proposal to the state to permit extending the program to other high schools in the area. When Franklin mentioned the negative findings, the director told him to focus on the positive findings. “I want an upbeat proposal,” he said. Would it be unethical to write the proposal but not mention the known weaknesses of the program that the evaluation detected?</p>

<h2 id="standards-for-the-practice-of-evaluation">STANDARDS FOR THE PRACTICE OF EVALUATION</h2>

<p>The need for ethical standards in research has prompted a number of organizations to develop statements defining ethical principles to guide the work of research teams (e.g., American Educational Research Association, 2002; American Psychological Association, 2010; American Sociological Association, 1999). Because the findings of program evaluations may be applied soon after evaluations are completed, evaluators face many more situations calling for ethical choices than do basic researchers, and many of these choices are different from those faced by laboratory scientists. Consequently, several statements of principles have been prepared specifically for the practice of program evaluation (Joint Committee on Standards for Educational Evaluation, 1994). <a href="#_bookmark3">Figure 5.1</a> includes the ethical principles adopted by the American Evaluation Association (2004).</p>

<p><em>Figure 5.1</em> Ethical principles adopted by the American Evaluation Association.</p>

<p>This chapter includes material based on the statements of ethical conduct in research, as  well  as  descriptions of good program evaluation practices. Ethics in evaluation means more than respect for research participants and honesty with money and data. Evaluators have the responsibility to provide clear, useful, and accurate evaluation information to the stakeholders with whom they work. Furthermore, evaluators seek to  work in ways that have the potential to improve services to people. Working in settings designed to help    people means that reports might have immediate relevance to the organization sponsoring  the  program  whereas the work of social scientists will go through a period of refinement before being utilized by organizations; for that reason, errors in basic research are less likely to harm people immediately. By contrast, poorly done evaluations have quickly affected the provision of services to people, disrupted the staffs of service organizations, and encouraged the use of harmful, novel medical treatments. Ethical concerns, thus, relate to    all stages of an evaluation—from initial planning through the presentation of the results to interested parties.     In the hypothetical example above, Morris Franklin learned that there may be crucial ethical challenges even after the evaluation has been completed.</p>

<p>As you will see in the following sections, ethical issues are divided into five categories: treating people ethically, recognizing role conflicts, serving the needs of possible users of the evaluation, using valid methods, and avoiding negative side effects of the evaluation itself.</p>

<h2 id="ethical-issues-involved-in-the-treatment-of-people">ETHICAL ISSUES INVOLVED IN THE TREATMENT OF PEOPLE</h2>

<p>The first responsibility of an evaluator, as it is with the basic researcher, is to protect people from harm. Since harm can be done to people in a variety of ways, concerned evaluators guard against harm to all people associated with a program. Further, federal regulations specify detailed rules and procedures to follow when conducting research with people. These regulations are known as 45 CFR 46, which refers to the Code of Federal Regulations, Title 45, Part 46 (U.S. Department of Health and Human Services, n.d.).</p>

<h3 id="detecting-ineffective-programs">Detecting Ineffective Programs</h3>

<p>Often the first issue evaluators face concerns whether any harm can come to someone receiving the program  that is being evaluated. Although medical, educational, and social service programs are offered  with  the purpose of helping those who participate, sometimes programs have either no impact or a negative impact.    Few social service and education programs have as dramatic negative side effects as some medications have had (Mosher &amp; Akins, 2007). Yet, attractive ideas appear that turn out to fail to help and indeed hurt those who hoped to benefit. Most people agree that self-esteem is related to doing well, but the ways to help children develop self-esteem are not well understood. It now seems that evaluations of teaching methods centered on increasing children’s self-esteem have shown that merely increasing self-esteem does not lead to increased achievement (Bronson &amp; Merryman, 2009). The students like teachers who encourage them, but do not correct their homework, but less is learned compared to a more structured teaching approach. These findings might partially explain the continued lag in mathematics and science achievement of American children compared to children of other nations.</p>

<h3 id="obtaining-informed-consent">Obtaining Informed Consent</h3>

<p>Another way to protect people is to obtain prior agreement from program participants who are to take part in an evaluation. This is especially important when evaluators plan an evaluation that includes random assignment of program participants to different forms of treatment or to a control group not receiving the treatment. When people are asked for their agreement, it is important that their consent be informed—that is, they must understand the request before giving consent. Informed consent means that potential participants themselves make the decision about whether to participate, and that sufficient information about the program be provided to enable them to weigh all alternatives. If a person is misled or not given enough information about the risks involved, then informed consent has not been obtained, even if a person has signed an agreement to participate.</p>

<p>Attempting to provide enough information to enable people to give informed consent can create an additional ethical dilemma for an evaluator. Revealing too much information about a new form of service can create expectations on the part of the participants or demoralize those not selected for the new program (Cook &amp; Campbell, 1979). When informed consent procedures have the potential to change the behaviors of the people in the evaluation, then the validity of the evaluation can be threatened. There is no clear way to resolve such a conflict. One approach, included in the American Psychological Association’s ethical principles, is to consider the potential harm to the participants. If the potential for harm is low and the harm, if any, is very small, the need for the evaluation may make it ethical to conduct the evaluation. Sometimes, those not getting a service immediately can participate in the program at a later time.</p>

<h3 id="maintaining-confidentiality">Maintaining Confidentiality</h3>

<p>Information gathered during an evaluation is to be treated with the utmost care so that the privacy of program participants or managers not be violated. The confidentiality of information can be protected in a number of ways. Identifying data with a person’s name is not always necessary. If matching information from various sources is necessary, evaluators can use identification codes that only a particular respondent would recognize, such  as  the  first  name  and  birth  date  of  the  respondent’s  mother.  If  contacting  the  respondent  later  is necessary, the project director alone should keep a master list of the respondents’ names and addresses and the code identifying the respondent. Some evaluators working with very sensitive information have stored the names and codes in a different country. Few evaluators deal with information that sensitive; however, confidentiality once promised must be preserved.</p>

<h2 id="role-conflicts-facing-evaluators">ROLE CONFLICTS FACING EVALUATORS</h2>

<p>Evaluators gather information for the purpose of assessing the quality of program plans, the fidelity of implementation, or the value of outcomes. Since people serving on the staff of programs earn their living from their work, it should not be surprising that conflicts can occur between evaluators and program staff. The   clients served by a program also have a stake in the conclusions of an evaluation; if the program is changed,  they may lose a needed service. The stakeholders of publicly funded programs include government agencies  and taxpayers. Although few taxpayers are aware of evaluations or even, for that matter, the programs themselves, taxpayers’ interests are served when programs achieve objectives efficiently. In an important sense, many ethical issues arise from conflicts of interest among the stakeholders involved with the program. Ideally, evaluators serve all stakeholders even though evaluators are usually employed by only one stakeholder—most often the organization management. As mentioned in Chapter 2, forming clear ideas of who wants  the evaluation conducted and how all the stakeholders could be affected by an evaluation is crucial. The stakeholders can be expected to have different, even conflicting, interests; anticipating these conflicts can spell the difference between a poorly received evaluation due to unresolved disagreements among stakeholders and    a carefully balanced, albeit controversial, evaluation.</p>

<p>Identifying the stakeholders of a program is an important ethical task. Imagine an evaluator was given the task of conducting an evaluation of a new method of sentencing convicted criminals. Suppose that it was   widely believed that the sentencing practices of judges were too lenient and varied widely from judge to judge. Suppose also that the state legislature took the determination of length of jail sentences for certain crimes out   of the hands of judges: a defendant judged guilty of one of these crimes now faced a sentence determined by law, not by the judge. The desired outcome of such a law would be a lowered crime rate because those inclined to crime would know that if caught, the sentence will not be light. Who would be the stakeholders for an evaluation of such a law?</p>

<p>First, the state legislators are stakeholders because they wrote the law that was being evaluated. Those state legislators who voted for the law likewise would have wanted a favorable finding. By contrast, some legislators who opposed the law may have preferred to see their position vindicated by a finding that the law did not have the intended effect. The governor would have an interest in the outcome of the study because she or he may have proposed the law and would want all evaluations of the work to be favorable in order to enhance chances of reelection. All citizens who look to the courts to help in preventing dangerous individuals from committing additional crimes would be stakeholders as well. Judges, too, would be affected because their day-to-day behavior was being studied. Prison officials needed to be aware of the evaluation findings, especially with respect to the possible influence of the new sentencing procedures on their facilities. Police officers also would have been interested to learn how the new sentencing law had been implemented. Defense lawyers, defendants,  and  the  state  attorney’s  office  would  also  have  stakes  in  the  outcome  of  the  evaluation  because their strategies might depend on how sentences were determined. Clearly, many groups care about the findings of program evaluations.</p>

<p>Evaluators should try to minimize potential conflicts among the stakeholders before the evaluation begins. Evaluators are subject to less pressure if they can negotiate agreements on issues such as who will have access  to the findings, what information will be used, and how different patterns of findings will be interpreted. If      the study begins without settling these questions, the probability that different groups can manipulate the evaluation to suit their own purposes increases. At times, suspicions and disputes about ownership of information have made it impossible to conduct a credible evaluation. Individuals who do not want an evaluation conducted benefit from such a result; all others, however, lose.</p>

<p>Public conflicts among stakeholders complicate the work of evaluators; moreover, less visible conflicts also exist. Some writers argue from a social justice ethical point of view (see Rawls, 2000) that evaluators should examine the program assumptions and outcomes in order to learn if justice is served by the program and how    it is administered. Evaluators try to detect when programs lead to undesirable outcomes for some stakeholders that might be overlooked. Guiding Principle E, Point 5 (see Figure 5.1 includes this point: “[E]valuators will usually have to go beyond analysis of particular stakeholder interests and consider the welfare of society as a whole.”</p>

<h2 id="recognizing-the-interests-of-different-stakeholders">RECOGNIZING THE INTERESTS OF DIFFERENT STAKEHOLDERS</h2>

<p>Considering the interests of different stakeholders is important for making an evaluation as useful as possible  for all those who may use or be influenced by an evaluation. Figure 5.2 presents an older yet still relevant example that illustrates how stakeholders’ views can lead to different conclusions, even when interpreting the same information. Focusing on the needs of one stakeholder group can easily lead to narrow and misleading conclusions (McGarrell &amp; Sabath, 1994).</p>

<h3 id="program-managers-are-concerned-with-efficiency">Program Managers Are Concerned with Efficiency</h3>

<p>Those who are responsible for managing an organization are concerned about the efficiency of its operations. For-profit firms must produce goods and services in an efficient manner; otherwise, competing firms will win a greater proportion of the market. In time, inefficient firms go out of business or are bought by more efficient ones. Nonprofit human service agencies do not operate on the same principles as private firms; however, effective managers seek to provide the most service possible within limited agency budgets. In a manager’s judgment, the most important aspect of an evaluation may frequently be information on the efficiency of the program. Chapter 12 covers cost-effectiveness questions important to managers.</p>

<p><em>Figure 5.2</em> An illustration of how stakeholders’ values can conflict. (From Brotman, B., “Workfare”: What state terms success others call boondoggle. <em>Chicago Tribune,</em> Sec. 3, pp. 1, 4, January 2, 1983.)</p>

<h3 id="staff-members-seek-assistance-in-service-delivery">Staff Members Seek Assistance in Service Delivery</h3>

<p>The needs of the staff are best served if the evaluation can provide practical guidance, improving the effectiveness with which they serve clients, students, patients, or customers. An evaluation of the flow of information through a large division of a firm may reveal inefficiencies. If the staff is provided with viable alternatives to the current procedures, they may be well served by an evaluation. An evaluation of teaching effectiveness in a college can help identify the areas in which new faculty members have the most difficulty  and, thus, lead department chairs and college deans to develop preventive actions to assist new  faculty  members before serious problems fester (Angelo &amp; Cross, 1993). An evaluation of a medical  residency program can estimate not only the proportions of unnecessary medical tests ordered by medical residents, but  the reasons for such errors in judgment. In this way education programs can be improved and residents can  serve their future patients more effectively (Posavac, 1995).</p>

<p>Evaluators also know that an evaluation is a good vehicle for recognizing the good work of staff members. Evaluations that focus on shortcomings to the exclusion of providing positive feedback are seldom as valued as more balanced ones.</p>

<h3 id="clients-want-effective-and-appropriate-services">Clients Want Effective and Appropriate Services</h3>

<p>The people for whom a service is designed frequently have no voice in the planning and implementation of either programs or evaluations (Bowen &amp; Tillman, 2015; Morris, 2015; Sturges, 2015). This oversight means that the group for which the program was developed is often the least consulted. However, participants are not unified, they seldom have spokespersons, and they do not hire evaluators. Among the ways evaluators can fulfill their responsibilities to participants is to compare the participants’ needs with the service offered, to help the staff and manager better understand those needs, and to structure recommendations around both needs and strengths. The interests of the manager and staff have sometimes taken precedence over those of the participants. It should be noted that there is no reason to assume that program participants—students, patients, drug abusers, or trainees—know fully what they need, but that does not mean that their views can be ignored.</p>

<p>Another aspect of the stake participants have in an evaluation is their interest in continuing to receive service during an evaluation. Conducting an evaluation without causing any disruption in normal service is usually impossible; however, ethical concerns require that service disruption be minimized.</p>

<h3 id="community-members-want-cost-effective-programs">Community Members Want Cost-Effective Programs</h3>

<p>Most service agencies receive some of their financial support from local community residents through taxes   and contributions, or indirectly through reductions in property taxes often enjoyed by nonprofit organizations.  In some ways the local community is in a position similar to that of the people served: The community is dispersed and does not hire evaluators. Human service agencies often receive further support from national or state bodies, charitable organizations, and foundations. These groups certainly have a stake in the success of   the programs they support. Frequently government offices and foundations commission evaluations of  programs receiving their support. In these instances their interests are probably well protected.  Ethically planned internal evaluations also reflect the interests of the groups that supply financial support for the service agencies.</p>

<h2 id="the-validity-of-evaluations">THE VALIDITY OF EVALUATIONS</h2>

<p>After potential harm to participants is minimized, possible role conflicts explored, and stakeholder needs identified, evaluators can turn to ethical issues associated with the validity of an  evaluation  project.  Conducting evaluations that are not appropriate to the purposes for which they were commissioned is just as unethical as, for example, not protecting the confidentiality of information obtained from participants. The following sections cover the ethical ramifications of four of the most frequently found threats to the validity of evaluations.</p>

<h3 id="valid-measurement-instruments">Valid Measurement Instruments</h3>

<p>Evaluators in educational and mental health settings frequently use standardized, published tests when measuring the expected outcomes of programs being evaluated. The most well-developed  tests  are  standardized achievement tests designed to estimate a child’s progress in school. Because these tests are so well developed, there is a temptation to use them even when they may not be appropriate to measure the outcomes   of a program. In other words, one cannot simply ask if a measurement tool is valid because validity varies depending on the setting and the specific program participants. Choosing an inappropriate way to measure a hypothesized outcome can obscure the effects of a program (Lipsey, 1990; Lipsey, Crosse, Dunkle, Pollard, &amp; Stobart, 1985) or, even worse, lead to a misleading conclusion. For example, in an evaluation of a course on ecology, a standard achievement subtest on science that included questions on hygiene, biology, and earth science was used as an outcome variable (Joint Committee on Standards for Educational Evaluation, 1994).  This subtest was not appropriate for measuring the achievement of the students in the innovative ecology course.  The  evaluation’s  negative  findings  did  not  reflect  the  actual  outcomes  of  the  program.  The  school district would have been better off had the evaluation not been conducted at all; the program was saddled with   a negative evaluation that—although not valid—had to be explained by program supporters.</p>

<h3 id="skilled-data-collectors">Skilled Data Collectors</h3>

<p>It is likely that even people with little training can competently administer a standardized, pen and paper test; however, many evaluations use information collected through personal interviews. Interviewing is not an easy job. Good interviewers possess interpersonal skills and common sense that permit them to obtain the information needed while maintaining the goodwill of the person interviewed. It requires a degree of maturity and a respect for truth to be able to record and report attitudes at variance with one’s own.</p>

<p>Skill in gaining the cooperation of people is also required of good interviewers. Interviewers need to exercise polite persistence as they seek to speak with potential interviewees. Carey (1974) interviewed terminally ill patients as part of an evaluation of hospital care for dying patients. It is difficult to conduct such interviews without appearing to be more concerned about getting information than the welfare of the patients and their families. Children present a very different challenge. Interviewing children in a compensatory education program designed to improve self-confidence, study habits, and social skills would require patience and skill to gain the cooperation of the children. An interviewer without experience in working with children may learn very little about their feelings and attitudes because children do not respond to social expectations as adults do. Indeed, if young children do not feel at ease, they might not say anything at all to an interviewer!</p>

<h3 id="appropriate-research-design">Appropriate Research Design</h3>

<p>One theme of this book is that the research design must fit the needs of those who might utilize the information sought. As later chapters illustrate, different evaluations require more or less scientific rigor and more or less deep understanding of the participants. However, once an information need has been articulated, conducting the evaluation would be unethical if it is known at the outset that the planned evaluation cannot answer the questions that the sponsors asked. One would hope that this would be unusual; however, Vojtecky and Schmitz (1986) concluded that few evaluations conducted on health and safety training programs provided accurate information on program outcomes. This seems related to the dilemma faced by Evelyn Marshall, as outlined at the beginning of this chapter; it is unlikely that she can conduct an evaluation that answers the questions the state legislature had initially specified.</p>

<p>At this point Marshall has at least three alternatives: (1) go ahead with the project and use her skills to make it seem as though the completed evaluation answered the questions originally posed, (2) decline to conduct the evaluation and hope that she can find an alternative contract to keep herself productively employed, or (3) negotiate about the actual questions to be addressed by the evaluation. The first alternative is unethical and the second is risky for Marshall; however, the third may well prove productive. Frequently sponsors of evaluations are unskilled in research methods and do not know when they have given an evaluator an impossible assignment. It may be that the legislature does not really need an evaluation that relates changes in parole procedures to later parolee behaviors. What is needed may be only a careful documentation of the actual   implementation   of   the   new   law   and   stakeholders’   reaction   to   it.   If   negotiations   reveal   that implementation is the primary issue, Marshall can conduct the evaluation; in fact, she can do a better job than she would have been able to if she had to devote effort trying to convince others that she was answering the initial questions.</p>

<h3 id="adequate-descriptions-of-program-and-procedures">Adequate Descriptions of Program and Procedures</h3>

<p>A basic characteristic of science is its public nature. Science is to be conducted so that others can evaluate the procedures used and repeat the research. In this way, mistakes and misinterpretations can be detected. Frequently evaluations are not described in sufficient detail to permit others to understand either the program or the evaluation procedures. Patton (1980) illustrated the advantage of describing the implementation of a program as part of a program evaluation. He found that the evaluators of a program to help young low- income mothers learn parenting and financial management skills concluded that the program was ineffective when, in fact, the program had not been implemented. If the evaluators had observed and described implementation, they would have learned that the planned program had not been put in place. A large proportion of evaluations have not addressed implementation issues (Lipsey et al., 1985; Shadish, 2002; Summerfelt, 2003). It would be very valuable to show that degree of faithful implementation of a program was related to better outcomes (Saunders, Ward, Felton, Dowda, &amp; Pate, 2006).</p>

<p>Besides describing the program, the evaluation procedures should be presented in enough detail so that others can understand how the evaluator obtained and analyzed information. As will be mentioned later, not all interested parties want to know all the details of the evaluation procedure. However, such detailed reports should be available to people who are considering implementing the program elsewhere or who want to compare evaluations of similar programs from different settings. If it is impossible to compare evaluations because they are inadequately reported, one might wonder if the report writers were clear in their own minds about what was done.</p>

<h2 id="avoiding-possible-negative-side-effects-of-evaluation-procedures">AVOIDING POSSIBLE NEGATIVE SIDE EFFECTS OF EVALUATION PROCEDURES</h2>

<p>The ethical issues in this section are less central to the conduct of basic research; however, in a program evaluation these questions can take on immense significance both for evaluators and for stakeholders affected  by the evaluation study.</p>

<h3 id="can-someone-be-hurt-by-inaccurate-findings">Can Someone Be Hurt by Inaccurate Findings?</h3>

<p>Inaccurate findings can show either falsely positive findings—erroneously suggesting that the program is effective—or falsely negative findings—erroneously suggesting that the program is not effective. When such false conclusions are due to random statistical variation, the first false finding is called a <em>Type I error</em>, and the second is termed a <em>Type II error</em>. There is clear evidence medical research sometimes provides support for treatments that are later found to be of marginal or no value (Ioannidis, 2005); the positive findings of the original evaluations may have been due to Type I errors.</p>

<p>Such false conclusions can also be made when insufficient attention is paid to the design of an evaluation. Without careful thought, evaluators can focus on the wrong variables or use too short a time span to show either positive or negative effects. At other times an evaluator’s enthusiasm for a program may lead to falsely optimistic conclusions. Inadvertently encouraging the use of a harmful program due to insufficient care in conducting an evaluation is an ethical problem with which basic researchers need rarely be concerned; however, it is often an important issue for evaluators.</p>

<p>In contrast to falsely favorable evaluations, falsely negative evaluations can also harm people by  encouraging the elimination of beneficial services. An overlooked aspect of a summer preschool program for children from low-income families, involving mothers in the education of their children, led to a negative evaluation (Lazar, 1981). Showing mothers how to work with their children and introducing them to a school system encouraged them to develop a level of involvement with the education of their children often not regularly found among lower income families. Evaluations that focused only on the intellectual growth of the children during the eight-week program were too narrow. The falsely negative conclusions could have harmed other children if other programs had been eliminated.</p>

<h3 id="consider-statistical-type-ii-errors">Consider Statistical Type II Errors</h3>

<p>Type II errors, being unable to conclude statistically that a program is effective when it is, in fact, effective, can occur because the sample of program participants involved in the evaluation was small or just happened to be atypical, and when the measures of outcome were of low reliability. Evaluators observe samples of participants, seldom whole populations, and the information sources available to evaluators are never perfectly reliable. Basic researchers worry about Type II errors because conducting research that yields inaccurate conclusions is a waste of time. However, Type II errors in a basic research study typically cause no harm to others. Evaluators, on the other hand, have an additional worry about random statistical errors: they do not want to conclude falsely that a valued program is ineffective. Thus, evaluators are much more concerned about Type II errors than are basic researchers (Lipsey et al., 1985; Sackett &amp; Mullen, 1993; Schneider &amp; Darcy, 1984).</p>

<p>Unfortunately, evaluators work in situations that make Type II errors very likely. In an attempt to reduce the demands made on program participants providing information, short, thus less reliable, surveys might be used and the number of variables might be limited. To reduce the disruption of services, only a few participants may be tested. A study of the effects of oat bran on cholesterol levels involved only 20 healthy participants (Swain, Rouse, Curley, &amp; Sacks, 1990). It should not have been surprising that no effect was found; follow-up evaluations showed that the conclusion of the initial evaluation was a Type II error (see Ripsin et al., 1992). After reviewing 122 published program evaluations, Lipsey et al. (1985) concluded that the research designs of a large proportion of evaluations were too weak to detect even a moderately sized effect, not to mention a small effect. Two ways to reduce Type II errors are to use large samples and outcome measures with high reliability (Lipsey, 1990).</p>

<h3 id="pay-attention-to-unplanned-negative-effects">Pay Attention to Unplanned Negative Effects</h3>

<p>Ethical evaluators are careful to examine the programs as implemented, not just as designed. One aspect of this issue is the low level of actual implementation of programs, as discussed earlier. A second aspect of this issue involves possible negative side effects of a program. Just as physicians are concerned about side effects of medications, evaluators work most effectively when they are alert to unexpected negative effects of a program. For example, using food stamps may embarrass recipients, prison regulations may cause dependency leading to more difficulty in making decisions after release, and an arbitrary method of introducing safer working conditions may alienate employees. Such outcomes, of course, are not found in the goals of the programs as planned.</p>

<p>Because program planners and managers might not anticipate the possibility of negative outcomes, evaluators can make useful contributions if they detect important negative side effects of programs. A sexual harassment educational program given without prior announcement at regularly scheduled university  department meetings had an adverse effect on male participants who felt they were being accused of doing wrong (Bingham &amp; Scherer, 2001). Recognizing a negative side effect can lead to an improved program or manner of presenting one. <a href="#_bookmark5">Figure 5.3</a> includes some additional negative side effects from policies  with  desirable goals. Sieber (1981) and Howard (1994) describe at length negative side effects from policies and  laws that added to the very problems that they were supposed to alleviate and created new problems. What is most critical is that the evaluator not hide negative side effects.</p>

<p><em>Figure 5.3</em> Serious negative side effects unexpected by the program developers.</p>

<h3 id="analyze-implicit-values-held-by-the-evaluator">Analyze Implicit Values Held by the Evaluator</h3>

<p>Conflicts between the role of the program advocate and the role of evaluator have already been mentioned. Other unexamined values may be hidden in statistical analyses. It is seldom mentioned that a  common  statistical procedure such as presenting an average (e.g., the mean) improvement requires the implicit assumption that the benefit for one participant is as valuable as the benefit to other participants. An early evaluation of the impact of watching “Sesame Street” examined the overall mean achievement of children and concluded that the series was successful in achieving some of its goals (Ball &amp; Bogartz, 1970). However, when the child viewers were divided on the basis of socioeconomic background, a positive correlation was discovered between improvement in cognitive skills and income level: the gap between the children of lower and upper income groups actually increased (Cook et al., 1975). Although “Sesame Street” was designed particularly to appeal to children of families with lower incomes, it appeared more effective with the children of more well-to-do families. In other words, even though the program had a positive impact, it did not provide the special  help for disadvantaged children that was a fundamental goal of the program. Without a creative reanalysis of  the evaluation, this important—but subtle—finding would have been overlooked. Some writers argue that   when program benefits are uneven, the less privileged group should benefit more (Bunda, 1983). What is important for evaluators to recognize is that merely examining overall effects implies endorsement of a simplistic interpretation of the utilitarian ethics of Jeremy Bentham, who argued  that  the  most  ethical  outcome would lead to the “greatest happiness for the greatest number” (Waller, 2005). Even Bentham recognized disparities among people (Atkinson, 2009). Regardless of what choices are made, it is always best   to deal with ethical issues in an explicit manner.</p>

<h2 id="institutional-review-boards-and-program-evaluation">INSTITUTIONAL REVIEW BOARDS AND PROGRAM EVALUATION</h2>

<p>Students beginning a project for a program evaluation class frequently ask whether they need approval from their  university’s  Institutional  Review  Board  (IRB)  before  beginning.  IRBs  are  required  by  federal  law  to ensure that medical and behavioral research studies are conducted ethically and research participants are protected from harm. Colleges, universities, hospitals, and other agencies sponsoring research are required to have a committee that examines research plans to be sure that participants are treated in compliance with   ethical standards. Do evaluators need to seek the approval of IRBs? There is disagreement among evaluators about this. Newman and Brown (1996) “strongly advise” evaluators to obtain IRB approval. Surely no harm  will be done to an organization if IRB approval is sought. However, requiring IRB approval for program evaluation would lead to rather odd limitations on the activities of program managers.</p>

<p>Imagine an arts and sciences college that began special, low-enrollment sections for first-year students. Suppose that these sections, called “freshman seminars,” were supposed to (a) encourage interaction among class members and (b) orient students to college life while still covering the essentials in the existing courses (i.e., psychology, history, political science, etc.). The college curriculum committee, the dean, and the admissions office would be interested in knowing whether students noticed a difference between freshman seminars and other courses and whether they found the experience valuable. Does the dean need IRB approval to evaluate the implementation of and the reaction to freshman seminars? It would seem strange to say that the dean cannot evaluate part of the curriculum for which he/she is responsible. In fact, 45 CFR 46 explicitly recognizes educational organizations’ rights to evaluate their own programs (U.S. Department of Health and Human Services, n.d.). Furthermore, it would seem strange to say that the dean could evaluate these courses, but that someone working in the dean’s stead would need approval by a committee.</p>

<p>Suppose a student in a program evaluation course has the opportunity to complete a class project for a manager of a human resources department in a nearby company. It would seem odd for the student to need    IRB approval in order to complete a project that the manager wants done and, in fact, supervises. IRB regulations were not designed to control these situations. IRBs are not charged  with  supervising  how  managers evaluate activities in their own agencies. Instead, IRBs are charged with supervising <em>research</em>, especially research done with publication in mind. Many IRBs have concluded that evaluations such as those just described do not fall under their purview and, thus, those IRBs would not seek to examine proposals to  carry out such evaluations. Reports of these evaluations are not going to be  presented  beyond  the  organizations sponsoring the activities being evaluated. Not requiring IRB approval does not mean that one    can be reckless or careless about how program evaluations are carried out. The dean and the human resources manager keep student and employee records confidential as part of their normal work. All people working  under the direction of such managers are expected to follow such ethical practices as well.</p>

<p>The situation changes if a faculty member or college student is <em>not</em> acting as the program manager’s agent.</p>

<p>Suppose that a student approaches a program director requesting to carry out a program evaluation for a master’s thesis. Although the project may be a program evaluation, the project looks a lot more like research instead of an aspect of responsible program management. In such a case, IRB approval is needed prior to gathering data.</p>

<p>Last, exercise common sense: evaluating a sex education curriculum for a middle school is much more sensitive than evaluating a new scheduling procedure for college students. Evaluators of programs in socially controversial areas (such as drug rehabilitation or sex education) would be wise to consult with someone familiar with evaluations in such areas even if there is no IRB in the agency sponsoring the program.</p>

<h3 id="ethical-problems-evaluators-report">Ethical Problems Evaluators Report</h3>

<p>Morris (2015) has recently updated earlier findings regarding the ethical challenges that evaluators say they  have faced themselves. Although readers are encouraged to examine the detailed table Morris presented of 25 distinct problems, two overarching themes are clear: the stakeholders requesting the evaluation exert pressure  on the evaluators to misrepresent the findings in one of a fair number of ways, or participants contributing to   the evaluation are not treated properly. The first problem clearly subverts the entire evaluation process, so it functions as one of the most destructive difficulties evaluators face. The specifics may vary: some stakeholders communicate their expectation that the evaluation will reach specific conclusions, whereas others attempt to prevent evaluators from learning certain information such as evidence of negative outcomes. But whether the interference is explicit or implicit, good evaluators are prepared either to persist with an accurate approach or   to decline to conduct such evaluations.</p>

<p>Related to the second theme of participants being mistreated, a number of writers have noted that such practices clearly follow patterns of power, where stakeholders with little economic or other forms of power are often the ones mistreated (Morris, 2015; Sturges, 2015). At times the mistreatment may appear more subtle, such as when those who receive the program being evaluated do not have a clear option to provide useful data, whether they are not asked or are assessed by inappropriate measures (Bowen &amp; Tillman, 2015). Good evaluators find appropriate ways to respond to such challenges to protect people and the integrity of the evaluations. And the good news is that, most commonly, ethical challenges in evaluations are of modest proportions, and actions well within most evaluators’ skill-sets can often lead to productive resolutions, as in the following case study.</p>

<h4 id="case-study-5">CASE STUDY 5</h4>

<p>Bridges to Tomorrow</p>

<p>In 2007, Metro United Way of Louisville, Kentucky, supported by multiple donors, began a major initiative called the Bridges to Tomorrow project to address child readiness for school with a broader approach than most projects with similar goals. Recognizing that children’s ability to succeed in educational settings is affected not only by individual factors such as motivation but also by important environmental factors such as family stability, the project targeted children’s experiences in child care settings by addressing staff training and their family’s economic situation by providing coaching that was especially prepared to address financial issues. (A later version of the financial coaching component was the subject of Case Study 14.) The Bridges to Tomorrow project was implemented in four community centers in Louisville that provided similar but not identical programming. For example, three of the sites were community centers with existing child care facilities. A fourth center was included despite never offering child care partly because it provided a greater range of racial and other diversity such as clients speaking at least six languages other than English.</p>

<p>Program evaluators Dr. DeDe Wohlfarth and Dr. Ken Linfield at Spalding University began to work with the Bridges project in 2008, providing annual reports from 2009 to 2013. The project involved a number of groups of contributors, some of whom worked independently and had no contact with some others. In particular, a team from another, nearby university conducted the regular formal assessments of the children enrolled in the child care centers that were a part of the Bridges project and presented their own annual reports. Early in the project, the assessment team and the evaluators drew slightly different conclusions from the same data regarding one of nearly 20 analyses—the evaluators used a slightly relaxed level of probability for determining statistical significance due to the limited statistical power and the desire to avoid Type II errors (saying there is no effect when, in reality, an effect is present) in addition to Type I errors. A subsequent report by the assessment team indirectly took issue with the approach used by the evaluators.</p>

<p>Although the program staff did not perceive a major conflict, in an attempt to address the situation in a helpful way, Dr. Linfield contacted one of the members of the assessment team and met, starting with more casual introductions and then proceeding to talk about the project and the teams’ approaches. As a result, the teams first came to an agreement on a common set of parameters for analyses, so their reports in subsequent years were completely consistent. In later years, although the two teams continued to submit their separate reports, they also collaborated on a joint Executive Summary. Although this is an example of a relatively minor resolution of conflict, it illustrates that direct conversations that find and build on common ground are often much more productive than other, more contentious, approaches.</p>

<p>Meeting Needs</p>

<p>The evaluation reports noted the nested needs—a broad need for higher quality child care, many low-income people need better knowledge and skills with regard to financial matters, and a coaching approach is one of the best ways to engage people for long-term change. Addressing all three of these needs at the same time is more likely to build children’s readiness for school in a sustainable way. As virtually all stakeholders in the project already accepted the reality and scope of these needs, the main points were made without extensive documentation of details.</p>

<p>Implementation</p>

<p>Earlier reports provided timely descriptions of the elements that had been fully implemented as planned, as well as the minor changes in some details and the common experience of recruitment of participants occurring at a somewhat slower pace than hoped. Later reports summarized earlier points and provided the perspective of a longer time frame, such as noting that the modifications could be best understood “as a refinement, not a departure, from the original program goals” (Linfield, Wohlfarth, Gibson, &amp; Jones, 2013, p. 2).</p>

<p>Stakeholders</p>

<p>Although most reports noted the various groups of stakeholders implicitly, the final report included data from “Interviews with Key Stakeholders,” noting that these “included Metro United Way Staff (e.g., project manager, CEO, etc.), Vendors/Contractors (e.g., teacher mentors, program evaluators, etc.), and Agency Partners (e.g., community center program directors, coaches, etc.).” (Linfield et al., 2013,</p>

<p>p. 12, emphasis in the original). Further, representatives from the agencies contributing support were present at regular program board meetings throughout the years and especially at the annual meeting.</p>

<p>Side Effects</p>

<p>Comments on side effects were included in some, but not all, reports, noting, for example, that enthusiasm and mutual support among participants at the same centers was an unexpected positive benefit, and that the emphasis on data collection was substantially distracting to some participants.</p>

<p>Improvement Focus</p>

<p>The reports and the frequent interactions between the evaluation team and program staff and administrators were consistently framed toward improvement, noting, for example, the understandable basis for problems with data collection in a service-oriented project. As with many programs with multiple reports over several years, the earlier reports were explicitly formative while later reports focused more on summative results.</p>

<p>Outcomes</p>

<p>The reports addressed a wide range of outcomes, from the formal assessments of children’s readiness for school in six areas, to details of the quality of the child-care centers, to parents’ knowledge and attitudes about finances, to the nature of goals parents determined and the extent of progress toward those goals. As is common, results were mixed with clear evidence of change in some of the areas, but no evidence for improvement in others, although no findings showed deterioration. For example, the child-care centers showed nearly universal, substantial improvement in quality to the point of reaching a ceiling, whereas, despite important work toward financial and other goals, parents showed minimal change in their financial status.</p>

<p>Nuances/Mechanisms</p>

<p>One  of  the  evaluators’  expectations  was  that  an  important  mechanism  in  this  project  would  be  participants’  progression  in  their readiness to change according to the trans-theoretical model of stages of change (Prochaska &amp; Norcross, 2001), so they added measures to assess  participants’  baseline  and  later  readiness.  Qualitative  data  from  focus  groups  and  other  sources  appeared  to  indicate  substantial movement from pre-contemplation to action. But one important challenge in the project was that helping participants with their immediate situation took precedence over data collection, so missing data reduced the statistical power available. Thus, those participants for whom data was available did not show clear evidence of progression on the measures as a group, and the changes documented were not significantly associated with the positive outcomes.</p>

<p>Summary</p>

<p>This multilayered, complex program evaluation demonstrates several key takeaway lessons. First, programs with multiple goals and intentions need multiple assessment measures. Although simplicity is generally ideal, multiprong programs do not typically allow for such simplicity. Second, a key component of success in program evaluation is working collaboratively to resolve differences of opinion, scope, or data analysis. Being direct without being overbearing can help navigate even challenging interpersonal relationships. Finally, complex program evaluations rarely lend themselves to short sound bites of information. Although the goal of program evaluation is to present data in clear, simple, understandable language to key stakeholders, sometimes the answer to whether the program is meeting its goals is, “Yes, sort of. In some respects, yes, and in others, no. Mostly it depends.”</p>

<p><em>Sources</em>: Prochaska, J. O., &amp; Norcross, J. C., <em>Psychotherapy,</em> 38, 443–448, 2001; Linfield, Wohlfarth, Gibson, &amp; Jones, 2013.</p>

<p>Emphasis area in the PsyD program at Spalding University, where she is a Professor  in  the  School  of  Professional  Psychology.  She explains that, for her, being a program evaluator is not another role that she assumes, but is simply another way to describe what she does,    just like “professor” and “clinical psychologist.” Whatever the label, she sees herself as a bridge—a bridge between the university and the community or a bridge between research and people, ordinary people.</p>

<p>Dr. Wohlfarth sadly acknowledges that there is currently a chasm between researchers and everyday people, partly because too many researchers express themselves in ways that have to be translated before they connect with regular people. She notes that empirical evidence has the potential to make enormously valuable contributions to people’s lives, because the right choices need to be based on “meaningful data,” not just gut feelings, which can easily be wrong. For example, she recently led a workshop on parenting and based many of the specific points on clear principles of behaviorism that have been identified through rigorous research. Many people, however, raise their children based on “gut feelings” and ignore research that might be helpful to them.</p>

<p>Dr. Wohlfarth notes “numbers are not bad”; instead, they are tools to help us. We have to ensure, however, that numbers make sense and are relevant for people. DeDe understands this frustrating situation, noting that she is the parent of four teenagers, so anything she uses has to work in the real world. One particular area needing better translation is when findings are based on one particular population and need to be applied to many diverse populations. Addressing these limitations requires knowing the limits of otherwise good tools and understanding how to adapt research when the data fall short. Working at Spalding with its focus on social justice has both helped her refine some of these techniques as well as provided a particularly good setting to use them. As she notes, Spalding is located in one of the ten poorest zip codes in America, so “we don’t have the luxury of being an ivory tower.”</p>

<p>Interestingly, like Ben Birkby (see Profile 1), DeDe had a Graduate Assistantship with Bob Illback at REACH when she was in the PsyD program at Spalding, and her dissertation was a program evaluation. Although it had not been the most obvious choice for her, given her passionate interest in clinical work with children, she once had asked Bob, “What do I need to do to be part of the future?” Bob’s advice was to develop “a diverse set of skills,” and she did. She offers one explanation of why such proactive choices are not more common with a favorite quote: “The problem is not that we can’t read the writing on the wall. The problem is we think it is addressed to someone else.”</p>

<p><em>Source</em>: D. Wohlfarth (personal communication, January 25, 2018).</p>

<h2 id="summary-and-preview-1">SUMMARY AND PREVIEW</h2>

<p>The major ethical issues involved in conducting evaluation studies fall under five topics: the protection of people studied, the danger of role conflicts, meeting the diverse needs of different  stakeholder  groups,  avoiding threats to the validity of the evaluation, and staying alert to negative side effects that may be related    to the program or the evaluation itself. The applied and political nature of the settings in which program evaluations are conducted create more ethical dilemmas for program evaluators than are experienced by basic researchers. Although these conflicts and limitations require evaluators to exercise considerable care  in  carrying out evaluations, these issues also make the role of the evaluator vital and exciting.</p>

<p>This chapter completes the general, introductory section of this text. The next topics concern the specific types of evaluations. Measuring needs for programs and developing methods to monitor programs are essential tasks in themselves, but they also help clarify the conceptual foundations of programs. These aspects of program evaluations are addressed in the next two chapters.</p>

<h2 id="study-questions-1">STUDY QUESTIONS</h2>

<ol>
  <li>Analyze the different views of stakeholders who would be involved with the development of a teacher evaluation procedure in a college or university. Be sure that you do not assume that all students have the same interests or that all faculty members would react to such evaluations in the same ways.</li>
  <li>Contrast the problems that Type I and Type II errors create in basic research versus those in program evaluation, with special regard to ethical considerations.</li>
  <li>Some evaluators do not mention all negative or unfavorable findings in official evaluation reports, but do bring such issues to the attention of program managers during private meetings. What are the reasons favoring such a practice? What problems might this practice create or encourage?</li>
  <li>Suppose you have been asked to develop a report on the wishes of employees of a nonunionized company concerning benefits. After a day of interviewing you bump into an employee whom you do not know as you are leaving the plant. He tells you that the company’s general manager has told the employees to hide their true feelings and thus mislead you into thinking that they are quite happy with the way things are. Because employees do not want to rock the boat, he says that everyone is going along with that idea. The employee hurriedly walks off. If he is telling the truth, what ethical problems does that give you? What should you do next?</li>
  <li>Imagine how to deal with pressure to violate confidentiality.</li>
</ol>

<p>a It is not unknown (although not common) for evaluators to be asked to identify individuals who made certain statements. Consider how to keep this problem from ever coming up.</p>

<p>b Suppose someone alleges that someone else in the organization is engaged in criminal behavior. It could be misappropriation of funds. What ethical duties would you have to the accused, to the accuser, and to the organization sponsoring the evaluation and the program?</p>

<h2 id="additional-resource-1">ADDITIONAL RESOURCE</h2>

<p>Morris, M. (Ed.). (2008). <em>Evaluation ethics for best practice</em>. New York, NY: Guilford Press.</p>

<p>A variety of program evaluation scenarios that call for ethical reflection are presented. Many leaders in program evaluation comment on the situations and describe what they would do in those situations.     Often we learn best when we see what experienced people do as they apply principles in  specific situations.</p>

<h1 id="6the-assessment-of-need"><strong>6</strong> The Assessment of Need</h1>

<p>Human services and educational programs are developed to serve people in need and to facilitate positive development. Congress appropriates funds for federal nutrition agencies, small towns sponsor volunteer fire departments, and school districts build and staff schools because communities have decided that needs of  people will be unmet without such agencies and services. How do we decide that there is a need to be met through community action? Who decides? What information can be gathered to demonstrate the level of community need? Perhaps the <em>wants</em> of a population must be changed so that it recognizes its own <em>needs</em>  (Weiss, 2002). How does an evaluator work with planners who select programs to be offered?</p>

<p>Perceptions of need are greatly influenced by local and national media. A series on local crime presented on TV news caused people to feel less safe even if they had experienced no crime themselves (Heath &amp; Petraitis, 1986). Descriptions of flu led some parents to take their children with colds to emergency rooms. Groups that attract the attention of media often create the sense that their concerns are more important than those of      others (Morgan, Fischhoff, Bostrom, Lave, &amp; Atman, 1992; Slovic, 1993). Such reactions may well mobilize opinions, but judgments about what services are needed should be based on quantified, representative information.</p>

<p>People mean many different things when they say that they need something. In this chapter, a definition      of need is developed to remind evaluators to check on how the term is being used. Next, the most widely used methods  of  studying  needs  for  human  services  without  ignoring  a  community’s  strengths  are  reviewed.  In settings in which an internal evaluator is likely to be called upon to evaluate a program under development, internal evaluators might seek to be part of the planning committee. Involving evaluators in the development effort provides planners with an independent perspective as they formulate program goals and impact models.</p>

<h2 id="definitions-of-need">DEFINITIONS OF NEED</h2>

<p>Many analyses of need do not include a definition of need. When evaluators and program staff talk about assessments of need, they usually refer to measuring a discrepancy between what is and what should be. Roth (1990) pointed out that there are at least five discrepancies that people could have in mind when they speak of needs. There might be a discrepancy between the actual state and (a) an ideal, (b) a norm, (c) a minimum, (d)     a desired state, or (e) an expected state. In social service settings, it is probably safe to assume that  discrepancies from ideals are seldom being discussed. But sometimes discrepancies from a norm are used to define a need as occurs when schoolchildren of normal aptitude read below their grade levels. This approach, however, leads one to conclude that people do not have needs when they enjoy an actual state that equals or exceeds the norm, minimum, or expected states (Scriven &amp; Roth, 1990). This would lead to  the  odd  conclusion that people drinking orange juice each morning do not need vitamin C because they have enough;     a discrepancy definition, if used alone, is not helpful. Most assuredly, people do need vitamin C because  without any we would, in time, become ill. Although needs assessments are not always explicitly linked to program theory, knowing the theoretical reason for an intervention can guide the needs assessment.</p>

<p>It is likely that many evaluators have been able to conduct needs assessments without a clear definition of need because program planners possess an implicit understanding of what is meant; nevertheless, it seems wise to  adopt  an  explicit  definition.  Scriven  and  Roth’s  definition  has  merit:  need  refers  to  something  (X)  that people must have to be in a satisfactory state. Without X they would be in an unsatisfactory state; with X they achieve but do not exceed a satisfactory state. For example, people need access to a health care worker when they accidentally cut themselves severely enough to require stitches. McKillip (1998) describes a difficulty with this definition. While we can all agree that someone needs stitches, we draw different conclusions about needs as we move from community to community. Most people in the world live in situations in which a bachelor’s degree is not needed. However, in nations with knowledge-focused economies a college education   is very important and, indeed, needed to hold a large proportion of jobs. Further, such needs may also change over time.</p>

<p>A teenager may want a cell phone that can transmit images, music, and text to  gain  status  and  convenience, but not having one would hardly make his life unsatisfactory. However, a real-estate executive may need a cell phone for client services because without it, she may lose enough business to place her in an unsatisfactory economic state. In other words, a need cannot be defined without regard to the social context in which a person or group lives and works. Consequently, McKillip (1998) concluded that the decision that an unmet need merits a programmatic response is the result of “… <em>a value judgment</em> [not an objective assessment]</p>

<p>that some <em>group</em> has a <em>problem</em> that can be <em>solved</em>” (McKillip, 1998, p. 263). Note also that only problems that</p>

<p>we think we can solve are treated as unmet needs, which is a good match when evaluating the program that is intended to meet those needs.</p>

<p>Lurking beneath some of the confusion about the definition of need is the distinction between things     people need on a regular basis (such as oxygen or vitamin C) and things needed only once (such as a high  school diploma). Another confusion occurs between those community resources whose use is fairly predictable (such as elementary schools) and those whose use may be predictable for a whole community, but whose use cannot be predicted on an individual basis (such as a trauma center). Individuals may believe that they will never need a trauma center (and most people will be correct); however, the community at large may need a trauma center. Those intrigued by various approaches to needs should study the  table  of  categories  and specific meanings of need in Watkins and Kavale (2014).</p>

<p>When estimating the need for any type of program, distinguishing between the <em>incidence</em> and the <em>prevalence</em></p>

<p>of a problem is helpful. Incidence refers to the number of people experiencing a problem during some time period, such as a year; prevalence refers to the number of people who have the problem at a given time. For example, the incidence of the common cold is high: most adults contract at least one cold each year; children contract more. However, most people recover within 10 days, so the prevalence at any one time is low. The distinction is important: a response to a problem will be different depending on  whether  a  problem  temporarily affects many, or is a long-lasting problem for a few people. For example, attempts to help unemployed people will differ depending on whether it is believed that the unemployed are mostly between  jobs or are likely to be out of work for a long time.</p>

<p>Note that the definition of need does not rely on people knowing that they have a particular need. People may be unaware of a need (an iron deficiency), deny a need (alcoholics usually deny a need for rehabilitation), or misidentify a need (some adolescents desire drugs when they need social skills). Of course, most people know when they need assistance with housing, education, health care, or employment. Unfortunately, there is no single procedure or one source of data that provides all the information required to estimate needs. Still, one important pattern is that those served by programs are less frequently involved in defining needs than more influential stakeholders (Altschuld &amp; Watkins, 2014; Watkins &amp; Kavale, 2014).</p>

<h2 id="sources-of-information-for-the-assessment-of-need">SOURCES OF INFORMATION FOR THE ASSESSMENT OF NEED</h2>

<p>The major sources of need-relevant information include objective data about the health, educational, social, and economic conditions of a community, facts and opinions that the residents themselves provide, and the conclusions of experts who know the community well. Before heading out with surveys in hand, evaluators should address several preliminary issues. First, who are the people whose unmet needs are being studied? Second, what are the resources currently available to these people?</p>

<h3 id="describing-the-current-situation">Describing the Current Situation</h3>

<p><em>Population to Be Studied</em></p>

<p>The people to be studied should be identified before gathering any data. The population might be the residents of a state, the pupils of a school district, or the employees of a factory or business. Instead of defining the population as all members of a geographical area or organization, evaluators might narrow their definition  to unemployed adults, homebound elderly, unmarried pregnant teenagers, or high school  dropouts.  Some public agencies may be limited in whom they may serve. Private organizations such as hospitals and businesses may have fewer limitations and thus more difficulty in identifying the people they wish to serve with a new clinic or new product. A university opening a branch campus would want to know how many of its current students live in the area of the proposed campus, how many area students will graduate from high school   during the next decade, and how many people in the area are currently enrolled in continuing education programs. Some information describing the population would be easy and inexpensive to obtain; sometimes    we can just ask people what they need. A management support company might use a self-report survey with    its own employees containing items such as the illustrative activities in Figure 6.1. Job skills employees say   that they need and want training in would provide one view of unmet needs for job training. Such information would be supplemented by other views of training needs.</p>

<p>The needs assessment report might show that part of the organization or community has great unmet needs, whereas other subgroups require no additional attention. Learning how the people seek to meet unmet needs would be important. Employees of the hypothetical management services company in <a href="#bookmark0">Figure 6.1</a> might seek help from fellow workers causing inefficiency. In a community setting, some people might spend a great deal of time seeking assistance from agencies not designed to provide the needed help.</p>

<p>An evaluator can contribute to the planning process by being concerned with putting numbers on unmet needs. It is one thing to talk about the need for services to juveniles in trouble with the law and quite another    to know the number and percentage of community male adolescents aged 16–19 arrested each year. Saying “The community’s elderly residents lack easy access to social opportunities” is less informative than reporting “2,200 residents over age 75 have no access to public transportation, and half of them would require transportation in order to participate in services offered to them if the services were housed in the town hall.”</p>

<p><em>Current Resources in the Community</em></p>

<p>One common problem in needs assessment is ignoring the strengths of communities and residents when developing programs (Altschuld &amp; Watkins, 2014; Fine &amp; Vanderslice, 1992). Indeed, the concept of needs assessment has been criticized for a myopic focus on problems without regard for how existing resources could be strengthened (McKnight, 1995). Social program planners might take a hint from businesses: it would be unwise to locate a new store or restaurant in a community without noting the location  of  established  businesses. Thus, a careful cataloging of current programs, employers, health care facilities, churches, schools, and community organizations should be included in a needs assessment. The cooperation  of  such  organizations can make programs more successful in reaching people in need, and the integration of existing services and organizations can lead to more comprehensive opportunities for eligible residents.</p>

<p><em>Figure 6.1</em> A self-report of training needs could be part of an assessment of the need for employee training in a company.</p>

<h3 id="social-indicators-of-need">Social Indicators of Need</h3>

<p>Communities needing additional social services probably display many dysfunctional characteristics. Trying to measure the degree of dysfunction—or its converse, well-being—is difficult. One approach is to  track  variables, such as crime and school dropout rates, that are symptoms of social problems (Felson,  1993; MacRae, 2009). Just as symptoms of physical illness can lead physicians to diagnose an illness,  social indicators can suggest the underlying social problems that should be addressed (Carley, 1981; Ferriss, 1988). When a social problem is alleviated (as occurred with the advent of Medicare), social indicators should show a lower degree of social distress and dysfunction.</p>

<p>More and more, information can be found online to address at least some aspects of most needs. U.S. census data, crime statistics, information on health and disease, and more can be located using Internet searches. The eResources provide both descriptions of major categories of information and active hyperlinks to some ways to access that data. It is also important to note, however, that local information, when contrasted with regional and national data, can provide a powerful argument that there are local problems that might be solved or at least relieved by new or refined programs.</p>

<p>It is important to remember that indicators of problems do not reveal how those problems can be solved. Planners need a theory connecting the indicator to an unmet need as well as one connecting a possible intervention to the hoped-for improvement. With physical health, a high temperature indicates a problem. However, the problem causing the temperature is not obvious. Without a valid theory connecting elevated temperatures with a specific illness, ineffective remedies may be used. Recall that during American Colonial times, a flushed face and high temperature were thought to indicate the presence of too much blood, an invalid theory that led physicians to bleed sick people.</p>

<p>Using social indicators to identify community problems and to shape programs requires knowing the relationship between the indirect measures of the problem and the underlying social, educational, or economic malady. Gaining such knowledge is certainly not easy. What malady underlies the decline in achievement in high schools? Some say TV watching and video games, entertainments that divert  attention  away  from reading; others, the absence or inattention of parents who fail to take an interest in school work; and still   others, unimaginative teachers. The alleged maladies suggest different remedies. Even if we agreed on the malady, sometimes several different responses are still suggested. If many teachers are not teaching well, perhaps teacher salaries should be raised to attract more competent people, perhaps we need to test teachers,     or perhaps teacher certification requirements are keeping out people with wide experiences who lack the traditional credentials, but may become excellent teachers.</p>

<p>Do limitations on the use of social indicators mean that they cannot be used? No, but the limitations do mean that social indicators cannot be used as the only source of information about the needs of a community. Social indicators are like physical symptoms; they can tell us that problems exist and where they are most pressing. They cannot tell us what the fundamental causes of problems are or what society should do about them. Other sources of information can assist evaluators and planners to be more specific about the causes of the underlying problems and the options for alleviating them.</p>

<p>It is important to note that social indicators can be corrupted. Fraudulent social indicators are ultimately discredited, but often not until policies have been affected. Information is increasingly difficult to control as   the development of Internet resources have multiplied; however, as with all data, evaluators do well if they carefully assess the credibility of social indicators before using them. With the wide variety of information available, it may become harder to purposefully distort social indicators in developed, knowledge-intensive countries than in the recent past when access to information was more easily controlled. Unfortunately, false reports spread rapidly as well. This warning has become increasingly relevant as the charge of “fake news” is used more and more to try to discredit reports that one simply dislikes.</p>

<h3 id="community-surveys-of-need">Community Surveys of Need</h3>

<p>A straightforward way to estimate community needs is simply to ask people about their needs. The residents of a community have opinions about the development of human services and about the particular services needed. Their opinions can be part of the information used in planning. If a service is relevant to all residents, the community should be surveyed systematically to obtain a representative sample of the opinions of the residents. Obtaining a truly representative sample is extremely difficult and thus expensive. A possible compromise is to use intact groups that are likely to be fairly representative of the community. Such a strategy would not be as expensive because intact groups completing a survey do not require much time from a survey administrator. Public schools and church groups are organizations that attract participants from wide sectors of a community. Depending on the nature of the program being planned, special groups of likely users could be sought. Thus, an interviewer gathering ideas about the needs of the elderly might use more detailed questions when a household including elderly people is found.</p>

<p>A survey to assess needs is often used to estimate the magnitude of need and the acceptability of particular approaches to meeting needs. Preparing a survey requires much care. Because completed surveys look simple, few people realize how much effort is required to write clearly, avoid leading questions, and obtain the information desired (Schwarz &amp; Oyserman, 2001). When assessing needs it is easy to write questions that encourage respondents to endorse plans; one could list various services and ask if the services “should be available.” If we were asked whether our hometown should have agencies able to provide counseling for residents having a personal or family crisis at any time of the day or night, we might answer “yes.”</p>

<p>Although such questions appear to get right to the heart of the matter, they are inadequate in at least three ways. First, the questions do not deal with unmet needs, but with possible solutions to  problems.  As  mentioned in Chapter 2, planners often fall into the trap of dealing with potential solutions before fully understanding the unmet need being addressed. It would be best to include questions on residents’ opinions about the extent of community problems and about their own problems. Second, asking whether a service “should be available” would be a leading question. Attitude measurement research has shown that people tend  to agree more readily than disagree with questions (Babbie, 2009); this bias is called <em>acquiescence</em>. To avoid writing leading questions, it is helpful to use questions that require responses other than agreement or disagreement. Asking people for examples of problems or to compare the relative extent of two different problems might be useful approaches. Third, the direct question suggested above fails to mention costs of providing the service or to acknowledge that providing one service means that another cannot be provided. When respondents are asked what they would pay to support such services or additional safeguards, they are  less enthusiastic (see Viscusi, 1992); however, research has shown that answers to questions about willingness to pay for services may be misleading (S. S. Posavac, 1998).</p>

<h3 id="services-already-available">Services Already Available</h3>

<p>People with unmet needs often seek assistance from a variety of sources that provide services related to what they believe they need, or they travel far to obtain services, or they do without the assistance that could benefit them. Evaluators can contrast the extent of need estimated with the level of services currently available in a community. Furthermore, a thorough analysis of the services available assures planners that new services   would not be duplicating the work of existing agencies.</p>

<p>A planning group exploring the need for additional mental health services should identify all agencies,  public and private, that provide at least some therapy or support for people with mental health or substance abuse problems. Thus, clinics, hospitals, courts, Alcoholics Anonymous chapters, school drug-abuse  counselors, public health visiting nurses, church-based counseling centers, primary care physicians, psychiatrists, and social workers should be considered in the effort to learn about the people being treated in some way for emotional problems.</p>

<p>Planners may be required to work directly with agency files in order to develop a demographic description  of clients. When this is necessary, absolute guarantees of confidentiality must be made and kept. When contacting such agencies for estimates of caseloads, it would be helpful to ask about additional providers of care. Developing estimates of the number of people using services is particularly difficult when services are provided informally, as child care often is.</p>

<p>Information concerning people needing a service, but not getting it, would be very hard to obtain since few  in community agencies would be aware of such people. One way to estimate the numbers of such people is to examine agency reports of individuals who sought assistance but did not receive it because they did not meet qualification requirements, could not afford the fees, or had problems that did not fit  into  the  services  provided by the agency. Love (1986) illustrated another way to include unserved people in a needs assessment. Love used case studies of troubled teenagers to show how the structure of the welfare and mental health  services in metropolitan Toronto made it impossible for an adolescent to get help even though his alcoholic father had thrown him out of his home. The unstable and, at times, aggressive 17-year-old had several recent addresses in different communities and needed several different services. It was unclear where he should have sought help, and only a few agencies could assist him with his multiple problems.  Although  several  compelling cases do not show the extent of unfilled needs, case studies illustrate vividly that service reorganization or expansion may be needed to relieve certain types of needs.</p>

<h3 id="key-informants">Key Informants</h3>

<p>Key informants are people who know a community well and could be expected to know what needs are going unmet. Finding such people is not easy, however. One way to start is to meet with professionals whose work brings them into contact with people who may have unmet needs to be served by the program  being  considered. In addition, people active in the community, such as clergy, politicians, or YMCA leaders, can be approached for information. All key informants can be asked for recommendations of people who might also have information. A sample formed using such recommendations is called a “snowball sample” (Babbie, 2009) because new participants are added to existing ones somewhat like when rolling a large snowball. Guidance counselors in schools might have some good ideas for needed changes in vocational school curricula, and they might be able to recommend human resource directors in local businesses who would have valuable  information on strengths and weaknesses of recent high school graduates. For health issues, physicians, clergy, clinic managers, school nurses, and social workers would all be important. Although surveys can be mailed to such people, a far better level of cooperation can be obtained if they are interviewed personally.
 informants may see a great number of such people daily, the numbers of people in need are  often  overestimated. This tendency is called the “availability” bias because the person making the estimate can easily recall individuals who fit the category (Heath &amp; Heath, 2008). Consequently, psychologists are likely to overestimate the number of people needing counseling and remedial reading teachers the number of people needing literacy tutoring.</p>

<p>A related problem with key informants is that their expertise may lead them to view a problem in ways that are quite different from how others view the problem. Give a child a hammer, and the child begins to act as though everything needs to be hammered upon. Teachers view education as the answer to social problems, lawyers view legal services as a primary unmet need, and trucking company executives are not likely to view railroads as a solution to shipping needs. Having expertise in a particular form of service makes it likely that informants would readily think of ways of meeting unmet needs that are related to  their  expertise.  A  premature focus on any one alternative leads to overestimating the likelihood that the alternative is the correct one to relieve unmet needs (see Posavac, Sanbonmatsu, &amp; Ho, 2002). This is a reason why planners should consider several alternatives before choosing a course of action.</p>

<h3 id="focus-groups-and-open-forums">Focus Groups and Open Forums</h3>

<p>In addition to the option of administering needs assessment surveys to representative samples of community residents, two approaches can be used to encourage residents to share their opinions with others.</p>

<p><em>Focus Groups</em></p>

<p>Marketing research firms use small informal groups to discuss new products and ideas for new products (Malhotra, 2010). Although the kinds of questions addressed to focus groups are similar to the open-ended questions used in interviews, a discussion among 7–10 group members prompts participants to reflect more thoughtfully about the reasons for their opinions than they would if interviewed alone. The focus group leader tries to remain in the background to encourage a relaxed atmosphere and to foster sharing of opinions among members; however, the leaders must be sure participants keep on the topic. Focus groups usually consist of people who are similar to each other in age, income level, and education. Since the goal is to foster a free exchange of opinions, great differences among members could lead some members to guard their comments. Although similar to each other, the members of a focus group should not  know  each  other  prior to the meeting and, most likely, will not see each other afterward. Some incentive for attendance is usually given since invited participants must travel to the meeting location and spend, perhaps, two hours in the discussions. Focus groups can be used in any phase of an evaluation; however, they serve particularly well in learning about the reactions to potential services or changes in current services of community agencies or private organizations. Krueger and Casey (2000) described using focus groups to discover why agricultural extension classes were drawing fewer and fewer enrollments among farmers in Minnesota even though a written survey suggested that farmers were interested in such classes. It could have been assumed that financial troubles were responsible, but focus groups told a different story. Farmers said that they were interested and that they could afford the fees; however, they wanted to be sure that the instructors were knowledgeable and that the material was applicable to their farms. In addition, it was learned that personal invitations would be more effective than the printed flyers that had been the standard way of announcing the classes. After changes had been</p>

<p>implemented, attendance increased markedly.</p>

<p>The questions addressed to focus groups tend to be open-ended questions. <a href="#bookmark1">Figure 6.2</a> includes some questions that could have been used in focus groups of rural business people. Plans were being made to   develop an assistance program for small rural businesses as part of a state-funded project to encourage rural economic development. Extension classes for owners and managers of rural businesses were being considered, but before such classes could be developed, colleges needed to identify the information needs of business owners. These questions were designed to reveal those needs. See Case Study 8 for an example of the use of focus groups and some of the areas of questions used.</p>

<p><em>Figure 6.2</em> Focus group questions to learn about the information needs of owners of rural businesses. Note that discussion among the members occurs after each question. (Adapted from Krueger, 1994.)</p>

<p>Failing to seek the reactions to planned programs may lead to offering services that are not wanted. An English department faculty member developed a series of workshops for non-English faculty members in her college to show how they could help students improve their writing. She obtained funds from the dean, but failed to assess whether faculty would attend the workshops. At the first workshop there were more graduate student program evaluators present than faculty participants. If a focus group of faculty members had been presented with the plans, she might have learned that few faculty members were interested in changing how they taught their classes or in attending these workshops.</p>

<p><em>Open Forums</em></p>

<p>An open forum is another method based on group interaction. Unlike focus groups that are selected on the    basis of some common characteristics, participants of an open forum are self-selected. Often a governmental agency will announce the date of a community meeting to consider some planning issue. In many cases there  are legal requirements to have such meetings. Those who learn of the meeting and choose to attend can participate. Sometimes participants’ comments are limited to perhaps three minutes.</p>

<p>The advantages of open forums include unrestricted access for anyone who wishes to participate, the very low cost, and the possibility that good ideas may be offered. One major disadvantage is that self-selected  groups are not representative of a community because many people cannot or will not attend such meetings. Once when a town held an open meeting to discuss how to spend a small federal grant, only one resident attended; he urged the development of a children’s park along a railroad right-of-way, a location the planners viewed as dangerous. A less obvious disadvantage is the possibility that public discussions may raise expectations that something can and will be done to meet the needs discussed. It is also possible that assertive individuals with personal grievances can turn the meetings into gripe sessions producing little information on the needs of a community (Mitra, 1994).</p>

<p>Users of focus groups or open forums need to be cautious in drawing conclusions from the views of specific individuals. The needs of individuals can be quite compelling to casual viewers of TV news programs; in fact, even people trained as researchers have been known to put aside the findings of carefully conducted research and adopt conclusions based on the personal, compelling experiences of one or two articulate individuals    rather than more valid, but abstract, information (Rook, 1987).</p>

<h2 id="inadequate-assessment-of-need">INADEQUATE ASSESSMENT OF NEED</h2>

<p>The reason to assess needs is to improve the quality of program planning. Without understanding needs, programs cannot be designed appropriately nor can program objectives be developed wisely. When objectives are not specified clearly, it is impossible to learn if the program achieved what was wanted. When needs and the context of needs are not assessed accurately, programs and services cannot be as efficient or effective as possible. Furthermore, providing programs designed to meet needs that are already being met is not a good way to spend resources. The most common problems are as follows: sometimes needs are not measured, the context is not understood, the type of implementation required is not considered, and the denial of need is not considered. In contrast, a thorough assessment of the most obvious needs often uncovers other, sometimes more important needs (Lepicki &amp; Boggs, 2014; Wedman, 2014).</p>

<h3 id="failing-to-examine-need">Failing to Examine Need</h3>

<p>It might seem a truism to say that program designers should be sure that proposed programs are planned on the basis of the needs of the people to whom the services are directed. It should be unnecessary to make this point, but sometimes programs are implemented without careful attention to unmet needs. Administrators at a college were concerned about the high attrition rate of freshmen who withdrew from college even though they were in good academic standing. As tentative ideas about possible program changes were being considered, it became apparent that the focus was on the desire to retain students, not on meeting the needs of the  students  who  withdrew.  Administrators  had  only  speculated  about  the  students’  unmet  needs  and  how program changes might prompt more students to reenroll for the sophomore year. Perhaps the attrition rate was being caused by personal and family situations that the college could not influence, or perhaps the students developed interests in areas of study in which the college did not offer courses. Attempts to lower the attrition rate among such students were bound to fail because no retention program could meet such unmet needs.</p>

<p>Another example of failure to examine need occurred in 1993 when it was proposed that the U.S. federal government provide free immunization vaccines for all children under 18. This was proposed because it was believed that cost was a barrier to immunization. However, when poor children—the least immunized group</p>

<p>—are covered by Medicaid, immunizations are free already. According to the GAO (Chan, 1994, 1995), the reasons some poor children go without immunizations include parental disinterest and physician lack of initiative in immunizing children when the children saw their physicians for other needs. The planned, and nearly implemented, program would not have led to higher immunization rates because the reasons why some children did not obtain immunizations had been assumed, not identified empirically.</p>

<h3 id="failing-to-examine-the-context-of-need">Failing to Examine the Context of Need</h3>

<p>It  is  quite  possible  to  have  an  accurate  understanding  of  a  community’s  need,  but  to  fail  to  assess  the community’s  capacity  to  support  the  program  or  the  cultural  context  in  which  a  program  would  be implemented (McKnight, 1995). One of the tragedies of the relationship between the developed nations and the undeveloped nations during the last several decades is the repeated failure of foreign aid initiatives to reflect the economic and cultural context of the nations being aided. Many well-intentioned efforts have been misdirected. At best, such efforts do no harm beyond wasting resources; at times, such efforts have left many in the recipient nations in worse conditions (Devarajan, Dollar, &amp; Holmgren, 2000). In the area of health care, a basic problem is assuming that the residents of developing nations need advanced medical services. Unfortunately, developing nations cannot support advanced technologies, and the foreign aid available can provide this type of care to only a small minority of the population.</p>

<p>Bringing foreign nationals to European or North American medical schools for training means that such people will become accustomed to technology-intensive medical practices. Partially as a result of this training, some physicians have returned to their home countries to practice in city hospitals serving only the rich; in recent years 90% of the medical resources of Nigeria have been spent in cities where only a minority of the population lives (Manoff, 1985). This happens while many rural children go without immunizations that are inexpensive. Some have said that much foreign aid has actually harmed the recipient nations (Easterly, 2007).</p>

<p>A favorable outcome occurred when planners examined treatment for severe diarrhea, which can lead to    the death of children in poor areas if the fluids and nutrients lost are not replaced. Such fluids can be replaced using intravenous feeding; however, the cost would be staggering for an undeveloped nation. Premixed packs   of salt, sugars, and other ingredients were developed to make it easier to prepare a drink for affected children. Widespread introduction of such premixed packages, although costing only a fraction of intravenous methods, would have been unfortunate because the packs were still too expensive to supply everyone in need and their  use would imply that the people could not take care of themselves. Manoff (1985) reported that planners calculated that 750 million packs would have been needed in 1990. A better approach was found: locally available ingredients were used in recipes that could be taught to residents. Health care programs that provide medically appropriate skills are far more effective than ones that meet the needs of a few but foster feelings of helplessness among many others.</p>

<h3 id="failing-to-relate-need-to-implementation-plans">Failing to Relate Need to Implementation Plans</h3>

<p>Sometimes a program is prepared on the basis of a reasonable understanding of unmet needs, but the implementation makes it difficult for people to participate. Food stamps are designed to assist poor families to purchase food at reduced costs; it is a compassionate response to nutrition needs. Unfortunately, through local indifference and target group ignorance only 20% of those eligible for food stamps were using them in the early years of the program. An advertising campaign in New Mexico tripled the number of eligible people using food stamps in six months (Manoff, 1985). In 1997 only half of eligible households participated in the food stamp program (Castner &amp; Cody, 1999). Some health promotion programs fail to consider all aspects of effective marketing (Winett, 1995).</p>

<h3 id="failing-to-deal-with-ignorance-of-need">Failing to Deal with Ignorance of Need</h3>

<p>If a group in need does not recognize the need, a program must include an educational effort (Cagle &amp; Banks, 1986; Conner, Jacobi, Altman, &amp; Aslanian, 1985). People screening for and treating high blood pressure deal daily with patients who do not recognize a need for care. When the condition first develops, elevated blood pressure seldom produces noticeable symptoms; in addition, medication can have undesirable side effects (Testa, Anderson, Nackley, &amp; Hollenberg, 1993). Because patients expect to feel better, not worse, after    taking medications, many reject the treatment. A rejected treatment means program failure, even though the medication is effective in reducing the likelihood of strokes and other cardiovascular diseases.</p>

<p>Some people believe that when it comes to good health practices and the availability of educational opportunities, society fulfills its responsibility by simply providing the facts. They argue that publicly funded service providers are under no obligation to persuade anyone to take advantage of the programs. By contrast, social marketing techniques (Weinreich, 1999) are used to foster the adoption of more adaptive health behaviors, wise use of government services, and the maintenance of safer environments. Social marketing is needed not only to provide basic education, but to strengthen motivation. Such efforts are needed to counter    the extensive advertising that encourages the use of products that can harm health, such as very sweet and high-fat foods, cigarettes, and alcohol. The city of Cleveland launched a social marketing effort beginning in 2008 to encourage children to value education and to counter “gangsta rap posters” and other negative influences (Perkins, 2008).</p>

<h3 id="using-needs-assessments-in-program-planning">Using Needs Assessments in Program Planning</h3>

<p>Once the level of need has been assessed for a particular population, then program planning may begin. Planners seek to develop a service or intervention to help the population achieve or approach a satisfactory state. The probability of developing a successful program increases markedly when the important stakeholder groups are involved in the planning and cooperate in selecting both the services to be offered and the mechanisms for delivering them. Stakeholders can be involved through formal representation on committees or in focus groups responding to developing plans.</p>

<p>After outcome goals have been specified, the next step is to consider the intermediate goals that need to be achieved on the way to achieving outcome goals. Intermediate goals could include, for example, accomplishments, behaviors, attitudes, or levels of knowledge. In order to specify intermediate goals that   would indicate how much the target population is being helped, planners must describe the program theory. Unfortunately, as mentioned in Chapter 2, planners sometimes develop programs without asking what must happen to the program participants before they can reach the outcomes hoped for; in other words, an impact model ought to be developed as the program is planned.</p>

<p>After developing an impact model, the planning group can specify what actions the agency must take to implement the program. The resources that need to be considered include the knowledge and skill of the staff, the physical resources, financial resources, advertising, and outreach efforts. Since planners cannot operate   with blank-check assumptions, the projected resources required are compared with the financial support that would be available. If the resources are not sufficient, the planners have several options. One temptation is to leave the goals intact while reducing the intensity of the program and hope for the best. If the planning had   been sound and if the conceptual basis of the plans is valid, such a decision is a prescription for failure (Rivlin, 1990). To fail in the complex task of helping people change is not a disgrace, but to continue to plan a    program that planners suspect cannot work well because it is too weak is a waste of valuable resources and fosters cynicism about social programs. More useful alternatives include reducing the number of people to be served, narrowing the focus of the service, or even changing the plans to meet a different need, one that   requires fewer resources.</p>

<p>Although this presentation seems straightforward and linear, the planning committee will frequently check back to verify that they have been faithful to the mission of the organization that will sponsor the program. In fact, while developing intermediate goals, planners might discover that some of the outcome goals were unrealistic. What does not change is the need to work in a generally backward direction during the planning process, beginning with what is to be accomplished and ending with a plan to achieve those goals (see Egan, 1988a,b). If there are no clear goals, rational planning cannot occur. Surprisingly often, people in agencies and organizations sense a need, but do not measure it carefully or even explore it in a qualitative manner (Mathison, 1994). Sometimes they then watch helplessly as the program fails to have an observable impact on the population in need. Altschuld and Witkin (2000) discuss how evaluators can help in translating a needs assessment into a program, but the actual planning is not a primary responsibility of evaluators.</p>

<h4 id="case-study-6">CASE STUDY 6</h4>

<p>Improving Systems to Address Needs</p>

<p><strong>Tony L. Sheppard, PsyD, CGP, FAGPA</strong> was involved in the follow-up phase of a program evaluation project while on his predoctoral internship with a large community mental health agency in the Midwest in 2001–2002, building on the initial phase started by the previous predoctoral internship class. The community mental health center, recognized for its substance abuse programs as well as for those treating chronic mental illness, wanted to better understand how their services addressed the comorbidities among these presenting problems. As a part of the needs assessment phase of the study, it was discovered that the diagnosis of psychiatric conditions in their addiction treatment programs and the diagnosis of substance abuse disorders in their psychiatric programs were both well below the state and national averages. By interviewing staff, reviewing medical record data, and considering agency forms and paperwork, it appeared that the staff in the psychiatric programs were not screening for substance abuse adequately and that the staff in the substance abuse programs were not screening adequately for psychiatric conditions. Further, the paperwork used in the respective programs did not facilitate such screening.</p>

<p>The initial evaluation recommended cross training. Staff from the substance abuse programs would train those in the psychiatric programs in screening for and diagnosing addictions. Likewise, the staff in the psychiatric programs would train those in the addiction programs in screening for and diagnosing psychiatric conditions. Further, staff from both programs would collaborate on ensuring that paperwork facilitated this process. In some cases, this was as simple as including substance use screening questions and psychiatric symptom checklists on intake forms.</p>

<p>Meeting Needs</p>

<p>Although many needs assessments are conducted before a program exists, often to determine whether or not to start the program, this case illustrates the value of needs assessment in an established program. As noted, the agency intended to serve a substantial number of people with comorbid substance abuse and other chronic mental illness, knowing that many of those with both problems seek services for one or the other, but not both. Despite this good intention, various factors interfered with their goals, especially clearly identifying the clients with comorbidities. This case illustrates that the various elements of program evaluation are much more integrated than separate, as understanding the actual condition of the clients who sought their services involved also understanding the actual practices of the agency (implementation) in detail.</p>

<p>Implementation</p>

<p>This case highlights an important point for evaluators—challenges within programs  and  the  corresponding  opportunities  for  improvement do not always or even generally result from avoidance or resistance in implementation. The agency and the staff specifically wanted to identify clients who would benefit from the coordinated treatment of both substance abuse and other psychiatric disorders. The      fact that substantially fewer clients than expected were recognized as having comorbidities  was  identified  by  careful  examination  of multiple sources of information and the discrepancy between agency diagnoses and external norms. Although not explicitly stated in the evaluation, it is very likely that a more engaging style of evaluation made it possible to discover the problem, whereas evaluators with a       style that staff found punitive or threatening might well never have learned the essential details.</p>

<p>Stakeholders</p>

<p>Clearly the clients and staff were important stakeholders. The clients of this community mental health center were often from underprivileged populations. Their awareness of comorbidity and therefore, their ability to advocate for themselves were generally lower than in more privileged groups. The staff and that agency as a whole are also important stakeholders. This agency is consistently recognized as a regional leader in innovative and effective treatment. It is vital to the reputation of the organization to maintain the highest standards. Further, the agency prides itself on providing its staff the tools necessary to perform their jobs proficiently.</p>

<p>Side Effects</p>

<p>Although not explicitly noted as a side effect, the factors that prevented initial determination of comorbidity were clearly a negative unintended consequence of the way the program was originally conducted. No positive side effects were noted.</p>

<p>Improvement Focus</p>

<p>As implied in the Implementation section, the genuine interest of the evaluators to help the program not simply improve but to meet their desired goals was an obvious component. Not to overemphasize the point, but evaluators who assume good intentions on the part of program stakeholders generally develop greater trust, better cooperation, and more useful findings. Further, good evaluators also know that having their own good intentions is most helpfully matched by communicating those good intentions consistently to stakeholders.</p>

<p>Outcomes</p>

<p>Following the cross-training and paperwork revision processes, agency records were reviewed again and staff were interviewed regarding their perceived proficiency in assessing for the given symptoms. Not only did staff report improved proficiency in the areas addressed, the data suggested that comorbidities were more in line with the state and national numbers. This program evaluation project was written up and shared agency-wide as an example of quality improvement. This was used to build morale in the agency, but also to encourage the rural clinics in the organization to embrace these findings and, likewise, adopt the new forms and procedures.</p>

<p>Nuances/Mechanisms</p>

<p>As noted previously, this project was shared agency-wide as an example of quality improvement. This particular agency prides itself on this type of dedication to its employees and clients. This process was shared with the rural clinics that often have the same clinicians providing both services. The paperwork revisions were shared with the rural clinics enabling them to better screen for comorbid conditions.</p>

<p>Summary</p>

<p>Involvement in this project inspired Dr. Sheppard to remain involved in ongoing program monitoring and improvement. It demonstrates a commitment to continual monitoring of one’s processes to ensure that the best possible services are offered. It also emphasizes the importance of accurate diagnosis in the provision of services in a large health system.</p>

<p><em>Source:</em> T. L. Sheppard (personal communication, January 16, 2018).</p>

<h2 id="summary-and-preview-2">SUMMARY AND PREVIEW</h2>

<p>The importance of assessing needs before beginning to plan programs cannot be overstated. The definition of need emphasized that evaluators try to learn what people need in order to be in a satisfactory state, in their social context. Although we tend to know a lot about our own needs, we often do not fully understand what it takes to satisfy those needs, as illustrated by marginal understanding of objective nutritional needs. Since educational, psychological, and social needs are very complex, it would not be surprising to learn that these needs are not fully understood either. Consequently, it is wise to seek information on needs from a variety of sources. Community surveys, agencies currently serving people, social indicators, focus groups, and expert informants are sources of information on needs. Once needs are clear, planning may begin. Planning has the best chance of success if the conceptual basis of the program is developed and used to set the outcome and intermediate goals.</p>

<p>After programs are in place, evaluators continue to contribute since programs must be monitored to guarantee their integrity. Furthermore, information gathered by monitoring serves to show what aspects of a program should be adjusted. Chapter 7 includes an introduction to management information systems and their use in program evaluation.</p>

<h2 id="study-questions-2">STUDY QUESTIONS</h2>

<ol>
  <li>Examine the daily newspapers for articles on social services or consumer products that show  how  a human service or a new product was affected by an analysis of need. Note that if the example concerns a consumer product, the definition of unmet need in this chapter may not be appropriate; in such cases, manufacturers are usually dealing with meeting wishes or preferences, not serving unmet needs.</li>
  <li>Look for examples of agencies or producers who failed because they did not conduct an appropriate needs assessment before offering a service or bringing a product to the marketplace. What kind of information might the organization have sought to avoid the error?</li>
  <li>Many colleges sponsor a learning assistance program for students who do not perform as well as they wish in their classes. Sketch a plan to do an assessment of needs for such a program. Assume that there is   a program already, but the college dean wants to know how well the services match the needs and   whether services should be expanded. Consider the information you want and the sources as well as the methodology to use in gathering the information.</li>
  <li>Under what conditions would an assessment of needs be threatening to an organization?</li>
  <li>Thinking of unmet needs sometimes clarifies planning. Sometimes we hear that a community needs something, a recreation center, for example. What unmet needs might underlie the assertion that a town needs a recreation center? What would you measure to test that idea? What are the alternative ways of meeting the unmet need you thought of?</li>
</ol>

<h2 id="additional-resource-2">ADDITIONAL RESOURCE</h2>

<p>Altschuld, J. W. (Ed.) (2010). <em>The needs assessment kit</em>. Thousand Oaks, CA: Sage.</p>

<p>This is a set of five books addressing needs assessment. The five are: Book 1, <em>Needs Assessment: An Overview</em>; Book 2, <em>Needs Assessment Phase I: Getting Started</em>; Book 3, <em>Needs Assessment Phase II: Collecting Data</em>; Book 4, <em>Needs Assessment: Analysis and Prioritization</em>; and Book 5, <em>Needs Assessment Phase III: Taking Action for Change</em>. It is a comprehensive guide that provides more information than many evaluations will need, but    most evaluators will eventually run into situations or questions that are addressed well in this series.</p>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
