<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chapter 9 Is the Intervention Effective | 同志社大学社会学研究科　陳凌雲</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Chapter 9 Is the Intervention Effective" />
<meta name="author" content="JAMES R. DUDLEY" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PART V" />
<meta property="og:description" content="PART V" />
<link rel="canonical" href="https://mrlyn20.github.io/Lyn/2023/08/09/Social-Work-Evaluation-Chapter-9.html" />
<meta property="og:url" content="https://mrlyn20.github.io/Lyn/2023/08/09/Social-Work-Evaluation-Chapter-9.html" />
<meta property="og:site_name" content="同志社大学社会学研究科　陳凌雲" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-08-09T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Chapter 9 Is the Intervention Effective" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"JAMES R. DUDLEY"},"dateModified":"2023-08-09T00:00:00+09:00","datePublished":"2023-08-09T00:00:00+09:00","description":"PART V","headline":"Chapter 9 Is the Intervention Effective","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrlyn20.github.io/Lyn/2023/08/09/Social-Work-Evaluation-Chapter-9.html"},"url":"https://mrlyn20.github.io/Lyn/2023/08/09/Social-Work-Evaluation-Chapter-9.html"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/Lyn/assets/css/style.css?v=97b06ba2048c98509e06ce5ad3ed82faa3ff7a7f">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/Lyn/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://mrlyn20.github.io/Lyn/">同志社大学社会学研究科　陳凌雲</a></h1>

        

        <p>地域福祉とソーシャルワーク評価について学びます。</p>

        
        <p class="view"><a href="https://github.com/MrLyn20/Lyn">View the Project on GitHub <small>MrLyn20/Lyn</small></a></p>
        

        

        
      </header>
      <section>

      <small>9 August 2023</small>
<h1>Chapter 9 Is the Intervention Effective</h1>

<p class="view">by JAMES R. DUDLEY</p>

<p><strong>PART V</strong></p>

<p>The Outcome Stage</p>

<p>Chapter 9, the only chapter in this section, introduces several types of evaluations used during the outcome stage of a program or practice intervention. The overall purpose of outcome evaluations is to determine whether the intervention resulted in positive outcomes for the clients receiving it. Client outcomes are also discussed in the context of whether there is enough evidence of a causal relationship existing between an intervention and the client outcomes.</p>

<h1 id="chapter-9-is-the-intervention-effective">CHAPTER 9 Is the Intervention Effective?</h1>

<p><em>A central question to ask at this stage is, how do we know that our intervention actually helps clients?</em></p>

<p>This chapter addresses this outcome question. To many people, the evaluation of programs and practice interventions mostly means evaluating whether the intervention resulted in positive outcomes for clients. Outcome questions have a familiar ring to them: Did the clients reach their goals? Did they make significant progress? Is there evidence that the program made a difference in their functioning? Although outcome studies are presented as only one of three general types of evaluations in this text (i.e., planning studies, implementation studies, and outcome studies), it must be said that they are ultimately the most important type. If progress on clients’ goals is not evident after the completion of a program or practice intervention, a provider will have real difficulty justifying its continuation. This is the case no matter how well the program was planned, how qualified the staff members are who provide services, and even if the program’s quality is superior. This is also true for practice interventions. If practice interventions do not lead to substantial gains for clients in the intended areas, then the intervention needs major modifications or should be discontinued and replaced with another approach.</p>

<p>This chapter describes two critical ingredients in outcome evaluations: a clearly defined intervention and measurable client outcomes. Several types of evaluation designs are used in outcome evaluations and are presented in the following discussion. The advantages and limitations of each design and what each design can claim are described, with examples of outcome evaluations given to help illustrate the relevant concepts. (See Figure 9.1.)</p>

<p><img src="https://i.imgur.com/mQWgJfT.png" alt="Imgur" /></p>

<p>Based on the logic model, outcomes for clients are directly linked back to several activities completed in the planning stage. During that stage, clients’ problems, needs, and the causes of their problems were identified and assessed. Then goals and measurable objectives were crafted to measure what the clients’ situations would look like when these problems were resolved. Developing an intervention came next, created specifically to help clients reach these goals. Now in the outcome stage after the intervention is implemented, we focus on measuring the clients’ goals to deter-mine if they have been achieved.</p>

<h2 id="the-nature-of-outcomes">THE NATURE OF OUTCOMES</h2>

<p>Program and practice outcomes can focus on many things. Outcome measures may be identified for staff members’ performance, material resources, recruiting of more clients, or other concerns. Ultimately, however, the outcomes for clients are the focus of human service outcome evaluations. Outcomes are the entities that help us determine whether our interventions were effective in helping our clients. Chapter 7 explained how to identify, describe, and measure goals as outcomes. This chapter continues that discussion with planning how to implement outcome evaluations.</p>

<h3 id="outcomes-are-not-outputs">Outcomes Are Not Outputs</h3>

<p>The outcomes of clients, which are the ultimate interest of a program or practice evaluation, have several distinct characteristics. Unfortunately, many agencies focus almost exclusively on outputs rather than outcomes in their performance reports made available to the public. For example, an agency report may focus on the number of staff hours of counseling provided, the number of clients who use the services, or the costs of a specific program.</p>

<blockquote>
  <p><strong>Excerpts From an Annual Report of a County Government</strong></p>

  <p>The following statistics are from the annual report of the Cabarrus County (2007) government:</p>

  <ul>
    <li>322,000 people visited county-operated parks.</li>
    <li>14,706 people participated in athletics, competitions, and other recreational events held in county parks.</li>
    <li>7,801 calls and personal contacts came into the Veterans’ Services office.</li>
    <li>976,939 miles were driven by county vans.</li>
    <li>81,847 rides were provided to disabled and/or elderly residents by the county transportation service.</li>
    <li>35,335 individuals visited the Department of Social Services for services or assistance.</li>
    <li>5,085 families in a crisis received assistance through the Department of Social Services.</li>
    <li>3,885 children were protected from abuse, neglect, or dependency through provision of child protective services.</li>
  </ul>
</blockquote>

<p>Note that all these indicators are outputs, even though some may be misrepresented as outcomes. The last output in the example, “3,885 children were protected from abuse, neglect, or dependency,” is incorrectly stated and should be rephrased “3,885 children were provided with child protective services.” Based on a follow-up inquiry, the agency reported that it did not have any evidence to conclude that the children were protected. If that information had been available, it would have been an outcome.</p>

<h3 id="outcomes-ultimately-focus-on-the-client-not-staff-interventions">Outcomes Ultimately Focus on the Client Not Staff Interventions</h3>

<p>A frequent problem for many who are new at crafting outcomes for clients is confusing client outcomes with outcomes for the staff that are helping them. For example, consider the statement “The client will be referred to X agency for assistance whenever the program cannot help her.” Note that this statement refers more to what a staff member will do (referral activities) than to what the client does. Many stakeholders may think that the goals of the staff members are outcomes when it is more accurate to say that staff activities are the interventions that assist clients in achieving their goals or outcomes.</p>

<p>Articulating and documenting outcomes for staff activities can be helpful, especially because client outcomes usually depend on the success of staff in reaching their goals. Outcomes for staff members can be viewed as an appropriate evaluation focus at the implementation stage, such as when we ask whether staff members use evidenced-based practice interventions or provide quality services. Yet, these outcomes are not client outcomes.</p>

<blockquote>
  <p><strong>Quiz: Are These Outcomes for Clients or Staff Members?</strong></p>

  <p>Try to pick out which of the following goals are for clients and which are for the staff members.</p>

  <ol>
    <li>The clients will be transported to the grocery store in the agency van every Monday.</li>
    <li>The clients will purchase groceries for the evening meal.</li>
    <li>The client will have  access  to  a  planned  meal  written  out  on  a  piece  of paper.</li>
    <li>The client will pay for the groceries using the money available for shopping.</li>
    <li>The clients will have enough money to buy the groceries.</li>
  </ol>
</blockquote>

<p>The correct answers to the outcomes in the preceding quiz are that #2 and #4 focus on clients, and the others focus on staff members. The fifth outcome could be intended either for the clients or for staff, depending on who has control over the funds for shopping; unfortunately, it is presented in an ambiguous way. Overall, the five examples show how dependent client outcomes are on staff outcomes.</p>

<h3 id="outcomes-are-logically-linked-to-the-interventions">Outcomes Are Logically Linked to the Interventions</h3>

<p>As has been mentioned several times before, it is important that the outcome measures be directly linked to the intervention based on the logic model. This should be the case both conceptually and practically. Often, having an easily and well understood intervention helps in crafting realistic outcome measures. An example of this important issue is illustrated with the community organization, First Things First, mentioned in Chapter 7. This community organization wanted to reduce the number of divorces in their community; this was their initial outcome measure. Their intervention was to offer a premarital weekend retreat format at several different houses of worship to prepare recently married couples with help in staying together. The retreat curriculum included five major components: communication between the couple, having realistic expectations of a partner, finances, health issues, and sexuality and intimacy. The sponsoring organization recruited speakers to lead the retreat who had expertise in each topical area. The evaluator devised a pretest–posttest measure of outcomes for the retreat based on what could be realistically expected from participation in each retreat component. Specific questions were devised based on the specific objectives of each speaker. For example, the topic of having realistic expectations of your partner included questions such as, “Do you know what your spouse expects of you?” “Do you and your partner know how to resolve conflicts when they arise?” and “Do you know how to meet your partner’s needs?” The community organization hoped that the program would have an impact, however small, on the divorce rate. However, they needed help in considering a less global outcome, such as that the program participants, not the larger community, would be staying together, particularly during the challenging years of early marriage. The organization came to realize that the community’s divorce rate had multiple causes, many of which were well beyond its influence and ability to change.</p>

<h3 id="outcomes-can-be-determined-at-different-points-in-time">Outcomes Can Be Determined at Different Points in Time</h3>

<p>The point in time to measure a client outcome is an important factor to consider. For example, outcomes can be measured when a client completes a program or at some future time. Outcomes can be short-term (e.g., when a client completes the program), intermediate (e.g., three months later), or long-term (e.g., twelve months later). The time selected and the specific outcome expected depend on the amount of time realistically needed for the program to have such an impact.</p>

<blockquote>
  <p><strong>Example of Outcomes at Different Points in Time</strong></p>

  <p>A crisis center for transient youths decided to use short-, intermediate-, and long-term outcomes for the youths they sheltered. Their outcome measures were as follows:</p>

  <p><em>Short term</em>: reduction in runaway behavior, physically aggressive behavior, and serious physically aggressive threats while residing at the crisis center.</p>

  <p><em>Intermediate term</em>: maintaining residence in a safe, least restrictive environment for two months after discharge and increased family access to community services.</p>

  <p><em>Long term</em>: maintain residence in a safe, least restrictive environment for six months after discharge, increased school attendance, and reduction in involvement with the juvenile justice system.</p>
</blockquote>

<p>Unfortunately, most evaluations are set up to measure client outcomes only when clients complete the program or otherwise terminate from the agency. This is an important time to measure outcomes, as it provides a measure of how much change occurred while the client was in the program. Yet this single short-term outcome measure usually reveals little, if anything, about the long-term effects of a program. It could even be considered a client output, not a client outcome. Inpatient programs for drug-addicted clients, for example, are often criticized for not providing a long-lasting impact on recovery. Clients who leave a program may be drug free, but the true test of their success is likely determined over time, once they return to their community and reconnect with their social supports, both previous and new. Unfortunately, many substance abuse programs do not conduct a follow-up evaluation to determine whether their services led to long-term recovery or a return to addiction and a lifestyle that supports addiction.</p>

<h2 id="varied-ways-to-measure-outcomes">VARIED WAYS TO MEASURE OUTCOMES</h2>

<p>Martin and Kettner (1996) identify what they perceive as four kinds of outcome or performance measures: numeric counts, standardized measures, scales of level of client functioning, and client satisfaction indicators. <em>Numeric counts</em> are generally used to measure client conditions, status, and behaviors. Examples include attendance at school, frequency of drinking alcohol, and numbers of violent and other antisocial behaviors. <em>Standardized measures</em> usually measure such things as client feelings, attitudes, and perceptions. Examples of such scales include scales on self-esteem, clinical anxiety, hope, and happiness. <em>Level-of-functioning scales</em> measure client and family functioning. Examples include scales of adaptive behavior, employment behavior, wellness, and social integration. Martin and Kettner (1996) also refer to client satisfaction as an outcome; however, they view client perceptions of the services received as only an intermediate outcome, not a final one.</p>

<blockquote>
  <p><strong>Example: Child Welfare Outcomes Documented by Children’s Bureau</strong></p>

  <p>The U.S. Department of Health and Human Services’ Children’s Bureau (2017) includes child welfare outcomes as part of its annual report to Congress. The report also provides data on the states’ performance in meeting the needs of children and families who are in contact with the child welfare system. Federal data-reporting systems are used to gather data on seven outcomes: reduced recurrence of child abuse and neglect, reduced child abuse and neglect in foster care, increased permanency for children in foster care, reduced time in foster care before reunification, reduced time in foster care before adoption, increased placement stability, and reduced placement of young children in group homes and institutions. The executive report and additional information can be found on https://www.acf.hhs.gov/cb/resource/cwo-10-14</p>
  <h3 id="practical-measures-of-outcomes">Practical Measures of Outcomes</h3>
</blockquote>

<p>Ideally, virtually every agency has identified client outcomes that they are helping clients accomplish. Because the outcomes are the ultimate reason for the agency’s existence, it makes sense to always keep them in the forefront of the practitioners’ work, their conversations with clients, their referrals, and their education of the public. One agency, for example, does just that. Community Choices Inc. (2006) helps pregnant women addicted to alcohol and/or drugs by offering group treatment. The outcomes of this group treatment intervention are evident in much of its material, including the brochure handed out to prospective clients and referral agencies. Outcomes that they highlight for clients are becoming drug free; maintaining or regaining custody of children; having a healthy, drug-free baby; becoming self-sufficient; finding a job; finding permanent housing; improving parenting skills; and feeling better about themselves. The open and direct communication about the outcomes has many benefits, including helping the women identify what they want or need, helping them select this agency because of its desired outcomes, and communicating the agency’s purpose and intentions to the public.</p>

<blockquote>
  <p><strong>Outcome Indicators Used for Clients in a Domestic Violence Agency</strong></p>

  <p>The following client outcome indicators are from White (2005):</p>

  <ul>
    <li>Lower homicide rate (chosen by the county government)</li>
    <li>Perceived degree of safety</li>
    <li>Knowledge of a safety plan that could be used in an emergency</li>
    <li>Acknowledgment of a broad definition of domestic violence</li>
    <li>Favorable responses to client satisfaction questions</li>
    <li>Referral contact rate (when client follows through and contacts another agency he or she is referred to)</li>
    <li>Recidivism rate (returning to agency a second time or more)</li>
  </ul>
</blockquote>

<p>Litzelfelner (2001) gives another example of a need for practical outcomes when the overall goal of a program is not measurable. She points out that the goal of child protective services to protect children from abuse is not usually measurable. Abuse and re-abuse of children are often undiscovered or unreported. She offers what she views as practical alternative measures in a set of four general areas related to child abuse: child safety, child permanency, preserving families, and child health and development. For each area, she identifies several measurable objectives and activities to reach the objectives. For example, for child safety, she suggests two objectives as examples: (a) 90 percent of children served will not re-enter the system within two years following their initial discharge (based on reports of the agency’s home visit and case management services), and (b) each quarter, 85 percent of caregivers participating in the parent education program will demonstrate three nonphysical behavior management techniques during the agency’s home visits. As an exercise, please identify the weaknesses in this approach to measuring child abuse. Then develop an alternative approach that could be another way to measure child abuse.</p>

<p>Outcomes can also be measured by qualitative methods. One student who was working with a group of emotionally disturbed children in a public school decided to use journaling to measure progress on their personal goals related to self-esteem (McLean, 2007). She encouraged her group members to keep an ongoing journal of their impressions discussed in the group. They were given ten minutes at the end of each session to write about their impressions. During most sessions, members were asked to journal on the topics that had been discussed such as, What are the benefits of self-esteem? or how can mistakes be opportunities for growth? Journaling during the first and last sessions was used to evaluate their progress in working on their personal goals. During the first session, members were asked to journal on their goals and expectations for the group. The topic for them to focus on was, what do I want to get out of this group? The final session was used to journal on the topic: How has the group changed my self-esteem? Quantitative ratings were also used to evaluate their progress on goals during the last session.</p>

<h2 id="criteria-for-choosing-outcome-measures">CRITERIA FOR CHOOSING OUTCOME MEASURES</h2>

<p>What determines whether an outcome measure is adequate or acceptable? Martin and Kettner (1996) offer seven criteria: the outcome measure’s usefulness, validity, reliability, precision, feasibility, cost, and unit costs.</p>

<h3 id="usefulness">Usefulness</h3>

<p>First, the measure needs to be useful and relevant to stakeholders. This is a commonsense criterion that raises an important question: How will the outcome measure be useful? Will it offer helpful information to stakeholders to support the program or assist them in improving, expanding, or even reducing it?</p>

<h3 id="validity">Validity</h3>

<p>Is the outcome measure valid? Validity is one of the two psychometric standards for determining whether a measure is satisfactory. Validity refers to whether the outcome measure reflects what it is supposed to measure and whether it does so accurately. Accuracy is the key issue. The more valid that a measure is, the better it is. For example, an outcome measure for father involvement should include several different functions or activities, not just one. Some will involve direct contact with their child (e.g., how many direct contact hours in an average week, quality of the specific activities), and other functions may involve no contact but direct responsibilities (e.g., attending a parent–teacher conference, arranging a dental appointment for the child). From this example, it is clear how complex outcomes like father involvement are and the enormous challenges they pose for creating accurate measures. Father involvement requires at the very least a certain amount of contact. The quality of that contact, whether direct contact or indirect responsibilities, is also important to consider.</p>

<h3 id="reliability">Reliability</h3>

<p>The third criterion for choosing an outcome measure is reliability. Reliability is a second key psychometric standard in determining whether a measure is satisfactory. Reliability refers to the internal consistency of the measure. It asks two questions: whether the measure of the outcome consistently measures it from one time to another and whether the measure is consistently applied among different people (e.g., do different clients consistently understand the measure in the same way?). Reliability is weak if the questions asked in a measurement instrument are ambiguous, with words likely to be understood and interpreted differently at different times or by different respondents. For example, satisfaction with a program can be interpreted in many ways.</p>

<h3 id="precision">Precision</h3>

<p>Precision is the fourth criterion. Precision refers to how well the outcome measure captures the incremental changes in clients’ condition, either quantitatively or qualitatively. Quantitative measures are at the interval, ordinal, or nominal levels. Qualitative measures are at the prenominal level. For example, incremental quantitative measures are the number of times truant children attend school (interval level) and their average letter grade (ordinal). An example of an incremental qualitative measure is the subtle variations in how a parent uses nonviolent strategies in disciplining their children.</p>

<h3 id="feasibility">Feasibility</h3>

<p>A fifth criterion in deciding on outcome measures is their feasibility. Is the measure viewed as practical and acceptable to all stakeholders? Some may be opposed to a measure because it is at odds with their cultural values or ethics. For example, a measure of assertiveness needs to consider the ways that various cultural groups view confrontation with authority. Feasibility may also be a problem if it is likely to demand a lot of the attention and time of staff members in explaining or administering it.</p>

<h3 id="cost">Cost</h3>

<p>The cost of an outcome measure is another criterion. Does the measure require some major start-up or maintenance costs? Another possible cost factor is the purchase of any standardized scales owned by other people or organizations.</p>

<h3 id="unit-cost-reporting">Unit Cost Reporting</h3>

<p>Unit cost reporting is the final criterion. Can the measure be described as a cost per unit of outcome? For example, the cost of a client graduating from a program could be used as a cost per unit of outcome. Numeric counts lend themselves to this type of cost more than other measures, such as standardized scales.</p>

<h2 id="outcomes-and-program-costs">OUTCOMES AND PROGRAM COSTS</h2>

<p>Sometimes outcome studies are conducted that focus directly on costs. Some refer to these evaluations as cost–benefit or cost-effectiveness evaluations. Cost-related evaluations are increasingly important to consider, given the diminishing funds for human service programs and the increasing call for financial accountability. One ethical dilemma with cost–benefit studies is that it is too difficult to directly translate the benefits of a program, such as reduced child abuse or treating and managing severe mental illness, into financial savings. Although immediate financial savings are evident in such things as fewer health care visits and avoiding legal costs in a court case, the benefits of protecting a child from harm go well beyond material factors and may have implications for the life of the child and for the child’s offspring. In brief, although fiscal matters have obvious value, their value is limited and should be subordinate to the value of basic human needs.</p>

<p>Cost-effectiveness evaluations, like cost–benefit studies, may have more utility in human service programs than cost-benefit studies. In this case, the client outcomes resulting from a program can be analyzed according to cost. The cost, for example, of obtaining adoptions, on average, for special needs children could be considered in terms of costs. The costs could then be compared to the costs of such an outcome in the previous year, or two different programs could be compared based on the costs incurred in completing a successful adoption.</p>

<p>It is complicated but possible to calculate costs in a human services program. For example, one could categorize costs into direct (e.g., staff salary) and indirect (e.g., secretarial salary, telephone bills). Costs could also be divided into fixed (e.g., building maintenance) and variable (e.g., salaries), and recurring (e.g., rent) and nonrecurring (e.g., equipment). Determining the costs of providing services to an average client must take all these costs into account, which sometimes becomes a real challenge to figure out. For example, in the 1970s, with the initiation of deinstitutionalization of people with mental retardation, some legislators argued in favor of continuing institutional services rather than establishing a community service system because they viewed institutional services as having lower per capita costs. This debate led to the discovery that it was easier to calculate the per capita costs of community-based services because they were discrete (e.g., residential, vocational, recreational). In contrast, the cost of institutional services, encumbered by extensive capital costs such as maintenance of buildings and grounds, was more difficult to calculate accurately.</p>

<h2 id="evidence-based-interventions">EVIDENCE-BASED INTERVENTIONS</h2>

<p>Once the outcome measures have been carefully selected for an evaluation, another challenging task is to demonstrate that the measures are logically and empirically linked to the intervention that will be used. Interventions that are empirically linked to important outcome measures are considered evidence-based interventions. This raises a critical point about evidence-based interventions—they are interventions that have, to some degree, a demonstrated a causal relationship to specific client outcomes.</p>

<p>Evidence-based interventions were defined in Chapter 2 as integrating individual practice expertise, the best-available external evidence from practice research, and the values and expectations of clients (Gambrill, 1999, p. 346). This definition suggests that evidence-based should not be either defined simply by one criterion or based solely on one source. Instead, the interventions should have multiple factors in common. One such factor, a necessary one, is that evidence-based interventions have the best evidence available that they are effective in helping clients, and strong evidence is available to support the claim in the measures of client outcomes. Thus, we can say that evidence-based interventions should be grounded one way or another in outcome evaluations.</p>

<p>O’Hare (2015) describes four important tools for determining evidence-based practices: they use critical-thinking skills, obtain scientifically credible evidence, seek multiple sources of evidence whenever possible, and seek diversity-sensitive evidence.</p>

<h3 id="critical-thinking-skills">Critical Thinking Skills</h3>

<p>Critical-thinking skills are used to determine whether an intervention is evidence-based. Critical thinkers are natural skeptics about how well an evaluation is conducted, whether someone else’s evaluation or one’s own. Critical thinkers are also skeptical about whether their interventions actually work. As mentioned in Chapter 1, Gibbs and Gambrill (1996) identify several types of problems that evaluators experience when they fail to be critical thinkers, including the selection of weak or inappropriate interventions.</p>

<h3 id="scientifically-credible-evidence">Scientifically Credible Evidence</h3>

<p>Evidence-based interventions must have documentation that is scientifically credible. This means, in part, that there is scientific evidence of a causal relationship between the intervention and clients’ outcomes. Such evidence for programs is most likely to come from group designs, especially experimental and quasi-experimental designs, which are known to be superior to other evaluation strategies (Cook &amp; Campbell, 1979). Evidence for practice interventions is most likely to come from a version of a single-system design, especially a design that controls for as many extraneous influences as possible.</p>

<p>Scientifically credible evidence and critical thinking share many common values. Both approaches involve skepticism. Both carefully distinguish between questions of fact and questions of value. Both use caution when inferring what may have caused improvement in the client outcomes and when to generalize to other client groups, and both emphasize the need for measurement of clients’ needs, interventions, and outcomes. Finally, both perspectives are likely to continually ask, does this intervention work?</p>

<h3 id="seeking-multiple-sources-of-evidence">Seeking Multiple Sources of Evidence</h3>

<p>Triangulation, a process of using multiple methods to measure something, is also important to evidence-based interventions. Evaluators who obtain two or more measures of an outcome for comparison purposes triangulate the findings. The process also adds confidence that the two measures are valid because of their similarity. The more sources available that document that the intervention has a causal relationship to client outcomes, the better. An example is to find out the frequency of a nonresidential father’s contact with his children in an average week by seeking a report from two or more sources (e.g., the father, the children’s mother, the father’s current partner).</p>

<p>Criterion-related validity is another way to triangulate the findings of an evaluation. Criterion-related validity is a means of determining whether the scores of an outcome measure are valid by correlating the scores with those of another measure of the same construct (Babbie, 2016). The other measures of the same construct are external criteria. For example, an external criterion could be a behavioral indicator that correlates with a verbal response of a client to an interview question. If a short-term outcome measure of an evaluation is regular attendance at a drug treatment program, the evaluator could compare the client’s report of his or her attendance with a report of attendance obtained from the drug treatment agency. In another example, the outcome measure could be a depression scale developed by an agency, and the scores could be correlated with the scores of the same clients using a well established measure like the Beck Depression Scale (Beck, Steer, &amp; Brown, 1996).</p>

<h3 id="evidence-is-diversity-sensitive">Evidence Is Diversity Sensitive</h3>

<p>If there is evidence that a program or practice intervention is effective with one group of people, it is extremely important not to automatically conclude that it will be effective with other groups. This is a fallacy that has resulted from the misuse of previous findings, especially evidence that if a specific approach helps white, middle-class men, then it can be generalized to being effective in helping people of other races and social classes as well as white women. Evidence must be obtained skeptically for each group before arrival at such a conclusion. The issue of generalizing the results of a group design to other people is referred to as the extent that the group design has external validity.</p>

<p>Thyer and Meyers (2003) recommend that the most realistic way to generalize the results of a group design to other client groups is to use the process of replication. Replication involves repeating a group design with different client groups and at various times. If the results of a group design reveal that an intervention works for one client group, the intervention could be tried with a different client group that has similar characteristics. If the same result occurs, a claim that the intervention is effective would be somewhat strengthened. This could be followed by further replication, such as with similar clients in another agency or by other evaluators with similar clients in other geographic areas. Also, if the results of the group design indicate that an intervention is effective with a group of men, the intervention could be tried with a group of women. Or if an intervention works for young-adult clients, it could be tested with middle-aged adults. Repeated replication of a group design can be extremely valuable in exploring the extent to which an intervention can be generalized to others. Such replication can also be used to determine the limitations of an intervention and the characteristics of clients with whom it may not work and with client circumstances that are contraindicated.</p>

<blockquote>
  <p><strong>Example of Setting Standards for Evidence-Based Practice</strong></p>

  <p>Harding and Higginson (2003), in a systematic review of articles from relevant professional journals, found twenty-two interventions for helping cancer and palliative care patients and their caregivers. They analyzed the twenty-two interventions based on the type of patient and caregiver populations, the evaluation design used, and findings. They graded the evaluations as follows:</p>

  <ul>
    <li>Grade I (strong evidence): randomized control trials</li>
    <li>Grade II (fairly strong evidence): group designs with a comparison group</li>
    <li>Grade III (weaker evidence): retrospective or observational studies</li>
    <li>Grade IV (weak evidence): cross-sectional studies, Delphi exercises, consensus of experts</li>
  </ul>

  <p>They concluded that there was a lack of outcome evaluation designs used to evaluate the effectiveness of the interventions. Instead, most articles contributed more to understanding feasibility and acceptability issues than to effectiveness. The authors encouraged programs to conduct more experimental and quasi-experimental designs to determine whether interventions effectively helped cancer patients and their caregivers.</p>
</blockquote>

<p>Decisions on the sources of evidence for social work evaluations are important. Evidence can come from the reports and experiences of administrators, staff members, and faculty; clients’ records; observations that document evidence; interviews and questionnaires that document perceptions of evidence; case examples of clients; other types of recordings; and, most important, scientific studies. Sources will vary in relevance and importance depending on their purposes. Evidence-based interventions that are described next are concerned with interventions in which there is scientifically credible evidence of a causal relationship between the intervention and clients’ outcomes.</p>

<h2 id="determining-a-causal-relationship">DETERMINING A CAUSAL RELATIONSHIP</h2>

<p>How do we know whether the program was the cause of the progress in client outcomes? The answer to this question is complicated. Three conditions must be met before a causal relationship can be claimed. <em>The first condition</em> is that the program (or practice) intervention must precede clients’ improved outcomes. This condition makes logical sense: a provider could not ethically claim responsibility for clients’ progress if the progress occurred prior to the introduction of the program.</p>

<p><em>The second condition</em> needed to claim a causal relationship is that an association is found between the introduction of the intervention and the client’s improvement in his or her outcome measures. An association means that the client’s improvement becomes evident after the intervention is implemented. To be able to determine whether improvement occurs in the outcome measure during the period of implementation, we need to obtain a measure of the outcome both before (i.e., pretest measure, or baseline measure) and after (i.e., posttest measure) the intervention is introduced.</p>

<p>An association between the intervention and clients’ outcomes can be determined through one of two methods: a test of statistical significance or an established standard of clinical significance. <em>Statistical significance</em> can be calculated by applying a statistical test, which determines whether the probability of an association is high enough to claim significance. Usually, a probability of 95 percent is expected, with the likelihood of error at 5 percent.</p>

<p><em>Clinical significance</em> is defined by clinical criteria established by a professional group or agency. In this case, the amount of improvement in client outcomes is satisfactory when it meets the established clinical criteria. Clinical significance may be preferred over statistical significance in many circumstances, as it is based on clinical judgment, not mathematical probability. However, statistical significance is usually expected in summative evaluations when evidence of a program’s impact is expected to be based on scientific principles.</p>

<p>To illustrate these points, consider the example of a recovery group of clients with alcohol abuse problems. The group has as its desired outcome reduced alcohol consumption. Assume that the recovery group is a weekly treatment group that emphasizes cognitive behavioral therapy and a modified version of the twelve-step program of Alcoholics Anonymous. The blood-alcohol level of group members is the measure used to determine whether there is improvement. If statistical significance is the standard to be used, then there must be a statistically significant difference between pretest and posttest scores of the average blood-alcohol level of group members. A statistical test, such as a <em>t</em>-test, can tell us whether the two measures were significantly different according to probability theory. The various types of <em>t</em> -tests and other statistical tests that can be used to determine statistical significance are discussed in the next chapter.</p>

<p>Let’s determine whether group members improved significantly according to clinical criteria. Let’s assume that the agency sponsoring the group intervention has a clinical standard based on the definition of satisfactory improvement reported in the professional literature. The agency has found studies that have reported that an intervention is not fully successful until a person totally withdraws from alcohol. Therefore, the agency decided that clinical significance would not be fully achieved until the group members totally withdrew from drinking; in other words, the members would not show enough improvement if they fell short of this high standard.</p>

<p><em>The third condition</em> for claiming the intervention is the cause of the client’s improvements is the most difficult to achieve. This condition is the claim that the improvement occurring in the clients’ outcome measure is due, at least in part, to the program intervention. We need to consider that many other factors, such as a change in family circumstances or joining an Alcoholics Anonymous support group, can have an influence on the outcome as well. Usually, an intervention is not expected to be totally responsible for clients’ improvement. If we are interested in determining how much of the improvement is due to the intervention, we would have to use an experimental design. A less dependable method is a multiple regression test, which is discussed in the next chapter.</p>

<p>Using the example of the recovery group mentioned earlier in this section, an experimental design could be used that includes an intervention group (i.e., clients randomly selected and assigned to the recovery group) and a control group (i.e., another group randomly selected and assigned to receive traditional individual services). If the outcome scores of the intervention group members (their average blood-alcohol level) positively changed significantly from pretest to posttest and the outcome measure of the control group members changed less or not at all, then it can be claimed that the intervention provided to clients had a causal influence. The amount of influence that it had on the recovery group members is determined in two steps. First, the change in the recovery group members’ outcome score from the pretest to posttest is calculated as the amount of change that occurred in that group. This change would then need to be reduced by the amount of change, if any, that occurred in the outcome measures for the control group.</p>

<p>Quasi-experimental and pre-experimental designs can also be used to explore whether a social work intervention has a causal influence on changes in client outcomes. In such cases, only tentative conclusions can be made about the intervention’s influence or impact. However, in most cases, an experimental design cannot be realistically used for ethical reasons, as field-based evaluations are not amenable to the controls that an experimental study requires. Quasi-experimental, pre-experimental, and experimental group designs are all reviewed next for program interventions.</p>

<h2 id="group-designs-for-programs">GROUP DESIGNS FOR PROGRAMS</h2>

<p>Several group research designs are available for conducting outcome evaluations of programs. These designs are known as group designs because they measure the impact of an  intervention on  a  group of clients, not on one client. Two  overall  categories  of  designs  are  possible in an evaluation study: cross-sectional and longitudinal. A cross-sectional design collects data from participants at one time only, and a longitudinal design collects data from research participants at two or more times. Longitudinal designs provide the most powerful evidence of a causal relationship between a program intervention and client outcomes.</p>

<p>Internal and external validity are important standards to use when evaluating the strength of a group design. Internal validity addresses whether the intervention and no other factors are responsible for improvement in the client outcome variable. Several extraneous variables could be responsible for improved outcome in addition to or instead of the intervention. For internal validity to be strong, for example, factors independent of the intervention present in clients’ lives (e.g., receiving services from another professional agency, joining a self-help group) could be responsible for improvement in the outcome measure in addition to or instead of the intervention.</p>

<p>External validity addresses the issue of generalizing the results of a group design to other people. In this case, if the participants in the group design were randomly selected from a large population (e.g., a client group of one hundred or more), generalizations to the larger group are possible. However, if participants selected for the group design were drawn from a smaller population or were selected using nonprobability sampling, generalizations are not advisable.</p>

<p>Experimental designs, quasi-experimental designs, and some pre-experimental designs are longitudinal designs that can be used to explore a causal relationship between a program and its impact on client outcomes. Several group designs are described in Campbell and Stanley (1973). Six designs are highlighted here because they are among the most practical ones for outcome evaluations of programs in human service agencies:</p>

<blockquote>
  <p>Pre-Experimental Designs</p>

  <ol>
    <li>One-group posttest-only design</li>
    <li>One-group pretest–posttest design Quasi-Experimental Designs</li>
    <li>One-group pretest–posttest design with a comparison group</li>
    <li>Time-series design Experimental Designs</li>
    <li>One-group pretest–posttest design with a control group</li>
    <li>Posttest-only control group design</li>
  </ol>
</blockquote>

<p>All these group designs can be used to explore the impact of a program on a group of clients, whether the intervention focuses on individual, family, small group, or community systems. All these designs require that there be a clearly defined program intervention that can be replicated and at least one client outcome measure. These are the central elements of any group design. Other elements are included in some of these designs and not others.</p>

<blockquote>
  <p><strong>Concepts Relevant to Group Designs</strong></p>

  <ol>
    <li><em>Clients’ outcomes</em>: outcome measures of what clients are to achieve</li>
    <li><em>Program intervention</em>: an intervention defined clearly enough to be replicated</li>
    <li><em>Pretest or baseline measure</em>: a measure of an outcome for clients prior to the intervention</li>
    <li><em>Posttest measure</em>: a measure of an outcome for clients after implementation of the intervention</li>
    <li><em>Intervention group</em>: group of clients receiving the program intervention.</li>
    <li><em>Comparison group</em>: a group of clients in a group design similar but not identical to the group receiving the program intervention; this group does not receive the program intervention</li>
    <li><em>Control group</em>: a group of clients in a group design considered identical to the group receiving the program intervention, based on either random selection and assignment or matching; this group does not receive the program intervention</li>
  </ol>
</blockquote>

<p>Let’s look at each of the six group designs and the concepts incorporated into each one. Diagrams of each design are also included in Table 9.1.</p>

<p><strong>Table 9.1. Pre-Experimental, Quasi-Experimental, and Experimental Designs</strong></p>

<table>
  <thead>
    <tr>
      <th>DIAGRAM</th>
      <th>DESCRIPTION</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>X O<sub>1</sub></td>
      <td>1. One-group posttest-only design: intervention provided to intervention group followed by a posttest measure of client outcome</td>
    </tr>
    <tr>
      <td>O<sub>1</sub> X O<sub>2</sub></td>
      <td>2. One-group pretest/posttest design: intervention provided to intervention group preceded by a pretest measure and followed by a posttest measure of client outcome</td>
    </tr>
    <tr>
      <td>O<sub>1</sub> X O <sub>2</sub> / O<sub>3</sub> O<sub>4</sub></td>
      <td>3. Pretest/posttest design with a comparison group: intervention provided to the intervention group preceded by a pretest measure and followed by a posttest measure of client outcome; also, a comparison group that does not receive the intervention preceded by a pretest measure and followed by a posttest measure of client outcome</td>
    </tr>
    <tr>
      <td>O<sub>1</sub> O<sub>2</sub> O<sub>3</sub> O<sub>4</sub></td>
      <td>4. Time-series design: multiple measures of client outcome before and after the intervention provided to the intervention group</td>
    </tr>
    <tr>
      <td>R-O<sub>1</sub> X O<sub>2</sub> / R-O<sub>3</sub> O<sub>4</sub></td>
      <td>5. Pretest/posttest design with a control group: intervention provided to the intervention group preceded by a pretest measure and followed by a posttest measure of client outcome; also, a control group that does not receive the intervention preceded by a pretest measure and followed by a posttest measure of client outcome. The R indicates that the two groups were randomly selected from a larger group and randomly assigned to either the intervention or control group. Therefore, they can be viewed as identical groups of people.</td>
    </tr>
    <tr>
      <td>R-X O<sub>1</sub> / R- O<sub>2</sub></td>
      <td>6. Posttest-only control group design: intervention provided to the intervention group followed by a posttest measure of client outcome; also, a control group that does not receive the intervention followed by a posttest measure of client outcome. The R indicates that the two groups were randomly selected from a larger group and randomly assigned to either the intervention or control group. Thus they are viewed as identical groups of people.</td>
    </tr>
  </tbody>
</table>

<p>Notes: X symbolizes the program intervention, O represents a measure of the client outcome, and R indicates that the two groups were randomly selected from a larger group and randomly assigned to either the intervention or control group.</p>

<h3 id="one-group-posttest-only-design">One-Group Posttest-Only Design</h3>

<p>Pre-experimental designs are often important to consider initially because they are easy to implement and take less time than stronger designs. This design involves collecting data at only one point in time after implementation of the intervention to the intervention group. Thus, this is a cross-sectional rather than a longitudinal design. During time 1 the intervention is introduced, and during time 2 the posttest measure of client outcome occurs.</p>

<p>This design is a good one for a beginning exploration of program impact, and it can simply answer the question of whether it is possible that the program had an impact. A one-group posttest-only design has several weaknesses. Most important, there is no pretest measure to determine whether any improvement occurred in the outcome variable during the time of the intervention. Furthermore, none of the extraneous influences (e.g., environmental changes) are controlled for. An important conclusion derived from this design if the client outcome measure is not positive or favorable is that the intervention does not work.</p>

<h3 id="one-group-pretestposttest-design">One-Group Pretest–Posttest Design</h3>

<p>The pretest–posttest design builds on the previous design by adding a pretest measure before the initiation of the intervention. This design, like the first one, is pre-experimental. However, it is longitudinal because data are collected from the clients at two different times—before and after the introduction of an intervention. The design is helpful in revealing how much improvement occurred between the pretest and the posttest. During time 1, the pretest score of the outcome variable is measured, during time 2 the intervention is introduced, and during time 3 the posttest score of the outcome variable is measured.</p>

<p>An example of a pretest–posttest design was described earlier in this chapter. The community organization that sponsored the premarital weekend retreat to reduce the number of divorces in the community was essentially interested in determining whether participants made progress between the time that the retreat began and ended. The pretest–posttest design served that purpose and helped them further refine the curriculum they offered.</p>

<p>With this design, we can only determine whether improvement occurred during the time that the program was introduced, not whether the program was necessarily responsible for any change in the client outcome measure. The most important weakness of this design is that it does not have a comparison or control group to control for extraneous influences.</p>

<blockquote>
  <p><strong>Example of an Evaluation with a One-Group Pretest–Posttest Design</strong></p>

  <p>The purpose of an evaluation conducted by Lynch (2007) was to explore the impact of an independent-living skills-training program on the resilience, social support, and life skills of sixteen ethnically diverse foster-care teenagers. The teenagers’ scores on standardized measures of resilience, social support, and life skills were compared before and after they participated in the training program. The improvements in scores from pretest to posttest were found to be statistically significant for social support but not for resilience and life skills.</p>
</blockquote>

<h3 id="pretestposttest-design-with-a-comparison-group">Pretest–Posttest Design with a Comparison Group</h3>

<p>The pretest–posttest design with a comparison group builds on the two previous designs. It is longitudinal in nature, as data are collected of the client outcome measure both as a pretest and as a posttest. However, another group similar to the first group is organized as a comparison group that does not receive the intervention. This design compares the progress of both groups during the intervention period. As Table 9.1 indicates, the design is diagrammed with two different groups: one that receives the program intervention and the other that does not. At time 1 a pretest score of the outcome variable is measured for both groups, and during time 2 the intervention is introduced for the group assigned to receive it and nothing happens for the comparison group. The comparison group could receive services that are usually available as an alternative to nothing. At time 3, a posttest score of the outcome variable is measured for both groups.</p>

<blockquote>
  <p><strong>Example of Pretest–Posttest Comparison Group Design</strong></p>

  <p>Engagement services during the intake period were introduced to female clients in a substance abuse treatment program as a motivational strategy to draw them into treatment (Comfort, Loverro, &amp; Kaltenbach, 2000). An evaluation was conducted to determine whether the engagement services resulted in an increased use of treatment services. The evaluation used a pretest–posttest design with a comparison group. The engagement services involved an engagement specialist offering van transportation to the agency and child care during intake interviews and making telephone calls to remind clients of intake appointments. Although transportation and child care were always offered to the group, members infrequently used child care. Nevertheless, the results revealed that clients receiving engagement services were more likely than those in the comparison group to use treatment services.</p>
</blockquote>

<p>A comparison group can be drawn from a larger pool of clients, such as those on a waiting list or clients who are receiving traditional services during the time of the study. This type of design can reveal with more confidence than the previous ones whether the program had a causal influence on client outcomes. The most important weakness of the design, however, is that the clients selected to be in the program and those selected for the comparison groups are likely to have many differences that the design does not control for. Such differences could have a considerable influence over the measures of client outcomes. This design also creates a potential ethical problem that needs to be addressed, as one could convincingly argue that the clients in the comparison group should receive the program intervention as well. They may, for example, have as great a need for the intervention as does the group receiving the intervention.</p>

<p>A comparison group should be as similar as possible to the intervention group, particularly with variables or characteristics important to a study. For example, both groups should have similar demographic characteristics (e.g., age, gender, race, ethnicity), and both groups should be similar in terms of the important variables of the evaluation, such as their mental illness diagnosis or the problems that the program addresses. Comparison group studies should report on these characteristics and indicate when the two groups are significantly different according to specific characteristics. This reporting helps the reader consider whether any of the differences in characteristics could have influenced their respective client outcome scores.</p>

<h3 id="time-series-design">Time-Series Design</h3>

<p>A time-series design is another type of quasi-experimental design. This design has many of the features of the previous designs and is also different from the other designs in that it has several pretest and posttest outcome measures, not one of each. The design involves obtaining several client outcome measures before the introduction of an intervention and several additional measures after the intervention has been implemented. Table 9.1 diagrams a time-series design.</p>

<p>A major advantage of a time-series design is that it overcomes some of the ethical problems of the designs that use comparison and control groups, as it has neither. An important feature of the multiple pretest and posttest measures of this design are the data trends that can help determine the extent to which the intervention, as opposed to other factors external to it, is the causal agent. Stated another way, its multiple measures of client outcomes provide more opportunities to determine whether extraneous influences exist.</p>

<h3 id="pretestposttest-design-with-a-control-group">Pretest–Posttest Design with a Control Group</h3>

<p>The fifth design, a pretest–posttest design with a control group, is the classical experimental design. It is the most powerful design for determining causality. It is similar to the pretest–posttest design with a comparison group with one exception. Clients are first randomly selected from a larger pool of prospective clients and then randomly assigned to either the intervention or the control group. Because of the two steps of random selection and assignment, the two groups of clients can be treated as statistically identical because the random selection and assignment tend to balance out the extraneous differences between the two groups. With this design, pretest and posttest measures are obtained from both groups before and after the intervention group receives the intervention.</p>

<p>This design is diagrammed in Table 9.1. Prior to time 1, clients are randomly selected and assigned to either receive the services of the program or to a control group. During time 1, a pretest score of the outcome variable is measured for both groups. During time 2, the intervention is introduced for the group assigned to receive it, and either nothing happens for the control group or it could continue to receive existing agency services. During time 3, a posttest measure of the client outcome variable is obtained for both groups.</p>

<blockquote>
  <p><strong>Example of a Pretest–Posttest Control Group Design</strong></p>

  <p>Ciffone (2007) evaluated whether a curriculum-based suicide prevention program was instrumental in changing unwanted attitudes and in reducing students’ reluctance to seek mental health treatment among groups of students in two demographically diverse high schools. He used a pretest–posttest control group design. Students were randomly assigned to treatment and control groups by the classes that they were assigned. Those in the treatment group were provided a three-day presentation by the school social workers who spoke in their health classes. Those in the control classes had no outside speakers in their health classes during the same period. The pretest and posttest instrument consisted of eight questions.  The attitudes toward suicide were measured partially by the item, “Most teens who killed themselves were probably suffering from mental illness.” Students’ responses to the item in the intervention group changed from “no” to “yes” when their attitudes improved. Two other items measured help-seeking attitudes or reluctance to seek mental health treatment: “If suicidal thoughts crossed my mind regularly, I would seek out and talk to a friend about these thoughts” and “If I was very upset and suicidal thoughts crossed my mind, I would be willing to talk with    a professional counselor about these thoughts.” Higher percentages of students     in the intervention group were favorable to these statements in the posttest than those in the control group.</p>
</blockquote>

<p>This design is quite powerful in determining whether the program intervention influences the outcome measure for the client group in the program because it controls for all extraneous influences. Nevertheless, the design poses even more serious ethical challenges than any of the other designs. It can be viewed as unethical to randomly select and assign clients to receive or not receive an intervention, particularly without their consent. Most likely, if agency resources can provide a program intervention for all clients, the decision to select some clients should be made based on professional criteria, not mathematical probability. This dilemma could be worked out by randomly selecting clients from a waiting list and then randomly assigning one group to receive the intervention and the other group to continue to wait for services offered later. However, it would be important to explain this arrangement to clients in an informed consent protocol. Also, other unanticipated consequences must be considered, such as exposing the vulnerabilities of clients to life-threatening crises in the control group during a waiting period.</p>

<h3 id="posttest-only-control-group-design">Posttest-Only Control-Group Design</h3>

<p>The sixth design, a posttest-only design with a control group, is the same as the experimental design except that it does not have a pretest for either group. The design depends completely on the initial random selection and assignment steps to ensure that participants in the two groups are identical, which is particularly important because there are no pretest measures to determine whether the two groups begin essentially at the same point in their outcome measures.</p>

<p>This design is preferred over the pretest–posttest design with a control group if there is concern that administering a pretest measure could influence the outcome measure. For example, a test about a knowledge area used as a pretest measure may also be used as a posttest measure to determine how much the clients learned during the intervention phase. An intervention could be safe-sex training, and the outcome measure could be a test that measures how well clients understand safe sexual practices. The problem here is that the test could be discussed informally among clients during the intervention phase to determine the correct answers and the clients could then report these answers in the posttest phase. In this case, testing factors rather than actual learning from the intervention could be responsible for any improvement in the test from pretest to posttest.</p>

<p>In other instances, this design could also be preferred over the pretest–posttest design with a control group if the pretest seemed unnecessary or undesirable. It may be undesirable or not logistically possible to implement, or an evaluator may find it too costly to administer a pretest measure because it would involve time and effort of the staff members to interview clients or administer a questionnaire as a pretest.</p>

<p>Table 9.1 diagrams the posttest-only control-group design. Prior to time 1, clients are randomly selected and are either assigned to receive the program’s services or assigned to a control group. During time 1, a pretest score of the outcome variable is not measured for either group. During time 2, the intervention is introduced for the group assigned to receive it, and either nothing happens for the control group or it continues to receive existing agency services. During time 3, a posttest measure of the client outcome variable is obtained for both groups.</p>

<p>This design, like the classic experimental design, is quite powerful in determining whether the program intervention influences the outcome measure for the client group in the program because it controls for all extraneous influences. However, this design also poses serious ethical challenges for the same reasons that it does for the classic experimental design. The clients could be randomly selected from a large waiting list and then randomly assigned either to receive the intervention or to wait for services to be offered later. However, it would be important to explain this arrangement to clients in an informed consent protocol, and a plan to address unanticipated consequences must be considered, such as the control-group clients’ vulnerability to a life-threatening crisis during the waiting period.</p>

<p>It is important to note that all group designs have another major limitation if they only measure client outcomes immediately after clients complete a program. Such outcome measures, as mentioned earlier, are usually only short-term outcomes. Therefore, programs are encouraged to consider one or more additional outcome measures at later points in time (e.g., three and six months later) to determine whether clients’ progress from participating in a program is sustained over time.</p>

<h2 id="outcome-evaluations-for-practice">OUTCOME EVALUATIONS FOR PRACTICE</h2>

<p>The previously described outcome evaluations are generally used in program evaluations. Practice evaluations tend to use different designs. Many of these designs are described next.</p>

<h3 id="client-satisfaction-and-effectiveness">Client Satisfaction and Effectiveness</h3>

<p>It is important that practitioners hold themselves accountable for the effectiveness of their interventions even when others may not do so. This is an ethical obligation mandated by the NASW’s (2017) Code of Ethics (see Chapter 3). The code points out that practitioners are to “monitor and evaluate policies, the implementation of programs, and (our) practice interventions” (section 5.02[a]).</p>

<p>There are some basic questions that practitioners can periodically ask clients, such as the following:</p>

<ul>
  <li>Are my interventions helping you? In what ways?</li>
  <li>Are my interventions helping you in the areas in which you are seeking help (e.g., related to the presenting problems, identified needs, solutions desired, expressed hopes)?</li>
  <li>Are my interventions helping you meet your goals identified in our initial work together?</li>
  <li>How would you describe your progress so far in achieving your goals? How have my interventions played a part in helping you or in hindering you?</li>
</ul>

<p>The specifics of how clients communicate that the practitioner’s interventions have helped (or have not helped) can be especially important. For example, did they simply respond to the practitioner’s questions or did they also spontaneously offer additional comments? Spontaneous comments may be more authentic and more likely to be valid indicators of their views.</p>

<p>These types of questions are considered client satisfaction questions, but they are also quite relevant to the question of effectiveness; they focus on client outcomes and their connection to the social worker’s intervention. They ask did the intervention have a role in bringing about any positive changes to clients’ goals or outcomes? In some instances, clients’ responses to these questions may be the only available information. In this regard, they have limitations, especially because they are subjective and not necessarily reflective of actual outcomes. For example, clients could report accomplishments that did not actually happen (e.g., discontinued drug use) or overestimate what happened (e.g., reporting completion of homework every day, when it may be sporadic). That is why objective measures of outcomes are also extremely important to obtain, such as checking whether drug use is evident in a client’s urine or obtaining an independent report of the client’s drug use from a spouse or another reliable source.</p>

<p>As discussed in Chapter 8, however, it can be argued that clients’ reports on their outcomes and how the interventions have affected them are a necessary but not sufficient indication of intervention effectiveness. Conversely, obtaining an objective measure of a client’s outcomes without some concurrence from the client can be viewed as overlooking the importance of the clients’ confirmation of their change.</p>

<h1 id="single-system-designs">Single-System Designs</h1>

<p>Practice evaluations of outcomes can be conducted in several ways, and many of them revolve around variations of a single-system design. A single-system design is a variation of a time-series design on a practice level in that it involves several measures of the outcome variable both before and while an intervention is being introduced. A single-system design is a practice evaluation tool that measures whether there is a causal relationship between the practitioner’s intervention and a client’s outcome measure. This design is typically used with one client system at a time whether it is an individual, family, group, or community. The single-system design uses a graph as a tool to visualize the clients’ progress, and it is intended to be used by both the client and the social worker in determining whether any progress has been made. The graph helps simplify the explanation for how much progress has been made and it does not require a statistical test for explanation, although one can be used.</p>

<p>An single-system design graph consists of two or more phases. A baseline phase (phase A) is almost always needed as well as one or more intervention phases (e.g., phases B, C, and D). The client outcome measure is plotted along the vertical axis, and a timeline for recording measures of the outcome measure is identified along the horizontal axis (e.g., daily, weekly). Figure 9.2 illustrates an ABC design with three phases: a baseline phase, a phase when the first intervention is introduced, and a phase when the second intervention is introduced.</p>

<p><img src="https://i.imgur.com/FXqCWF7.png" alt="Imgur" /></p>

<p>A single-system design can be conducted using a variety of designs, including AB, ABC, ABAB, and BAB. The simplest design is an AB design, which consists of a baseline phase and one intervention phase. In this case, the client outcome measure is obtained and plotted along a horizontal, standardized timeline, such as weekly or daily intervals during both phase A (baseline) and phase B (intervention phase). For example, a woman with clinical depression wants to develop new friendships but is resistant to getting out and meeting people. The social worker attempts to help her by providing supportive services, as support has been missing in the client’s life and is thought to be what she needs to begin attending social occasions in her church and local community center. The frequency of attending these social occasions each week was the client outcome indicator, measured every week while the client was in contact with the social worker’s agency. The social worker’s intervention, introduced during phase B, is the support service. This AB design requires measurement of the frequency of the client’s attendance weekly for a few weeks to obtain a baseline measure that is stable or does not fluctuate highly. This outcome indicator continues to be measured weekly during phase B, when the social worker’s intervention is implemented. The intent of the AB design is to discover whether the client attends more social occasions after the intervention is introduced than she did before.</p>

<p>Another single-system design is the ABC design, illustrated in Figure 9.2. This design begins with a baseline phase (phase A), followed by the first intervention phase (phase B), and then followed by another intervention phase (phase C). The ABC design is particularly useful when the first intervention does not have sufficient impact by itself. A second intervention can then be introduced in phase C to either replace or supplement the first intervention. Often the combination of the two interventions can have a significant impact when neither intervention has much impact by itself.</p>

<p>Using the same example of the woman with severe depression, providing support services weekly could be the first intervention to encourage more social contacts (during phase B). If this intervention does not succeed, teaching the client conversation skills could be added as a second intervention (during phase C). The ABC design can plot the client’s attendance at social occasions during phases A, B, and C to determine whether attendance increases during implementation of the first intervention and later adding the second intervention. In Figure 9.2, combining the two interventions, support services and teaching conversation skills, visibly documents greater social contact for this client.</p>

<p>Another variation is the ABAB design, which consists of a baseline phase followed by an intervention phase, then another baseline phase when the intervention is removed, and finally the reintroduction of the intervention phase (see Figure 9.3). This design has a special additional feature of withdrawing the intervention after phase B to find out whether other factors outside the agency (e.g., a self-help group, a new friend, joining a new church or synagogue) may be responsible for any continuation of client progress evident in phase B. If progress on the client goal declines during the reintroduction of phase A, this provides further evidence that the intervention and improvement are linked. Finally, the intervention can be reintroduced in the second phase B to find out whether progress reappears.</p>

<p><img src="https://i.imgur.com/IXrcGdN.png" alt="Imgur" /></p>

<p>Although the ABAB design can, in some ways, be an alternative to having a comparison or control group, it can also pose ethical problems if the intervention is withdrawn while the client still needs it. Similarly, it would also be unethical to withdraw the intervention if a client wishes to continue it or if withdrawal could cause harm. However, temporary withdrawal of the intervention may be less of an ethical concern if the social worker or client became unavailable because of a vacation or lengthy sickness. Withdrawal may also be less of a concern if the intervention is mandated (involuntary) and if the client wants a temporary break from services.</p>

<p>An ABAB design has another limitation in that it works only when the outcome measure can be reversed. If a client is taught a new set of behaviors, once he or she has learned the behaviors, they may not be reversed in many cases. For example, if a parent is taught new assertiveness skills in phase B, withdrawal of the intervention is not likely to lead to an immediate reversal of the newly developed skills.</p>

<p>Sometimes a BAB design is relevant if the social worker cannot begin with a baseline phase. This may be the case particularly when it is urgent to introduce the intervention during the first contact with the client. With this design, the baseline phase can be introduced after the intervention has been implemented for a while, and the intervention can be reintroduced in a third phase. However, like the ABAB design, withdrawal of the intervention could pose ethical problems and may not work with irreversible outcome measures.</p>

<h3 id="goal-attainment-scale">Goal Attainment Scale</h3>

<p>A goal attainment scale (GAS) is a versatile evaluation tool that can be used to measure client outcomes. It is quite useful in evaluating the extent to which the worker’s interventions affect clients’ goals or outcomes in a favorable way. The GAS works well as an outcome measure with any of the single-system design designs described previously. As Table 9.2 indicates, a GAS is a five-point scale ranging from the most unfavorable to the most favorable outcome thought likely at a future followup time. The middle point on the scale is considered the expected level of success. The client and social worker use the right-hand column to fill in qualitative descriptors of each of the five points on the scale, with the expected level of success usually determined first.</p>

<p>A GAS has many advantages as an outcome tool. It is an incremental scale that can be used to measure degrees of progress. It can be individualized to a client’s specific or unique circumstances. And it is used most effectively when the client and worker collaborate in the development and use of the scale. When the tool is used in this way, it can be a great motivator in getting clients to work on their goals. The scale can also become a useful discussion topic when discrepancies become evident between what the worker and client perceive as the client’s level of success. It can also become the focal point of an informal or formal contract between the worker and client.</p>

<p><strong>Table 9.2. Goal Attainment Scale</strong></p>

<table>
  <thead>
    <tr>
      <th>LEVELS OF PREDICTED ATTAINMENT</th>
      <th>SCALE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Most unfavorable outcome thought likely (–2)</td>
      <td> </td>
    </tr>
    <tr>
      <td>Less success than expected (–1)</td>
      <td> </td>
    </tr>
    <tr>
      <td>Expected level of success (0)</td>
      <td> </td>
    </tr>
    <tr>
      <td>More success than expected (+1)</td>
      <td> </td>
    </tr>
    <tr>
      <td>Most favorable outcome (unlikely but still plausible) (+2)</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>Table 9.3 illustrates the use of a GAS in a parent-training program in which several parenting skills were introduced and practiced in small groups for parents who had been abusing their children. This is a building-block approach that begins with reading; learning comes next, followed by using or applying a parenting skill. Note that the scale reflects a decision to set an expected level of success for these parents after three months of s imply l earning one parenting skill. Although this may not seem like a far-reaching level of success, this program documented evidence that learning one skill would realistically take this long to internalize.</p>

<p>Cox and Amsters (2001) effectively used the GAS as a multidisciplinary measure of client outcomes for rural and remote health services. They found several principles to be important in using a GAS. The goals need to remain realistic and relevant to both the clients’ capacity or potential and the agencies’ resources. In addition, the levels must be mutually exclusive and measurable. They found that the GAS had limitations as well; most important, it was not effective if staff were not fully trained in its use.</p>

<p>A GAS can be developed as either a qualitative or quantitative measure or a combination of the two. The scale in Table 9.3 uses both types of measures; for example, it measures the number of parenting skills learned and takes into account four different behaviors that overlap: ignoring, reading, learning, and using. The scale could also include more than two variables at each level even though it did not in Table 9.3.</p>

<p><strong>Table 9.3 Goal Attainment Scale With Goal of Improving Parenting Skills</strong></p>

<table>
  <thead>
    <tr>
      <th>LEVELS OF PREDICTED ATTAINMENT</th>
      <th>SCALE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Most unfavorable outcome thought likely (–2)</td>
      <td>Ignore parenting skills altogether</td>
    </tr>
    <tr>
      <td>Less success than expected (–1)</td>
      <td>Read a book on parenting skills</td>
    </tr>
    <tr>
      <td>Expected level of success (0)</td>
      <td>Learn one new parenting skill</td>
    </tr>
    <tr>
      <td>More success than expected (+1)</td>
      <td>Use one new parenting skill with children</td>
    </tr>
    <tr>
      <td>Most favorable outcome (unlikely but still plausible) (+2)</td>
      <td>Use two new parenting skills with children</td>
    </tr>
  </tbody>
</table>

<p>Notes: Date and rating at construction: January 5; Date and rating at follow-up or evaluation time: April 15.</p>

<p>A GAS can be implemented with the following principles and sequential steps kept in mind (Dudley, 2011):</p>

<ol>
  <li>Consider first whether the GAS is the most appropriate design to use with a specific client. Among other considerations, a GAS depends on clients meeting with a worker for more than one session and having measurable goals.</li>
  <li>Make sure that you cover and that the client understands all the pertinent issues of informed consent, including the purpose of the selected tool, how it works, how the client can benefit, and the other aspects of informed consent.</li>
  <li>Help the client identify the problem, need, and its causes.</li>
  <li>Identify a goal that can resolve this problem and need that is agreeable to the worker and client.</li>
  <li>Set a realistic date for the goal to be achieved, referred to as an evaluation time.</li>
  <li>Then work together to construct specific predictions for a series of five levels of reaching the goal (most unfavorable, less than expected, expected level of success, more than expected, and best anticipated success) by recording them on the scale. Usually the easiest way to do this is to first identify the expected level of success, followed by the outer limits of the most unfavorable and most favorable outcome. Finally, identify the less-than-expected and more-than-expected levels.</li>
  <li>Now the GAS is ready to be used by measuring the client’s actual level of attainment in reaching the goal during the first session as a baseline measure.</li>
  <li>Finally, the client’s actual level of attainment in reaching the goal can be measured during the last session. At this time, discuss the progress that has been made and how the GAS was helpful in measuring this progress.</li>
</ol>

<h3 id="target-problem-scale">Target Problem Scale</h3>

<p>The GAS has limitations. For example, some problems may not easily translate into goals, or clients may not be either willing or able to identify goals to work on. Also, a GAS does not work well if a client is seen for only one session. An alternative outcome scale that can be used is a target problem scale (TPS). In brief, a TPS helps clients identify their problem(s), apply an intervention to overcome or cope with them, and then periodically rate the problem to measure any deteriorations or improvements. A TPS is helpful, for example, if the client remains preoccupied or overwhelmed with multiple problems and is not prepared to immediately pursue goals to overcome the problems.</p>

<p>Like a GAS, the TPS is an incremental scale. It measures the degree of severity of the problem, and the scale is individualized to a client’s specific circumstances. Figure 9.4 describes a TPS (Marlow, 2011). In this scale, target problems are identified in the far-left column. This form provides space for up to five problems. Each problem is then assessed in terms of its degree of severity for each of six sessions.</p>

<p><img src="https://i.imgur.com/1Fsxb8f.png" alt="Imgur" /></p>

<p>The five-point severity scale measures problems from “no problem” to “extremely severe.” The next column to the right is the improvement scale used to calculate the degree to which a problem has changed for better or worse. Finally, an overall improvement scale can be used to determine the extent to which improvement has occurred.</p>

<p>The overall improvement rating is obtained by totaling the degree of change scores and dividing by the number of target problems. This results in a number that reflects clients’ overall improvement on all problems.</p>

<p>An example of a TPS used with parents who were lacking strong parenting skills is described in Figure 9.5. As the figure indicates, the parents had multiple problems, including overlooking their parental neglect, getting to appointments late, spending too little time with their child, getting the child to bed late, and yelling at each other. Figure 9.5 summarizes the clients’ progress or lack of it after six sessions. In brief, the TPS indicates that their difficulty in facing parental neglect improved from “extremely severe” in the first two sessions to somewhat better or “not very severe” by the sixth session. Overall, most of their progress on other problems over six sessions was in getting to counseling sessions on time and in spending more time with their child. A little improvement was evident in not yelling at each other so often.</p>

<p><img src="https://i.imgur.com/mMewYzt.png" alt="Imgur" /></p>

<h3 id="sequencing-blocks">Sequencing Blocks</h3>

<p>Other means of evaluating a client’s outcomes, positive or negative, are also available, even though they may not be readily known or used. For example, sequencing blocks are a qualitative exercise that can help clients explore the causal relationships among prior behaviors that resulted in a positive or negative outcome. In this case, the outcome behavior is known and the behavior that caused it is to be discovered. Sequencing blocks can be used to help clients gain insight into prior behaviors and interactions that could trigger a current problem behavior that is being addressed (Laura Kroll, graduate student placed at Alexander Youth Network, Charlotte, NC, personal communication, April 15, 2006). This is an exercise that can use narrative from the discussions between a worker and client about a problem behavior, such as cursing out a teacher or getting angry and walking off a job. The discussion typically begins with the current problem behavior and its larger context. Then the discussion can work its way in reverse, going back to what happened just before the problem behavior occurred, and then what happened before that. The conversation eventually can get back to the initial set of behaviors or incidents in the overall episode. Figure 9.6 portrays typical sequencing blocks. As the discussion between the social worker and client proceed, individual behaviors and interactions are written into the blocks in the order that they occurred.</p>

<p><img src="https://i.imgur.com/5WLK3UW.png" alt="Imgur" /></p>

<p>Figure 9.7 describes an example of a teenager who was expelled from school for cursing out his teacher. In this case, the teenager, John, was meeting with a school social worker about the incident upon returning to school from a suspension. The sequencing blocks were used to help him gain insight into the earlier triggers and the precipitating factors contributing to the problem. Their discussion began with the problem behavior and worked its way back one behavior and interaction at a time. It eventually got back to a trigger in which the teacher perceived, incorrectly, that John had initiated a conflict with a peer around opening a container of applesauce at lunch on a field trip. The sequencing block exercise helped John understand not only that his teacher had misperceived the cause of the conflict but also how he could have avoided the interchange with the teacher until after calming down and how he may have been able to avoid the incident in the first place, by avoiding sitting with the peer who got him into trouble.</p>

<p><img src="https://i.imgur.com/yv701g8.png" alt="Imgur" /></p>

<p>Sequencing blocks can also be used to identify prior behaviors or decisions that result in positive outcomes for a client. For example, consider a client with a disability who has difficulty meeting women but who is interested in a friendship and had an unusually positive experience one night in meeting a new acquaintance. The client initially explained this discovery as a “miracle.” The worker and client used this positive outcome to look back at prior behaviors and decisions made by the client that may have been positive triggers that could be tried again.</p>

<h2 id="summary">SUMMARY</h2>

<p>The chapter is devoted to introducing several types of evaluations that can be used during the outcome stage of a program or practice intervention. The overall purpose of outcome evaluations is to determine whether the intervention resulted in a positive outcome for the clients and whether there is enough evidence that the improvements in the clients’ outcomes are significant. Significance is explained as being determined in either of two ways. One way is to calculate significance using a mathematical test and probability theory. The other way is to use a prior clinical standard and then determine whether the clients’ changes met or exceeded that clinical standard. Client outcomes are also discussed in the context of whether there is enough evidence of a causal relationship existing between an intervention and the client outcomes. Three conditions are described and required before a causal relationship can be claimed. Ultimately, during the outcome stage, the purpose of an evaluation is to determine whether the intervention is effective in helping the clients it serves.</p>

<p>The chapter discusses what client outcomes are, how they can be measured, and the criteria for determining whether they are adequate or acceptable. Several criteria are described including the outcomes’ usefulness, validity, reliability, precision, feasibility, cost, and unit costs.</p>

<p>Six group designs are introduced that can be used to determine the extent to which <em>program</em> interventions have a causal effect on client outcomes. They include two pre-experimental designs, two quasi-experimental designs, and two experimental designs. Experimental designs are the strongest designs that can be used for claiming a causal relationship, but ethical concerns are also likely to exist with these designs. Pre-experimental designs are the weakest designs, but they are useful in beginning to explore causality when the intervention is in an earlier stage of development. Several types of evaluations are also described in the chapter for evaluating client outcomes for <em>practice</em> interventions. These evaluations include single system designs, GASs, TPSs, and sequencing blocks. These designs can be used in a variety of situations to determine how effective t he social workers’ practice is in helping each client system that they serve.</p>

<h1 id="discussion-questions-and-assignments">DISCUSSION QUESTIONS AND ASSIGNMENTS</h1>

<ol>
  <li>Review the six group designs summarized in this chapter and then answer the following questions (Appendix C has answers):
    <ul>
      <li>  Which design seems ideal? Why do you think so?</li>
      <li>  Which design is the easiest and least expensive? What can be concluded from this design?</li>
      <li>  Why are multiple measures of the client outcome variable obtained when conducting a time-series design? What are the benefits of the multiple measures?</li>
      <li>  Which design may be the most practical for determining whether the program intervention was responsible for positive changes in the outcome variables? Why?</li>
      <li>  What is useful about a pretest–posttest design?</li>
      <li>  Under what circumstances would an evaluator consider a posttest-only control-group design?</li>
    </ul>
  </li>
  <li>This is an assignment that can be completed in class or as an assignment to video and bring to class.
    <ul>
      <li>Role-play a worker–client relationship using a goal attainment scale (GAS). Conduct a first session when the GAS is introduced. Identify a goal that you and your client want to work on. Make sure that you mutually agree on the goal. Also identify a goal that both you and your client agree is measurable. Explain the GAS to the client so that it can be readily understood. Consider using a simpler version that fits your specific client: maybe a three-point scale, simpler words, modified wording (e.g., such as vision of the future or dreams rather than goals).</li>
      <li>Later, conduct a session in which the worker and client assess how well the client did in reaching the defined goal. Have your client rate her or his progress along with your rating, and then discuss why your ratings are different if they are. Finally, explore what the specific benefits of the evaluation tool are to the client.</li>
    </ul>
  </li>
  <li>Design a program or practice intervention and a set of outcome measures for a group of clients by following three steps:
    <ul>
      <li>Select a group to focus on (e.g., teen fathers, older widows, LGBT teens).</li>
      <li>Identify two problems/needs and two outcome measures for the group.</li>
      <li>Design a program or practice intervention to help the group reach these two outcomes.</li>
    </ul>
  </li>
  <li>Critique White’s (2005) choices of outcome indicators for clients experiencing domestic violence in a domestic violence agency (in a box earlier in the chapter). Which ones do you think are most useful and why?</li>
  <li>Assume that you are working in an agency serving people with a dual diagnosis of depression and alcohol abuse. Make up what you understand to be their specific problems/needs related to depression and alcohol abuse (e.g., for depression, the client has no social contact with others). Then identify two outcome indicators for clients that could be used to measure the impact of a program on their depression and identify two outcome indicators determining whether their alcohol abuse problem/need is brought under control.</li>
  <li>This is a policy-related assignment. Set up two teams of students: one assigned the task of identifying and arguing for the advantages of keeping welfare recipients on the welfare rolls and the other on removing them from the welfare rolls. Each team should begin by identifying two or three desired outcomes of either keeping or reducing, respectively, the number of welfare recipients. For example, a possible outcome for reducing the rolls could be savings to the taxpayer, while an outcome for maintaining recipients on welfare could be to insure that they have health insurance.</li>
  <li>This is a major class assignment. Carry out the following steps in completing this assignment.
    <ul>
      <li>Identify a need for a practice evaluation in your field agency and a client system that can benefit from an evaluation. Inform the client system about the evaluation and encourage them to participate.</li>
      <li>Select the type of evaluation and specific tool that most readily fits your client’s situation. Choose a tool from the following list: a single-system design, a goal attainment scale, a target problem scale, or sequencing blocks. Feasibility is one important consideration in your selection. For example, consider the number of sessions that you will likely see your client, the overall amount of time that your agency will serve the client, and any difficulties you may have in identifying a measurable goal(s) for the client. Discuss the assignment thoroughly with your field instructor to make sure that she or he understands the assignment and the evaluation tool that you will use.</li>
      <li>Obtain informed consent either in written or oral form. Make sure that all the pertinent issues of informed consent are covered and understood by the client (e.g., purpose of the selected tool; how it works; how the client, worker, and agency can benefit; expectations that you have for the client’s participation; informing the client that this is a class assignment to be turned in to your instructor; ensuring confidentiality; reminding the client of the option to say no or to withdraw after the evaluation begins). Whenever your field agency has a protocol for obtaining informed consent, use it to complete this assignment.</li>
      <li>Devise and implement your practice evaluation.</li>
      <li>Finally, describe what you have learned about evaluating your practice from doing this assignment and what advantages and limitations you see in using this tool.</li>
      <li>Turn in a paper or give a presentation that includes a description of how you implemented each of the previous steps. Also, include the instruments that you used.</li>
    </ul>
  </li>
</ol>

<h2 id="references">REFERENCES</h2>

<ul>
  <li>Babbie, E. (2016). <em>The practice of social research</em> (14th ed.). Boston, MA: Cengage Learning.</li>
  <li>Beck, A. T., Steer, R. A., &amp; Brown, G. K. (1996). <em>Manual for the Beck Depression Inventory</em> (2nd ed.). San Antonio, TX: Psychological Corporation.</li>
  <li>Cabarrus County. (2007). <em>Performance report 2006</em>. Concord, NC: Author.</li>
  <li>Campbell, D., &amp; Stanley, J. (1973). <em>Experimental and quasi-experimental designs for research</em>. Chicago. IL: Rand McNally.</li>
  <li>Ciffone, J. (2007). Suicide prevention: An analysis and replication of a curriculum-based high school program. <em>Social Work, 52</em>(1), 41–49.</li>
  <li>Comfort, M., Loverro, J., &amp; Kaltenbach, K. (2000). A search for strategies to engage women in substance abuse treatment. <em>Social Work in Health Care, 31</em>(4), 59–70.</li>
  <li>Community Choices Inc. (2006). <em>Helping mothers have healthy babies and families</em>. Charlotte, NC: Author.</li>
  <li>Cook, T., &amp; Campbell, D. (1979). <em>Quasi-experimentation: Design and analysis issues for field</em> <em>settings</em>. Skokie, IL: Rand McNally.</li>
  <li>Cox, R., &amp; Amsters, D. (2001). Goal attainment scaling: An effective outcome measure for rural and remote health services. <em>Australian Journal of Rural Health, 10,</em> 256–261.</li>
  <li>Dudley, J. R. (2011). <em>Research methods for social work: Being producers and consumers of research</em> (upd. 2nd ed.). Boston, MA: Pearson.</li>
  <li>Gambrill, E. (1999). Evidence-based practice: An alternative to authority-based practice. <em>Families in Society, 80,</em> 341–350.</li>
  <li>Gibbs, L., &amp; Gambrill, E. (1996). <em>Critical thinking for social workers: A workbook</em>. Thousand Oaks, CA: Pine Forge Press.</li>
  <li>Harding, R., &amp; Higginson, I. J. (2003). What is the best way to help caregivers in cancer and palliative care? A systematic literature review of interventions and their effectiveness. <em>Palliative</em> <em>Medicine, 17,</em> 63–74.</li>
  <li>Litzelfelner, P. (2001). Demystifying client outcomes. <em>Professional Development, 4</em>(2), 25–31. Lynch, C. J. (2007). <em>Exploring the implementation of a transitional services program for adolescents in the Texas Foster Care System</em>. Unpublished doctoral dissertation, University of Texas, Austin, TX.</li>
  <li>Marlow, C. (2011). <em>Research methods for generalist social work</em> (5th ed.). Stamford, CT: Thomson Learning.</li>
  <li>Martin, L. L., &amp; Kettner, P. M. (2009). <em>Measuring performance of human service programs</em> (2nd ed.) Thousand Oaks, CA: SAGE.</li>
  <li>McLean, T. (2007). <em>Evaluating progress on personal goals of a children’s group using journaling: Proposal for the formation of a new group</em>. Unpublished manuscript. University of North Carolina, Charlotte, NC.</li>
  <li>National Association of Social Workers. (2017). <em>Code of ethics</em> Washington, DC: Author. O’Hare, T. (2015). <em>Evidence-based practice for social workers: An interdisciplinary approach</em>. (2nd ed.) Chicago, IL: Lyceum Books.</li>
  <li>Thyer, B., &amp; Meyers, L. (2003). Linking assessment to outcome evaluation using single system and group research design. In C. Jordan &amp; C. Franklin (Eds.), <em>Clinical assess-</em> <em>ment for social workers: Quantitative and qualitative methods</em> (2nd ed., pp. 385–405). Chicago, IL: Lyceum Books.</li>
  <li>U.S. Department of Health and Human Services. (2017). Child welfare outcomes 2010– 2014: Report to Congress. Executive summary. Retrieved from https://www.acf.hhs.gov/cb/resource/cwo-10-14</li>
  <li>White, M. (2005, March 16). A demonstration of evaluation as a central aspect of social work practice at the Women’s Commission. Presentation to a social work research class, University of North Carolina, Charlotte, NC.</li>
</ul>



  <small>tags: <em></em></small>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/MrLyn20">MrLyn20</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/Lyn/assets/js/scale.fix.js"></script>
  </body>
</html>
