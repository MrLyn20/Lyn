---
title: Chapter 8 Improving How Programs and Practice Work
date: 2023-08-08
author: JAMES R. DUDLEY
---

**PART IV**

The Implementation Stage

Chapter 8 is the only chapter in this section. This chapter is devoted entirely to the evaluation issues at the implementation stage of a program or practice intervention. During the implementation stage, many overall evaluation questions can be asked and answered that revolve around the theme of how well the program or practice interventions are working.

# CHAPTER 8 Improving How Programs and Practice Work

James R. Dudley and Robert Herman-Smith

_How is the intervention supposed to be implemented? How well is the intervention working?_

This chapter addresses these implementation questions and others. It does this by conducting implementation studies. Documenting and monitoring how an intervention is implemented are vital areas of evaluation and essential for program integrity. During the implementation stage, many questions are asked and answered that revolve around the theme of how well the program or practice intervention is working. (See Figure 8.1.)

![Figure 8.1](https://i.imgur.com/BWLaUpe.png)

Implementation can be thought of as the intentional application of interventions and strategies to solve problems in real-world settings (Mitchell, 2011). Implementation evaluation is primarily concerned with and investigates the procedures and processes established to carry out a program. In other words, how well have agencies followed procedures to support a new intervention and how have these changes impacted clients?

The implementation stage of an intervention is an opportune time to conduct a variety of evaluations. These evaluations can raise numerous important, if not essential, questions about the integrity of a program or practice intervention. For example, are all the required components of an intervention implemented as planned? What program components seem to work and which ones do not? Has a team of qualified and competent staff members and volunteers been hired to provide the designated services? Are the key parties (e.g., administrators, staff, volunteers, and clients) communicating adequately? Many of the types of implementation evaluations covered in the chapter are identified in Table 8.1, along with some general evaluation questions that they raise. Although this is not intended to be an exhaustive list of evaluations of implementation, it offers numerous examples of what is important.

>Table 8.1. Types of Implementation Evaluations
>
>TYPES OF IMPLEMENTATION
>1. Linking the intervention to the clients' problems
>2. Implementing the intervention as proposed
>3. Adopting and promoting evidence-based interventions
>4. Focus on staff members
>5. Accessibility of the intervention
>6. Program quality
>7. Client satisfaction
>
>SOME OVERALL QUESTIONS ASKED
>1. Does the intervention address the causes of the clients problems that are the focus of concern?
>2. Is the intervention being implemented as it was intended or proposed?"
>3. Is there evidence from manuals, evaluations," and/or clinical practice that an intervention is effective?"
>4. How are staff members selected, trained, involved, etc.?"
>5. How accessible is the intervention to all the intended target groups?
>6. Is the level of quality of the intervention high enough to satisfy the stakeholder?
>7. How satisfied are the clients with the intervention?

![Imgur](https://i.imgur.com/VM6OoBL.png)

## LINK THE INTERVENTION TO THE CLIENTS’ PROBLEMS

During the input or planning stage, major attention is focused on the problems and needs of prospective clients. As stated in Chapter 6, a need is an aspect of a larger problem identified by a client that is perceived to be amenable to change. Meeting a set of needs is the intended focus of a proposed intervention. Another issue is also important to explore: What are the underlying causes that prevent the need from being met? This is a critical question because the proposed program is expected to address the underlying causes.

An example of the logical link between the causes of a problem and the approach used by an intervention to address it is briefly illustrated in Table 8.2 for the problem of child abuse. Several known causes of child abuse have been identified, including intergenerational transmission (abuse being taught from generation to generation), inadequate parenting skills, stresses associated with poverty, and isolation from important social supports. Each cause suggests a different program response. The example is somewhat simplistic because it infers that a complex problem such as child abuse has a single cause. Yet the example makes an important point. An intervention should be logically linked to the underlying causes of a problem. Each of the causes of child abuse suggests a response that will address it. As the example suggests, child abuse perpetrated by parents who were abused as children will not be reversed if it does not include some type of increased awareness and recognition of these intergenerational patterns as part of the intervention. Similarly, some form of parenting skills training is essential if the problem is inadequate parental skills since abuse signals that parents are likely to need alternative disciplinary techniques. The link between child abuse and poverty can be addressed by preparing parents for a higher-paying job. Likewise, if social isolation is one underlying cause of abuse, teaching social skills and linking families to healthy social contacts are logical responses.

As discussed in the last three chapters, the logic model provides an important

organizing framework for understanding evaluations. Introducing the logic model at the implementation stage provides a framework for considering many ways to improve an intervention, to correct its course if needed, and to maintain its quality. The logic model helps focus on the sequence of steps that link the implementation of the program back to the clients’ unmet needs and forward to the clients’ anticipated outcomes or accomplishments. In this regard, interventions should address the problems and needs of their recipients and the underlying causes. Further, the implementation of an intervention should result in the clients achieving their anticipated outcomes.

>**Table 8.2. Link Between Causes of a Problem and a Logical Intervention**
>
>IDENTIFIED CAUSE OF CHILD ABUSE
>
>1. Intergenerational cause (abusing parent was abused as a child)
>2. Lack of parenting knowledge and skill
>3. Economic stress from a low-income job
>4. Stress from social isolation
>
>LOGICAL PROGRAM INTERVENTION
>
>1. Facilitation of insight into intergenerational link through therapy
>2. Training in parenting skills
>3. Increase in income through new job training and/or job change
>4. Peer support group of parents

![Imgur](https://i.imgur.com/UsiMp3L.png)

For many years, agencies have been adopting the reasoning behind the logic model in requirements for most grant proposals. Grant writers are expected to document such things as the links between clients’ problems and the intervention they propose to implement. In brief, a convincing explanation needs to be mounted to the funding agency for how a proposed intervention can help clients resolve the problems of concern. For this reason, implementation evaluation is most concerned with the processes of the program and practice interventions of the logic model.

Some important implementation questions are raised based on the logic model. Does the intervention seem to be directly linked to clients’ problems? Furthermore, is there evidence that the intervention can provide solutions to these problems? According to Pawson and Tilley (1997), an evaluation of the links between the causes of a problem and the program approach answers three key questions:

1. What are the mechanisms for change triggered by a program?
2. How do these mechanisms counteract the existing social processes?
3. What is the evidence that these mechanisms are effective?

Alcoholics Anonymous (AA) offers an example of an approach to a social problem— substance abuse. Implementation evaluations are interested in how programs actually work; for example, what are the mechanisms of an AA support group that helps people overcome the addictive tendencies of alcohol? Is it, as the philosophy of AA suggests, the spiritual ideology and message of the twelve steps? Is it the support that comes from others going through the same struggles? Is it a combination of spiritual ideology and social support from others struggling with substance abuse? Or is it something else? Evidence of what makes AA work for so many people could partially be found in the answers to these questions from an evaluation of a representative sample of some of the thousands of AA programs that meet regularly across the country.

Evaluation studies also are expected to answer the question about the social and cultural conditions necessary for change to occur among the recipients of an intervention. In other words, how are the sociocultural factors recognized and addressed within a program? A new mentoring program for young African American men who did not have an adequate father figure for bonding when they were children provides an example. Several sociocultural questions could be asked of an agency sponsoring such a program. For example, to what extent and how does this program recognize the sociocultural factors? Are older African American men available to serve as mentors? Are the mentors capable of providing some of the missing pieces in well-being that these teenagers need? Do the mentors have any training or other preparation in male bonding based on an evidence-based curriculum?

One evaluation identified the essential elements of a program for preventing crimes in a housing complex for low-income residents. The evaluation team identified ten key elements of a crime prevention housing program that would be needed based on evidence of prior programs with a similar purpose that were effective.

> **Example of an Evaluation of the Essential Elements of a Housing Program**
>
>Foster and Hope (1993) wanted to identify the essential elements for preventing crime within a public housing complex. Their evaluation focused on identifying a list of key elements found to be essential in the effectiveness of prior programs of a similar nature. They concluded that ten elements were essential.
>
>1. A local housing office for the program
>2. A local repair team
>3. Locally controlled procedures for signing on and terminating tenants in housing units
>4. Local control of rent collection and arrears
>5. Tenants assume responsibility for caretaking and cleaning of the open space around units with the assistance of a locally supervised staff team
>6. An active tenant advisory group with a liaison to the management of the program
>7. Resources available for any possible small-scale capital improvements
>8. Well-trained staff that delegate authority
>9. A project manager as the key figure to be accountable for management of the program
>10. A locally controlled budget for management and maintenance

In the housing example, the evaluators accumulated substantial evidence that the successful housing complexes in preventing crime in their city had these ten essential elements. Housing complexes that were not totally effective were without all, some, or even one of the elements. As the elements suggest, some common themes included a housing management team with some local control, realistic expectations of the tenants, availability of important resources, an active tenant advisory council, and a collaborative relationship between the council and the management team.

## IMPLEMENT THE INTERVENTION AS PROPOSED

Other types of questions address whether the intervention is implemented as proposed or intended. How an intervention is supposed to function may relate back to an initial grant proposal or other early planning documents. Implementation as intended could also be based on more current reports describing the policies and practices of programs that have been running for some time. Provision of a detailed description of an intervention as it is supposed to be implemented is a first step in this kind of evaluation.

A clear description of the intervention is needed prior to monitoring how it is implemented. Therefore, it is often a good idea to begin with an accurate, written description of the intervention, whether articulated in an initial grant proposal or somewhere else. It is wise to describe an intervention in enough detail so that it can be replicated. An example of a program description is in a report about a visitation program for non-custodial parents (Fischer, 2002). The purposes of the program are to assist parents in establishing an access agreement with the custodial parent and in pursuing their legal rights and responsibilities as parents. The article documents the process of establishing and maintaining visitation agreements and identifies the principal barriers to establishing visitation. It includes a description of the policy and legal context for the program, a review of the pertinent literature, a description of a pilot program, a pilot process assessment, and a pilot outcome assessment. Data are also included on the factors associated with successful visitation. The program description came from several sources, including case files, administrative records, and results of the pilot assessments.

Some further questions in attempting to find out whether a program is implemented as intended include the following:

- Are the clients being served the ones intended to be served?
- Are current staff members adequately qualified and trained to provide the proposed services at the required level of specialty and quality?
- Are the program’s goals and objectives evident or identifiable in the way in which the program is implemented?
- What happens on a typical day in a program (e.g., a daily routine study)?
- How do staff from different disciplines collaborate or work together?
- How are the roles of BSW and MSW staff members differentiated and complementary?

Weinbach (2005) points out that new programs may need to ask different questions than older programs when it comes to how the intervention is being implemented. Newer programs may need to ask:

- Is the program at its anticipated stage of development?
- How many clients have been served to date?
- Is the program fully staffed with qualified people?
- How well known is the program in the community?
- How well have sources of client referrals been developed?
- In what ways is the program supported and in what ways is it being questioned within the agency and community?

According to Weinbach (2005), programs that have been implemented for a few years or more and are considered more mature may ask another set of questions:

- Do the services and programs appear to have the potential to achieve their objectives?
- Are the services and other activities of the program consistent with the program model?
- Is the program serving the clients for whom it was intended? If not, why not?
- Is the program visible and respected in the professional and consumer community?
- How much attrition has there been among clients and staff?
- Do staff members perceive that administrative support is adequate?
- How satisfied are clients with the program?

Often interventions are not implemented as they were intended or proposed, and they may have gone adrift of their intended course. This can occur for several reasons. Perhaps the description of a new program model that was to be implemented was not adequately articulated and discussed with all the stakeholders initially. Perhaps the program goals and objectives were not fully developed, were crafted as unrealistic, or were displaced for some changing circumstances. Also, a program could decide to change course because of the changing needs or understanding about the client population. Finally, the people in charge of implementing a new intervention could be different from those who proposed and planned it. In this case, if close collaboration did not occur between the two sets of people, a lack of continuity from the planning stage to the implementation stage likely happened. Also, if all or most of the stakeholders are not involved at least in an advisory capacity in both the planning and implementation stages, there may not be enough accountability to ensure that the planning decisions are implemented.

>**Example of an Overlooked Target Group**
>
>A Head Start program was established in a local community that had an important stakeholder group, a neighborhood civic organization. The organization was very concerned about the needs of local children. They wanted to make sure that families with the least available resources and the least ability to find an alternative program for their preschool children were given top priority. Once the Head Start program fully enrolled its cohort of children, the civic group decided to find out  the social circumstances of the children and their families. To the surprise of some, they discovered that almost all the children were from resourceful families with modest incomes that were likely to also have access to comparable alternative programs. Therefore, the civic group raised its concern with the Head Start organization. When it received an unfavorable response, it pursued a lawsuit against the Head Start organization demanding that because the neediest families were the mandated target group they must be served. This lawsuit eventually ended up as   a class action suit that resulted in a ruling that all Head Start programs in that city had to reserve a percentage of their openings for this neediest group of families.

Gardner (2000) offers an example of one way to create a description of a program involving a team of stakeholders. This program was developed using the logic model. One purpose of this exercise was to provide a clear program description; another was to more fully orient staff toward the program and its workings. At one point, some general questions were raised and discussed among all staff members, including “How would you describe how you go about working with clients?” and “What would be the important elements in the process of working with families?” Gradually, a diagram was developed consisting of a series of boxes, each of which described a step in the process. Stage 1 described how families were encouraged to request services from this program. Stage 2 included helping families assess their strengths and the constraints they faced. Stage 3 involved goal setting. Stage 4 involved matching resources to family goals. How the family and staff worked to reach the goals was the focus of stages 5 and 6, and stage 7 involved completing the contract. The program description was then tested by asking some of the families, staff members, and other agencies how they perceived that the program actually worked using their experiences with it. Although the results of the interviews largely validated the proposed stages and principles that had been identified, the results also suggested the need to qualify and further refine some principles.

Monitoring an intervention’s implementation can be done in several different ways. Sometimes agencies conduct staff activity studies using a list of prescribed activities, such as direct contact with clients, contact with other programs on behalf of clients, phone contact with clients, record keeping, staff meetings, and so on. In some instances, the studies may be interested in finding out whether too much time is spent on one type of activity, such as record-keeping; in other instances, the interest may be in finding ways to increase time spent in direct contact with clients. These studies tend to be largely quantitative in nature (e.g., staff members tally the number of hours and minutes in each activity, each day, for a week or so).

Other evaluations attempt to find out more about the intricacies of the practice interventions provided to clients. The evaluations can be open-ended qualitative studies that identify what the social worker is actually doing based on observations, videotapes, or analyzing journal entries recorded by the practitioners that describe what they are doing. Or the evaluations can be more deductive and quantitative by examining the extent to which prescribed activities reflecting a specific practice theory or practice model are implemented.

Exploration of the intricacies of a practitioner model can be developed by prescribing an intervention protocol. For example, a protocol can be encouraged for medical social workers of a home-health program to follow when clients manifest different types of problems. A frequently encountered problem in home-health settings are clients who are socially isolated, lack contact with friends and family, and are alone most of the time. In this case, a protocol could be to implement some or all the following interventions:

- Provide a list of resources available to the clients that can reduce their social isolation.
- If clients are interested, assist with referral to a support group relevant to their interests and needs.
- Encourage activities appropriate to their medical condition.
- Explore and facilitate the clients’ expression of interests in specific activities.
- Help clients express their feelings about themselves, their sense of satisfaction with their lifestyle, and any desire to change it.
- Help clients explore and resolve feelings related to social isolation, such as grief from loss, a recent loss of a previous health status, or an unresolved, conflicted relationship.

Once these and other activities are implemented, efforts can be made to document any evidence that the client has progressed toward specific outcomes, such as using additional supports from other agencies, increased contact with others, and less time alone.

## ADOPT AND PROMOTE EVIDENCE-BASED INTERVENTIONS

A needs assessment should result in general agreement among stakeholders about a possible service gap that is contributing to a social problem. However, once the need is identified, stakeholders must also come to consensus about various strategies or interventions they will use to address the identified need. This happens during the time when an intervention is adopted. As an example, a suburban community might be concerned about increasing juvenile crime. A needs assessment determines that most juvenile crime is taking place on weekdays between the hours of 2 pm, when high school is dismissed for the day, and 6 pm when most parents in the community arrive home from work; therefore, more community or school-sponsored after-school programs for juveniles might result in reduced juvenile crime. The basic need (adding more after-school programs) has been identified. However, the type of program needed to address the problem might not be a settled matter for stakeholders. Some stakeholders could believe adolescents in the community need more self-discipline. They might favor competitive sports programs or military-type programs. Other stakeholders could believe adolescents need better role models and more individual attention. They might prefer more mentoring programs. Consensus on the type of program needed is often difficult to achieve, but key stakeholders must come to agreement about the type of program they believe is acceptable for addressing the problem.

A central issue to be addressed in adopting an intervention is whether the

intervention is evidence-based. As was reported in earlier chapters, evidence that an intervention works can take many forms and can have varying degrees of validity or accuracy. It also varies in terms of its efficacy from one client population to another. The best evidence is based on evaluation studies using all the characteristics of evaluations described in Chapter 1. A definition of evidence-based interventions was given in Chapter 2 stating that evidence comes mostly from both research studies using quasi-experimental or experimental designs and clinical practice. It is also important that evidence-based sources are consistent with the values and expectations of the clients who receive such interventions. Evidence indicating that an intervention is effective needs to be pursued and promoted whenever possible. Three general ways to promote evidence when adopting an intervention are shared next. They are (a) using evidence-based manuals and toolkits, (b) generating clinically based evidence, and (c) critically consuming evaluation reports. All three have the common thread of exploring the association between the introduction of an intervention and the client outcomes that are desired.

### Evidence-Based Manuals and Toolkits

For the past several years, funding sources have begun requiring community programs to adopt evidence-based practices, which are intervention models with demonstrated efficacy in random controlled clinical trials (Hallfors & Cho, 2007). Some evidence-based interventions have a highly prescriptive treatment protocol; that is, they expect practitioners to carry out a set of activities with clients in a specific sequence. Treatment manuals and toolkits are more likely to be used in health and mental health services than in other types of social work services. Part of the appeal of evidence-based manuals is that they have a body of evidence that supports their ability to create client change.

However, a growing body of health and mental health services research has shown that efficacious interventions (those that yield positive outcomes when delivered under optimum conditions in research clinics) might not be as effective in producing desired outcomes for their target populations in real-world settings (Fixsen, Naoom, Blase, Friedman, & Wallace, 2005; Hallfors & Cho, 2007; Proctor et al., 2010). Furthermore, some practices can be difficult to implement, especially when dealing with client populations that are transient or faced with major life stressors associated with poverty, or such things as unreliable transportation, or problems locating trustworthy child care. There are at least three major concerns about the adoption of evidence-based practices available in manuals.

First, many program directors and practitioners have been reluctant to embrace these evidence-based practice manuals. One reason for their reluctance is the perception that evidence-based practices interfere with the freedom to do their jobs as they see fit. In other words, some practitioners believe that externally driven evidence-based practices do not value their clinical judgment. This can be problematic since mental health practitioners, like workers in other fields, resist practices they believe have been imposed on them (Brodkin, 1997; Glisson & Green, 2006).

Second, many funders demand that programs use evidence-based practices without an appreciation for the investment of time and resources required to make them work as intended. There is a large body of literature showing that implementing evidence-based practices takes at least two years, often longer (Durlak & DuPre, 2008). Funders must be willing to give programs the time, funding, and other resources needed to hire the right staff with the kinds of degrees, certification, and experience to perform skills required by the practice; also as time goes on to integrate new practices into the agencies where they are delivered. Programs must also devote considerable resources to training their staff to implement evidence-based programs if they expect them to be effective.

Third, evidence-based practices must be a good match for the problem and the client population for which they are implemented. A clinic-based nutrition program for pregnant teens delivered in public health clinics might be highly effective in an urban or suburban area. Urban and suburban clinics are likely to run several clinic sites, and potential clients might be able to rely on a well-developed public transportation system to get them to their visits. The same program might fail in poor, rural areas since it requires teenagers to visit health clinics that are spread out across large geographic areas. Reliable transportation might not be available in these areas. In brief, adopting an evidence-based intervention using a manual is not a guarantee of success. An intervention’s success ultimately rests on whether the right intervention was adopted for the right population at the right time and in the right context (Fixsen et al., 2005). Even under the best of circumstances, implementing a new intervention takes a considerable amount of support from the stakeholders. Yet, use of evidence-based manuals in combination with other initiatives such as obtaining clinically based evidence and critically consuming other evaluations is a worthy effort in promoting evidence-based interventions.

>**Implementing an Evidence-Based Manual in a Domestic Violence Intervention Program**
>
>A domestic violence prevention and intervention program called Safe Families operated in a mid-sized city for several years. Safe Families completed a needs assessment and found that no other agency in the region was providing services to very young children exposed to domestic violence. After securing funding for services, Safe Families decided to adopt an intervention called child–parent psychotherapy (CPP), an evidence-based mental health intervention that addresses emotional and behavioral problems experienced by children younger than six years old who have witnessed domestic violence (Lieberman & Van Horn, 2004). Treatment involves up to 30 one-hour sessions including both the parent and child. The model’s efficacy has been supported by two random controlled clinical trials (Lieberman & Van Horn, 2008; Lieberman, Van Horn, & Ippen, 2005). However, implementing CPP provoked several unanswered concerns among staff. CPP assumes that participants are no longer experiencing violence in the home, but staff, based on many years of experience with victims of domestic violence, knew that relationships between perpetrators and victims are often “on again, off again.” CPP also assumes that mothers are emotionally ready to process past violence and to accept that their children were affected by the violence, but staff knew that mothers enter services at different levels of readiness to process this type of information. They might require more individual treatment before they can properly understand how violence has affected their children. This example illustrates that it is often important for a program evaluator who introduces an evidence-based manual to have a prior dialogue with agency staff, acknowledge their questions about evidence-based interventions, and learn more about how staff members have addressed their concerns in the past. As an exercise, what are some questions you would want to ask staff to help you contextualize their implementation of CPP?

### Generating Clinically Based Evidence

Clinically based practice can be another important source of evidence that an intervention works. Of course, clinically based evidence depends upon an agency provider being committed to evaluating their intervention as it is being implemented. Anecdotal “evidence” with little or no effort to explore causal connections between an intervention and client outcomes cannot be taken as serious evidence. Nevertheless, anecdotal evidence that clients are benefiting from an intervention can be a starting point in attempting to introduce an evaluation process.

Clinical practice can be an important source of evidence, especially when it is accompanied by an evaluation component. This evaluation component should have all or most of the following features. First, an intervention that is being introduced should be described and recorded in as much detail as possible. One test of enough detail is whether it can be replicated by another agency or even another practitioner in the same agency. Adequately describing an intervention is not an easy task. It presumes the input and consensus of the varied stakeholders that it is worth implementing. Further, for replication purposes, an intervention needs a description thorough enough that it considers the varied ways in which it needs to be implemented with different clients. These variabilities and nuances in implementation are important to capture and articulate. For example, an intervention may need to be implemented for at least five sessions with clients having one set of circumstances, but at least ten sessions for another group. As another example, if the intervention is flexible enough to be implemented with different systems, then these variations also need to be explained. For example, how is the intervention introduced differently to an individual as opposed to members of a small group? Other circumstances that affect how the intervention needs to be described are also important such as whether the client is voluntary versus mandated to receive the intervention.

Another related consideration is how the individual characteristics of different client systems (e.g., ability, age, culture) are likely to respond to the varied ways that the intervention is implemented. For example, are clients with greater intellectual attributes more likely to respond well to an intervention than those without a strong intellect? Also, are racial or ethnic characteristics and the gender of clients likely to respond better to one variation of the intervention than another? Do recent immigrants need a prior emphasis in some areas such as resolving personal documentation issues or addressing fears of deportation before benefiting adequately from the intervention?

Most important, the components of a research design (i.e., group design) are important to utilize when an intervention is being implemented. The clinical practitioner should pay attention to sampling issues such as who was selected to receive the intervention and who was not, and what is the rationale for this sampling decision? The clients’ goals or outcomes are also very important to measure both before and after the intervention is implemented. It also makes sense to measure these outcomes periodically while the intervention is being implemented. Another important factor is to find ways to manage the influences outside the realm of the intervention by having a comparison group or using a single-system design that captures the impact of extraneous influences over time.

### Critically Consuming Evaluation Reports

Everyone in the human services, including students and faculty in professional programs and employees in social agencies, has a role to play in promoting evidence-based practice. One important way to do this is to regularly read and keep abreast of evaluations and other relevant material in professional journals, books, and unpublished reports. Several professional journals are mentioned in Chapter 6 that have an exclusive focus on evaluations. Professionals working in a specific field such as mental health or child welfare should obtain access to journals in their fields for a similar purpose. Agency sponsors should subscribe to these journals, use them in workshops, and make them available to staff members to read.

A primary purpose of many evaluation reports is to provide evidence supporting an intervention’s effectiveness. Strictly speaking, this evidence applies to the clients in the report who received the intervention. From there, a critical consumer of the report has the skills to determine the extent to which these findings may also be applicable to other client groups, always with some caution and skepticism. Reading an evaluation report to understand evidence-based information can be challenging. Critical consumer skills are definitely needed. These skills include being able to identify and comprehend all the sections of an evaluation report, including the problem definition and literature review sections, the purpose of the evaluation, the research design, the findings, and the recommendations. In addition, a critical consumer must detect the weaknesses, flaws, and biases of the evaluation, whether identified by the author or not. Not an easy task, and it is one that takes a lot of practice.

The findings and recommendations provide the most important information in a research report because this is where the consumers determine whether the report can be useful to them. In other words, can the findings be relevant to the consumers’ work with their clients? This is a bigger question that boils down to several smaller questions like: how are the clients in the report similar and different from the clients of the consumers; how is the intervention that is being evaluated relevant to the one used by the consumers; are the findings in the report pertinent to consumers’ clients and their goals; and how can these findings be applied, cautiously, with the consumers’ clients?

Chapter 12 focuses entirely on the critical consumer skills needed to evaluate an evaluation report. These skills are reviewed using the Seven Evaluation Steps as an organizing framework. Chapter 12 looks closely at each of these steps, their importance to stakeholders, how they should be described in a report, and what they reveal of interest to a critical consumer. An evaluation report published in a social work journal is used to illustrate how critical consumption can occur.

## FOCUS ON STAFF MEMBERS

There are several components of implementation within any program or practice area that could be evaluated (Fixsen et al., 2005). This section will address many of the most commonly examined areas of focus that pertain to staff members. They include staff selection; staff orientation and training; ongoing consultation, supervision, and coaching; staff adherence to the protocol of an intervention; diversity; efficiency; and staff working conditions. This presentation usually offers an ideal way to address these areas and anticipates that readers will implement them to the extent that they have available resources.

### Staff Selection

Hiring the ideal candidate is not always possible. Fluctuations in the labor market, accessibility of training programs, extent of interest in the type of position offered, and the pay scale for the position are just a few of the issues that affect hiring. Still, programs should devote considerable resources to hiring the right person.

An implementation evaluator should look for evidence of a hiring plan. There should be evidence that a written recruitment plan was developed before hiring began. This should include a basic job description, salary range, minimum qualifications, and preferred qualifications. Staff involved in hiring should agree on the kinds of skills the new hire should possess. If the agency is determined to diversify its staff, these efforts should be documented. Sometimes, this takes the form of special outreach programs such as advertising the position in non-English-language media or participating in a jobs fair that attracts potential applicants from cultural minority backgrounds.

Staff selection involves staff recruitment for the new program. Many staff positions have minimum educational and licensing requirements that applicants must meet to be considered for the job. For example, trauma-focused cognitivebehavioral therapy (TF-CBT) is an evidence-based, manualized intervention provided to children aged 3 through 18 who have experienced trauma (Cohen & Mannarino, 2008). This intervention has specific implementation guidelines for how it should be implemented. Only practitioners who have gone through the training can practice TF-CBT. Consequently, a program using TF-CBT must (a) hire someone who is certified to practice it or (b) obtain training for hires that are not yet certified. Program planners might prioritize hiring someone with certain characteristics if there is reason to believe doing so will facilitate client participation and engagement. For example, if many clients who will be receiving services under the program do not speak English, hiring someone who speaks that population’s first language would be important. Doing so would save the agency money since it would not require money to pay for interpreting services. People who need the services might also be more willing to participate if some staff members can speak their native language.

>**A New Hire That Was Not a Good Fit**
>
>One of the chapter authors (Herman-Smith) has a background in early childhood mental health. A highly regarded social worker accepted a position in an agency where he worked. She had over twenty years of experience performing mental health evaluations with children aged five through twelve. Part of the responsibilities in her new job was to complete mental health evaluations with children ages two through five. This social worker wanted to work with younger children but did not have much professional experience with them. During her initial work with younger children, she insisted that the children stay seated for most of the session without a break. She refused to sit on the floor with them and insisted they remain in a chair. She was annoyed that so many of the children “displayed poor boundaries” by reaching for and pulling on the very nice bracelets and necklaces she liked to wear. After three months, she quit the job. She could not adapt to the natural impulsiveness, shifting interests, and need for frequent movement that is common in younger children. Transferring skills from one setting and population and using them with another can be challenging. In this case, the agency could have provided more training and support to help the new employee adapt to the new work environment.

### Staff Orientation and Training

Staff selection is only the first step. Orientation and training are critical to the staff ’s success in a new program. New hires require orientation. Orientation refers to the process of teaching new hires the formal rules of the organization, from employee benefits to safety protocols in the agency to the agency’s chain of command. Orientation to the work environment is critical in helping new hires do their jobs safely and effectively. Implementation evaluators should look for evidence of a formal, written orientation plan.

Training refers to assistance with the more technical aspects of the new job. During training, employees are familiarized with the history, theory, and rationale for an intervention. In human service agencies particularly, training focuses on the how, when, and where of interacting with clients. Orientation usually refers to new hires only. Training involves teaching both new hires and existing staff who have taken a new position how to work effectively in their work roles. Training is needed, even if hires have several years of experience doing similar work in the past, since their skill sets will probably be used in new ways in their new jobs.

### Ongoing Consultation, Supervision, and Coaching

Perhaps too often, job skill development has been conceived as something that happens shortly after a person is hired. A person is hired, goes through orientation, gets training for some specified time period, and then does the job he or she was hired to do. There are two reasons this perspective on skill development is limited. First, social workers and other human service workers engage in work requiring them to make quick judgments in ambiguous situations. The “right” answer is not always obvious, and, in many cases, whether the social worker made the right call with a specific client at a particular time remains a matter of debate. Good clinical supervision and the support of peers and program administrators are critical, especially for new social workers. Second, the traditional didactic training approach, one in which a trainer does most of the talking during a time-limited lecture in a classroom, has a limited impact on professional skill development (Dixon et al., 1999; Proctor et al., 2007). Training introduces employees to the basic structures of intervention, but they need ongoing support to carry out their daily, on-the-job tasks as they were designed.

Ongoing consultation, supervision, and coaching that provide skill demonstrations from supervisors, opportunities to practice these skills, and immediate feedback are more likely to result in improved professional skills than training and workshops (Joyce & Showers, 2002; Schectman, Schroth, Verme, & Voss, 2003). Spouse (2001) defines coaching as having four components: on-the-job teaching, immediate assessment and feedback, supervision, and emotional support. Skill transfer for human services staff happens in much the same way as it does for children learning a sport (Fixsen et al., 2005). A coach instructs and guides the novice in the nuances of a new activity while rules are being applied and tested (on-the-job teaching). As the novice becomes more adept at the activity, the coach assesses performance and gives advice on ways to enhance the skills needed to improve (immediate assessment and feedback). Basic rules of the game are then re-explained, reinforced, and applied to new situations as they arise (supervision). All these activities are provided in the context of an emotionally supportive relationship that considers individual staff ’s needs and assets.

As in any teaching activity, coaching is often more effective if the coach/supervisor has a good working knowledge of the intervention that supervisees perform with their clients. Ideally, the supervisor has experience carrying out the specific intervention modality. A clinical supervisor might have years of experience assisting children who have experienced trauma. If the supervisee is required to implement, for example, trauma-focused cognitive behavioral therapy, and the coach/supervisor has limited experience with this approach, the quality and effectiveness of coaching may be limited.

Implementation evaluations should focus on the method and frequency of staff education. There is nothing wrong with off-site training and workshops for the purposes of raising awareness of practice issues, meeting basic continuing education requirements, exploring new practice models, or reinforcing ethical practices. If training is primarily defined as off-site workshops, with no on-site opportunities for immediate feedback and practice, the likelihood that practices will be implemented as intended is relatively small (Brownson, Colditz, & Proctor, 2012). Implementation evaluators should look at the basic structure of training, supervision, and feedback. In addition to off-site continuing education opportunities, ideally there should be evidence of a supervision plan that includes direct observation of staff ’s interaction on at least a semiregular basis. There should also be evidence that supervisors have given concrete feedback to staff and provided opportunities to practice skills improvement.

### Staff Adherence to Intervention Protocol

Implementation evaluation is not directly concerned about performance evaluations of individual staff. Instead, implementation evaluation is concerned with staff adherence and competence in carrying out a _specific_ treatment protocol. Adherence is the extent to which individuals implementing the intervention conform to the intervention protocol (Hogue et al., 2008). Measurements of adherence focus on the quantity or presence of prescribed treatment activities defined in an intervention manual or other established protocols for an intervention. Competence is the skillfulness of intervention delivery (Forgatch, Patterson, & DeGarmo, 2005; Stein, Sargent, & Rafaels, 2007) and includes qualities related to interpersonal communication, mastery of technical aspects of intervention, and skills in responding to individual clients. These qualities are not routinely a part of staff evaluation, but they can have a large impact on whether interventions are effective with clients. As with ongoing consultation and coaching, implementation evaluations look for evidence of a supervision plan that includes direct observation of staff ’s interaction on at least a semi-regular basis. There should also be evidence that supervisors have given concrete feedback to staff and provided opportunities to practice skills improvement.

>**Adherence and Competence in Parent-Child Interaction Therapy**
>
>Adherence is the extent to which staff complies with an established protocol. Parent–child interaction therapy (PCIT; Eyberg, Boggs, & Algina, 1995) includes several features that promote adherence to a protocol. PCIT is an evidence-supported clinical intervention for children three years through eight years of age with disruptive behavior problems. Training for PCIT is extensive, and therapists must adhere closely to the treatment manual. Much of the work involves working with parents. Each session has a list of activities to be completed; for example,    the therapist modeling behavior, parents practicing behaviors, and discussion of homework to be completed before the next treatment session. At the end of the session, the therapist completes a checklist of all the activities required to verify that each has been addressed for the session. In addition, the parent completes     a checklist with the same activities to verify they took place, such as “The therapist explained my homework assignment for this week” or “The therapist talked to me about how to give commands to my child.” Practitioners can adhere closely to protocol but still lack competency. Competence refers to the skillfulness of delivery. Practitioners with high levels of competence successfully convey warmth, empathy, and authenticity that is foundational to social work practice (Hepworth, Rooney, Rooney, Strom-Gottfried & Larsen, 2012; Murphy & Dillon, 2015). In the context of PCIT, for example, is the therapist approachable? Does she greet the family warmly when they arrive for a session? If the parent wants to discuss issues in her personal life other than parenting in a specific session, does the therapist cut her off or skillfully redirect the conversation? Is the therapist aware of other services that can help the family and assist with referrals as needed?

### Staff Diversity

Agencies should take a closer look at the demographic characteristics of their staff members and volunteers to determine whether they are similar to the demographic characteristics of the client population. In addition, it is a good idea to determine whether these characteristics consider projected changes in the client population over the next five to ten years. Having a diverse staff that roughly approximates the demographic characteristics of the client population is a meaningful ideal to pursue. Equal employment strategies can be helpful in reaching this ideal. The more genuine and rigorous the effort that goes into implementing equal opportunity standards in recruitment, hiring, and retention, the more likely it is that the agency will recruit diverse staff members and volunteers. Demographic characteristics of importance should include race, gender, ethnicity, social class, religious affiliation, age, regional background, marital status, having children or not, sexual orientation, and other characteristics related to the needs of specific populations. This type of evaluation is not necessarily intended to support the notion of pairing clients and social workers based on similar background characteristics (e.g., African American clients paired with African American staff); however, staff diversity can provide a signal to many of minority status that they belong there or are welcomed as program participants and recipients. In general, the more varied the backgrounds of staff members and volunteers overall, the greater their capacity will be to plan and implement a culturally sensitive and effective program.

### Staffing Efficiency

Staff members’ use of their time can be another focus of an evaluation. Administrators sometimes wonder how their employees spend their time. For example, how much time in a week, on average, are staff scheduled to spend in direct contact with clients, in meetings, in collaborating both with other in-house staff and staff of other agencies, in record-keeping, and so on. Often staff members are thought to see too few clients, and there may be a question about how they could use their time differently. Agencies should schedule staff and volunteer time efficiently, particularly in a human services environment with limited resources. Efficiency tends to increase the outputs and outcomes for programs. Keep in mind the distinction between staff evaluation and use of staff time. Staff evaluation refers to how efficiently staff members use their assigned schedules to accomplish tasks assigned by the agency. Staffing efficiency refers to whether management has structured staff time in such a way that they can accomplish the tasks they have been assigned efficiently. Staff members should not be held accountable by management for inefficient program designs.

### Staff Working Conditions

How satisfied are staff members with their working conditions? This is an important question that many agencies downplay. The rationale for minimizing its importance can vary. Some administrators may say that staff are paid to provide their services, so why emphasize their satisfaction? Other administrators may think that seeking to satisfy staff members could weaken their own authority. This rationale may be especially important to a top–down administrator who does not encourage staff participation in major decisions of the agency.

Nevertheless, studies have found a strong correlation between staff contentment with their working conditions and their productivity and retention. Agencies with low staff morale are usually among those with high staff turnovers. Mor Barak, Nissly, and Levin (2001) conducted a meta-analysis of twenty-five studies on retention and turnover of staff and their antecedents. The studies reported a range of staff turnover in the agencies studied of 30 percent to 60 percent in a typical year. Among the contributors to staff turnover were burnout, job stress, a lack of support from co-workers and supervisors, and other organizational conditions. Therefore, paying close attention to staff satisfaction with work conditions can be important, and staff satisfaction evaluations are among the ways to investigate such issues.

The issues revolving around working conditions and morale that make sense to periodically investigate are numerous. They are salaries and annual salary raises, availability of medical coverage for employees and family members, retirement benefits, workload issues (e.g., size of caseload), availability and quality of their supervision, opportunities for advancement, support for professional licensing, openness of administrators to staff members’ views on programs and personnel matters, availability of useful in-service and outsourced training opportunities, reasonable vacation and sick-leave policies, other family-friendly policies, and incentives and rewards for doing exemplary work. In addition, at least one study found that the challenges imposed by some client groups, such as clients with mental illness, also affect job satisfaction (Acker, 1999).

## ACCESSIBILITY OF THE INTERVENTION

As discussed previously, a major focus of implementation evaluation is how program and practice processes affect clients. Events and processes around implementation can impact interventions in ways that ultimately determine their effectiveness. How accessible is the program or practice intervention for clients and potential clients who are the intended recipients? Access is a problem if the clients from the target group originally intended as the recipients are underrepresented or tend to drop out. Access is also an issue if a specific racial, ethnic, or cultural group that needs the program’s services is underrepresented among the group of recipients.

### Outreach to Diverse Populations

Accessibility of programs is, in part, a diversity issue. Programs may be used primarily by people of one demographic characteristic, such as white and middle class, and used only minimally by other groups, such as Latinos or low-income earners. Over time, this demographic profile can become institutionalized and come to be viewed inadvertently by agency staff and the wider community as the norm. In other words, people can begin to assume falsely that the program must have been designed for this particular group. As a result, other groups may not even consider using this program.

Sometimes, programs are sensitive to diversity issues but fail to reach out to the target community. A large body of research investigates health and mental health disparities between groups related to race, ethnicity, and socioeconomic status. People who are poor or racial/ethnic minorities are less likely to receive health and mental health services, and the quality of services they receive is often inadequate. Implementation evaluation should examine the degree to which new programs reach diverse populations. This includes a written plan that defines diverse populations. Diversity can reflect race, ethnicity, income, geography, gender, sexual orientation, or other areas of difference that might affect program referrals, access, or engagement. Programs should be aware of the history of intergroup conflict in the communities in which they practice and be aware of how this history might affect some community residents’ interactions with the program. One way to reach out to oppressed groups is advertising services in neighborhoods where they predominate. Another way is to align oneself with someone who is a trusted leader or advocate for their community.

>**Adults with Developmental Disabilities**
>
>A community group wanted to serve adults with developmental disabilities having little or no social contact beyond their immediate families. They were people who simply spent all their free time in their family homes. The impetus for creating this program was that it would be uniquely available to isolated individuals in the city. The program would fit their special needs because there would be a cohort of professional staff members available and trained to help them and an agency van to transport them to and from the program. However, the sponsor of this program, a group of parents with developmentally disabled members, made very little effort to channel their resources into special marketing, recruiting, and other forms of outreach. As a result, the new program was quickly inundated by people with disabilities who were higher functioning and already had many social contacts in the community. Once these people found out about the program, they adopted it as their own, and the staff members took the path of least resistance and went along with organizing their efforts around creating social opportunities for them. The original target group was forgotten largely because it did not have a vocal advocate.

If comprehensive client records are kept, it can be relatively easy to determine whether some groups are accessing the program and others are not or if some use the program more than others. A first step in determining whether there are accessibility problems is to identify the groups of people (e.g., by race, income level, age, level of functioning) that the program has decided to serve. Next, determine the characteristics of the clients actually being served or those most likely to continue in the program beyond the initial session or stage. Finding the discrepancies between the characteristics of the two groups (those being served and those targeted to be served) can begin to reveal the characteristics of client groups that are underrepresented.

### Physical Access

Physical access is evident in different physical structures within which a program is located. Since the Americans with Disabilities Act (ADA) was signed in 1990, federal law has required most buildings to be accessible to citizens regardless of physical ability. Even when buildings are ADA-compliant, physical space can present problems for some clients. For example, a center-based program for people with early stage Alzheimer’s and their families might serve fewer clients if the program is in a large building or if parking is far from the building because this increases the chances they will become lost, confused, or agitated. Program planners cannot anticipate all the special circumstances that clients might require to overcome physical barriers; however, programs should be in facilities that minimize physical barriers and have resources available to accommodate as many people as possible, regardless of ability.

### Geographic Access

Geographic access refers to a program’s location. Geographic access plays an important role in program success. A program that is not on a known, easily accessible, and safe street poses obvious barriers. Further, the absence of common public transportation routes that travel to and from the program site are barriers. Any program located outside a downtown area or outside the main section of a town can be fraught with barriers for many people. This is a challenge for clients without access to a car, public transportation, or reliable transportation from friends or family. In some cases, well-known business and shopping centers may be the most accessible sites.

### Psychological Access

Psychological access refers to the environmental tone set by a program, that is, its sense of welcoming. Psychological access is often difficult to assess because staff behavior is interpreted subjectively. Some behaviors can be observed, for example, staff rudeness or a warm greeting to clients. An agency that does not return phone calls can deter potential clients. Other behaviors can be difficult to observe unless one is highly sensitized to the client’s perspective. Gay and lesbian people, for example, may be sensitive to subtle messages that appear judgmental or unwelcoming. An example would be asking a child with two parents of the same sex for the child’s mother’s name and father’s name. The absence of employees of the same racial or ethnic group is another possible barrier for some. A program brochure might seem exclusive if it does not mention that a specific group is among those who are eligible for services. For example, would a family service agency provide counseling to a divorced couple involved in shared custody or to a gay couple? It is important for the agency to mention in brochures and other marketing materials that they are inclusive, if in fact they are.

Psychological access is evident in many programs that focus on mental health issues or are identified as mental health agencies. Many people have misunderstandings about mental illness that make them reluctant to admit they or their family members have a need for mental health services. Often a program sponsor that attempts to reach people with such sensitivities may consider locating the program in a school, community center, or house of worship to ward off this possibility.

### Cultural Access

Cultural access issues are similar to psychological access issues. They can be subjective, subtle, and difficult to detect. However, as our society becomes more diverse, program sponsors are increasingly challenged to make special efforts to be sensitive to the cultural aspects of clients and potential clients. If a program wants to be inclusive in this regard, special considerations must be given to preparation for work with African Americans, Latinos, low-income clients of all ethnicities, and recent immigrants from around the world. Each of these groups and subgroups has religious and cultural beliefs, practices, rituals, and sensitivities that are important to consider in planning and carrying out a program (e.g., Dudley, 2016).

>**Access Issues for African Americans’ Use of Hospice**
>
>In Chapter 2, a study was described that investigated why African Americans did not use hospice services proportionate to their numbers in a specific city (Reese, Ahern, Nair, O’Faire, & Warren, 1999). The researchers’ activities began with a small qualitative study of African American pastors. This pilot study was followed by a larger quantitative study of African American hospice patients that documented their access barriers to hospice. Findings revealed that African Americans in this city preferred life-sustaining treatment (e.g., chemotherapy, resuscitation, life support) over palliative care. The respondents were also opposed to accepting terminality, planning for it, or discussing it with others. The findings of the studies were used to facilitate a social action effort to increase alternative services that reflected the priorities of this community.

A group service, for example, that emphasizes open sharing, self-disclosure, equality, and participation by all will likely find resistance from some cultural groups. The hesitation from various groups will come because of their emphasis on patriarchal families, hesitations to self-disclosure, varying beliefs about how anger should be expressed, how animated they can be in a lively discussion, how much they can confront others, how they perceive authority figures, and what they choose to talk about (Reid, 1997).

>**Barriers to Using a School Program**
>
>Child Trends  (Kennedy,  Wilson,  Valladares,  &  Bronte-Tinkew,  2007)  conducted a study of barriers to low-income children and adolescents using after-school programs. They identified five types of barriers:
>
>1. Safety, transportation, and cost: Unsafe neighborhoods, the cost of afterschool programs, and problems getting to and from a program are persistent barriers that limit participation for many children.
>2. Family responsibilities: Many adolescents have other responsibilities, such as babysitting younger siblings, preparing meals, or taking care of household chores that prevent them from participating.
>3. Desire or need to work: Many older youths take on part-time or even fulltime after-school jobs.
>4. Lack of identification with staff members: Trusting relationships between youth participants and staff members are a central feature of these programs. Children and youths often prefer staff members who are like themselves in race, gender, and experience, but the most important consideration is that staff care about children and youths and can connect with participants.
>5. Lack of interest in organized activities: Adolescents, more so than children, often have little or no interest in activities offered through after-school programs. Adolescents frequently cite boredom, a desire to relax and hang out with friends, and dissatisfaction with program activities as reasons that they would rather not participate.

Cultural access is important to consider in almost every program. Issues could include such things as a recipient’s lack of knowledge about the services provided by a program. In this regard, how well does an agency brochure or flyer explain the purpose of a program in a language that can be understood by client groups? Is the program material provided in languages other than English in communities in which there are groups of people who speak English as a second language? A lack of diversity among the staff members and volunteers could be another factor that discourages some people from seeking out a program’s services. A face that is like your own may be synonymous with being a friendly face. Any of these factors could be barriers; one way to determine how problematic they are is to openly discuss them with people of these cultures.

Accessibility barriers can go beyond the factors described here. They could be any of several things that we might suspect but cannot identify from those who hesitate to use services. In many instances, we do not know what the barriers are that keep some people from using a program or practice intervention. Yet we know that some people who need a program do not use that program. Further, we always need to be sensitive to and vigilant in seeking to learn what might get in the way of people engaging with and fully using a program. In summary, several issues pertaining to client access may need to be evaluated.

## PROGRAM QUALITY

Although virtually all implementation evaluations are interested in improving the quality of a program or practice intervention to some degree, some are especially known for their interest in program quality. When standards of quality are clearly defined and measurable, it is fairly easy to measure the performance of a program or practice approach against these standards. In some cases, however, clearly defined, minimal performance standards are not defined or do not even exist. Also, quality evaluations primarily use qualitative methodologies, and measures tend to be subjective. Thus, quality evaluations are not as exact and predictable as one would hope.

>**An Exercise in Evaluating Practice Quality**
>
>Social work practice classes at the MSW level often can get into discussions about how to implement a specific practice theory or approach, such as the solution-focused or person-centered approach. A fruitful way to explore such a question     is to select a specific client case, real or made up, that one group could role-play (after preparation) using the solution-focused approach and another group using the person-centered approach. Other classmates could observe the role-plays and attempt to identify specific techniques and behaviors that reflected the respective approaches and those that did not. Afterward, the entire class could summarize the salient elements in each approach that were manifested in the role-plays.

### Examples of Types of Quality Evaluations

Common models of quality evaluations described in Chapter 4 included accreditation studies of programs and quality assurance evaluations. A fuller discussion of these types of evaluations will help illustrate the complexities of evaluating a program’s quality.

### Accrediting a Professional Program

The process of accrediting a professional academic program, such as a professional social work program, is one form of quality control. The accrediting agencies propagate professional standards that academic programs are expected to adopt and implement. Member agencies typically are expected to prepare lengthy reports, referred to as “self-studies,” to document how they meet these standards in their programs and in their administration. The self-study report is submitted to the accrediting agency, which assigns a team of accreditation officials to carefully review the self-study and then conduct a site visit of the program. The purpose of site visits is to find multiple sources of evidence that the member agencies are actually doing what they report in the self-study. Multiple sources of evidence could include random samples of student records; informal observations; data on outcomes for graduates; and eliciting of the views of administrators, staff and faculty members, field instructors, and students. The standards of an accrediting agency tend to be broad and subjective.

>**Example of the Accreditation Process**
>
>The Council on Social Work Education expects all professional programs at both  the BSW and the MSW level to, for example, prepare their students with content on social and economic justice. Programs are expected to show how they do this  in the self-study and in each syllabus through lectures, outside speakers, assigned readings, assignments, and other methods. If the self-study and each individual syllabus show how and where social justice content is covered and how students are expected to demonstrate that they understand and apply  it  in  their  practice, the standard is essentially met. However, this form of quality control is quite subjective and leaves open the possibility of many unanswered questions. For example, what types of social justice are acceptable? How does the accreditation agency know that the content has been covered in every section of a course? How can faculty determine that students have embraced a belief system that supports this content? How can they know that graduates will use the content in their work once they have graduated?

### Quality Assurance

Quality assurance programs exemplify a focus on quality evaluations. Some agencies also sometimes refer to quality assurance as quality improvement or quality control. Quality assurance activities focus on a sampling of events and records that provide a snapshot of how the program and its staff members work. The key to quality assurance is determining whether an acceptable level of quality is reached. In a practical sense, the results of quality assurance data collection efforts can be immediately used to improve or fill omissions in a program. A case example was given in Chapter 4 of a quality assurance evaluation protocol of an agency providing residential programs for youth in foster homes. The example described the specific procedures that the agency used to conduct the evaluation.

Social agencies that conduct a quality assurance evaluation typically take a close look at several aspects of a program using multiple methods of data collection. These methods could include staff peer reviews of a random sampling of client cases, a close-up review of some client records, inquiries into client perceptions and satisfaction, and observations of a few services. Quality assurance is usually conducted by a team of staff members, not agency administrators, who are employed either by the agency being evaluated or by another agency. Because the process involves staff members, there is likely a strong tendency that reviewers are supportive of staff members and possibly biased in favor of their viewpoints and practices.

## CLIENT SATISFACTION

Client satisfaction studies investigate how satisfied clients are with the services they receive. Such studies are extremely important to conduct. They reveal clients’ perceptions of the services they are receiving, what they feel works, and what may be problematic. These studies can help pinpoint what was helpful to clients, where there might be a breakdown in services, and where improvements may need to be considered. Client satisfaction questions also offer a fuller picture of the interface between the clients and their service provider. They help agencies determine how each party (both client and social worker) perceives what is happening and any discrepancies between the two. In this case, a concurrent study of the social worker’s perceptions would also need to be conducted using the same or similar questions.

There is often a close correlation between whether clients are satisfied with a program and whether the program is effective in helping them. This is likely to be the case because if clients are not satisfied with the services they receive, one of several possibilities is likely. Dissatisfied clients may not trust their providers or the services they are offering, and therefore they may not fully engage in using them. Dissatisfied clients may throw out various obstacles to receiving services, such as withholding information, minimizing participation, avoiding in-depth interventions, sporadic attendance, or even discontinuing use of the program.

In addition, client satisfaction studies can reveal, in part, how effective programs are, if client satisfaction is viewed as a necessary though not sufficient condition for claiming program effectiveness. It is the authors’ view that if there is an absence or low level of client satisfaction generally, it is difficult to conclude that a program has been effective in helping them. Involuntary clients may be an exception, in that their lack of satisfaction is likely to be related to their involuntary status. In this case, a report of program dissatisfaction presumably relates more to their status than to any of their accomplishments or progress. In some studies, client satisfaction scores are even viewed as an outcome measure of success. In summary, the client’s perceptions are always important to consider, even though they are a subjective viewpoint that is influenced by their perspectives and biases.

>**Satisfaction of Clients in an Inpatient Psychiatric Facility**
>
>Baker, Zucker, and Gross (1998) report that client satisfaction studies are rare among inpatient mental health patients. Their 770 clients had serious and persistent mental illness and, in most cases, had schizophrenia and were involuntarily hospitalized. The authors explored, among other things, the clients’ perceptions about the different treatment modalities used, treatment goals, and the philosophy of treatment. The authors discussed several challenges revolving around conducting client satisfaction surveys with this type of population, including considerations of what aspects of satisfaction should and can be measured, whether such surveys can reflect client stability in satisfaction, and whether the results can be used for program improvement.

Although client satisfaction studies are important to have as a component of virtually every program and practice intervention, they do have their weaknesses. As already indicated, clients’ perceptions are bound to be subjective and perhaps biased. They can also be difficult to interpret. If you were to ask ten clients, “How satisfied are you with social work services?” what would it mean to each client? One person may associate “satisfaction” with one image, while another person associates it with something altogether different. For example, satisfaction or lack of satisfaction in a social worker’s help could mean any number of things:

- Liking or disliking some characteristic of the social worker
- Being unhappy with the initial waiting period
- Feeling angry or disappointed about what the social worker said during a recent session
- Being disrupted by a change to a new social worker
- Being grateful that the social worker helped them to find a resource
- Being pleased that the current social worker is not judgmental like the previous one
- Being comforted that the social worker listens intently

The list of possible interpretations can be almost limitless, which suggests that we may never know what clients mean when they check a specific response category of a satisfaction survey.

A related example of a weakness in client satisfaction studies revolves around interpreting what the word _satisfaction_ means to the clients who fill out a survey. Because the term is ambiguous, it will likely need to be defined. It could be interpreted in various ways, for example, as “no major complaints,” “being acceptable,” or “being preferred over similar services of competing agencies.” Satisfaction to some may mean a program or practice that meets a very high standard overall, such as being exceptionally well delivered, having almost every aspect done well, being reasonable in cost, and being offered at the best time and at a convenient location for the client. In contrast, to others the standard of satisfaction may be very low, such as simply being pleased to receive an opening in the program and being treated in a friendly way. Again, a client satisfaction study may never uncover what standard of satisfaction the respondents use.

Of course, one could respond to the point about the ambiguity of satisfaction by saying that it does not matter what the standard is. It is all about perception and the perceiver. If the client perceives the program or service as satisfying, then that is all that counts, particularly if satisfaction means they will continue to use the program and continue to engage the social worker in the helping process.

### Options for Determining What Satisfaction Means

Keep in mind that there are options for exploring a client’s satisfaction in more detail or depth. One option is to use a questionnaire format that asks forced-response satisfaction questions for each of several aspects or dimensions of the program and its services. It is up to the evaluator to decide which program dimensions are most important to include in the study. Examples of program dimensions include asking whether clients are satisfied with the agency’s fee schedule, the extent of the waiting period, the psychological accessibility of the agency, whether services are available at convenient times, the friendliness of the receptionist, the social worker’s ability to listen, and the social worker’s ability to help them find solutions to their problems. When analyzing the responses to these questions, the evaluator can zero in on the dimensions of the program that are more and less satisfying by comparing them. Further, if one or two dimensions are particularly troubling for clients, they can be singled out and addressed in this type of instrument, which may lessen the impact that the troubling feelings have on their responses to questions about the other program dimensions.

>**The Dimensions of Satisfaction Used by a Nursing Home Admissions Department**
>
>Family members were asked five questions about their satisfaction with admissions when admitting their loved one to a nursing home (Huntersville Oaks Nursing Home, 2005). These five questions addressed the following five dimensions of admissions:
>
>1. Support provided
>2. Information regarding financial issues
>3. The orientation to the nursing home
>4. The amount of information provided
>5. Overall assistance given
>
>A five-point Likert scale (excellent, very good, good, fair, poor) was used to frame the questions.

Typically, a client satisfaction questionnaire with forced-response questions has one or two open-ended questions at the end of the instrument. These questions provide an opportunity for the respondent to comment on something that the other questions did not address. The open-ended questions could be simply stated as “Please share any additional comments” “What are you satisfied with the most?” or “What are you least satisfied with?”

>**Example of Another Way to Explore Client Satisfaction**
>
>In the case of one client satisfaction study (Dansky, Colbert, & Irwin, 1996), two additional questions were asked:“ Would you recommend this program to a friend ” (yes, not sure, no) and “Would you return to this agency in the future if the need arose” (yes, not sure, no). Both questions get at client satisfaction with respect to telling others and returning in the future.

Another option is to use an unstructured interview format with open-ended questions about satisfaction. In this case, clients would be encouraged to respond to the questions in their own words as naturally as possible. Probing would be added as needed when clients’ responses were not fully clear or needed elaboration. A qualitative interview may take a fair amount of time, perhaps an uninterrupted period of an hour, in a place that feels hospitable to clients. An interviewer could be someone known by the clients or a stranger. Both choices have advantages and disadvantages. A person who is known to clients and identified with the agency sponsor would be able to establish rapport more quickly and possibly ask questions within the context of the specific helping process. A disadvantage of using a familiar interviewer is that clients may be hesitant to share negative responses for fear of jeopardizing their standing as clients. A stranger may have more challenges establishing rapport because he or she would be unfamiliar to the clients; a stranger, though, may also have advantages eliciting an honest set of responses if clients are assured that their responses will be kept confidential.

Occasionally, a qualitative questionnaire has been used to determine satisfaction. In one study, forty children of divorced parents were asked to share their perceptions of a family-in-transition program in which they participated (Oliphant, Brown, Cambron, & Yankeelov, 2002). They were asked to respond to open-ended questions about the usefulness of the program in helping them cope; their feelings, experiences, and ideas about the program; and additional topics that they would have liked to have covered. They were also asked to list specific things that helped them. A qualitative questionnaire, while relevant with many types of clients and in a variety of circumstances, may also be too open-ended, time-consuming, and challenging to complete. A lower response rate may also result. Another option, of course, is to combine two or more methods. Possibly the structured questionnaire could be administered first, followed by an unstructured interview.

The decision about which type of instrument to use depends on whom is being studied, the costs, and time available. For example, the best fit for a satisfaction study of people who are unable to read would be an interview rather than a questionnaire, whereas a phone interview, an interview sent as an e-mail, or a mailed questionnaire may be the best fit for a regional study of clients who live some distance from the evaluator.

>**Example of a Client Satisfaction Interview for Children**
>
>Prior, Lynch, and Glaser (1999) reported on a client satisfaction interview with children who were in a child sexual abuse program. The interview schedule used included both quantitative and qualitative questions. The children were asked to rate the social work services on several dimensions (listening and talking, providing information and explanation, social worker’s attitude and demeanor, continuity and accessibility, and special occasions) using a three-point scale of positive, neutral, and negative. In each of these cases, the children were asked to elaborate on their answers. For example, one thirteen-year-old girl elaborated on her positive response about the social worker listening by saying, “If I didn’t want to answer, which I sort of had to, she wouldn’t force me to, she’d just go on to the next question, she wouldn’t ask me to think.”

### Administering and Collecting Responses to a Client Satisfaction Instrument

It is often wise to alert clients that a client satisfaction survey is coming before it is handed out. Some agencies send a note or a postcard out to a client informing them of the study, explaining the purpose of the survey, stating how important clients’ feedback is in evaluating the program’s effectiveness, emphasizing confidentiality, and thanking them ahead of time. If the clients use e-mail, they could receive an email with the same message and perhaps be asked if they would be willing to respond to a questionnaire at a later time via e-mail. Or both the mail and e-mail could be used giving the client the option of using either one. One agency even offered to give clients a small gift the next time that they came to the agency as an incentive for filling out the survey. Such gestures often may be important in maximizing clients’ interest in participating. Further, such preparatory steps are often viewed as signs of courtesy and recognition of the value of clients’ time.

>**Example of Multiple Use of Satisfaction Surveys**
>
>An agency providing group homes to people with developmental disabilities regularly uses several satisfaction surveys to obtain feedback on how the agency is doing and how it can improve its programs (Lori Gougeon, executive director of Residential Support Services, Charlotte, NC, personal communication, January 21, 2005). This agency has a client satisfaction interview that is conducted with every client annually, using students and volunteers as interviewers. They also have a satisfaction questionnaire administered voluntarily to staff members and another questionnaire administered to all family members of clients. Finally, the agency conducts an annual satisfaction interview with several key community representatives who are familiar with the agency. The community representatives vary each year and have included representatives of other agencies, landlords and employers of clients, regular volunteers, leaders of civic associations, church members, and store clerks.

How the client satisfaction study is administered is also important to consider for the evaluator to have valid and reliable data. The person administering the questionnaire or interview needs to be well prepared and trained. In this regard, the person assigned to hand out a questionnaire to the clients is often not directly involved in the evaluation process for practical reasons. Usually it is not realistic for the evaluators or their assistants to be available to give it out to every client. Those assigned to hand it out could simply be asked to hand out the survey and remind the client to fill it out and return it. They could be a receptionist, secretary, a volunteer, another staff member, or even a manager who is uninformed about it.

In one example, one of the authors (Dudley) encountered a receptionist at a medical clinic who handed him a patient satisfaction questionnaire and asked him to fill it out before he even saw his physician. The receptionist apparently did not know what the questionnaire was all about, so the author filled it out imagining what the visit with the physician would be like. He did not contest this inappropriate request because he did not want to avoid any delay in seeing the physician.

Other examples of mistakes and overt biases that have been evident in administering client satisfaction surveys include identifying clients to fill out a survey only after they have communicated a favorable verbal impression of the program provider, failing to adequately explain the purpose of the satisfaction survey, placing completed surveys in an open pile that violates privacy rights, requiring clients to fill out the survey rather than giving them the choice to participate, and looking over the results of clients’ surveys in their presence.

>**What Students Learned from Conducting a Satisfaction Interview**
>
>Four graduate students conducted client satisfaction interviews with several people with mental retardation who lived in group homes. Afterward, they shared their experiences and what they learned with their class. Among the things that they learned were the following:
>
>- Open- and closed-ended questions elicited very different, sometimes contradictory responses.
>- Probe questions following interview questions can easily influence the nature of client responses, suggesting that probes should be standardized.
>- Meeting in private is important because some of the questions were about their daily lives on a very personal level.
>- The location of the interview in group homes made a big difference in responses (choices were the living room, the person’s bedroom, and the office where the staff member usually works).
>- In these instances, it became quite evident how much the staff member became an “interpreter” by speaking for the respondent, interfering with what he or she said, and having an influence merely by being present.
>- Some questions such as “What makes you happy?” brought more meaningful responses than most other questions.

Maximizing the chances of a high response rate by clients is another factor to consider. Because interviews are conducted with an interviewer present, their response rates are usually much higher than when using a questionnaire. How a questionnaire is introduced is critical. In terms of informed consent, all the necessary aspects of informed consent (e.g., purpose of the survey, confidentiality or anonymity, option to not participate, explanation of how results will be used, any potential harm or benefits from participation) should be shared, as both an introduction stated on the questionnaire and verbally by the person who administers it. Having a relatively quiet and private location for filling out the questionnaire is also important.

Another issue is how the survey is to be returned. Is it to be returned by mail? If so, it is necessary to include a self-addressed, stamped return envelope, with a reminder in large print to return it within a specific time, not to exceed seven to ten days from receipt. Furthermore, a follow-up reminder card or call helps increase the response rate in many cases. If the questionnaires are to be completed at the agency site, allowing time to fill them out and having someone designated to collect them are necessities.

Questionnaires sent via e-mail have some obvious advantages such as having the simplicity of returning them by merely clicking a command on the keyboard. Also, they can be filled out quickly using a computer and completed in the privacy of their home. Further, once returned, the responses can be automatically organized, entered, and ready for data analysis, thereby skipping over the steps of coding and entering the data manually. Disadvantages of using e-mail are evident as well such as not having a computer or not being able to comfortably open the program containing the questionnaire and filling out the responses. If e-mail is being considered for filling out a questionnaire, steps need to be taken to carefully determine whether the respondents have access to a computer, are capable of using it, and are willing to respond to a questionnaire in this way. It may be best to offer both options and let each respondents decide what they want to do.

## EVALUATING PRACTICE PROCESSES: SOME ADDITIONAL THOUGHTS

Many of the preceding types of evaluations could naturally focus on either programs or practice, even though the discussion so far has been mostly illustrating programs. More information on evaluating the implementation of practice approaches is presented next related to several topics. They include refining the practice approach, monitoring practice, accessing practice, and client satisfaction with practice.

### Refining and Individualizing the Practice Approach

Practice approaches usually are defined by several interrelated principles that are normally evident in their implementation. However, nuances in implementing an approach are important to consider so that the approach fits the individual needs of each client. The logic model helps us focus on the sequence of steps that link the implementation of a practice approach back to the clients’ unmet needs and forward to the clients’ anticipated outcomes. In this regard, the practice approach should address the problems and needs of their recipients and the underlying causes. Further, the implementation of a practice approach should result in the clients achieving their anticipated outcomes.

While much of the decision-making about what the practice intervention should be occurs prior to the implementation stage, continued work on a chosen practice approach is needed during implementation to make sure that it responds effectively to the individual needs of each clients. For example, how many sessions are needed will likely vary for each client. Also, some principles and techniques of the approach may need to be emphasized with one client and not another. One client may benefit from an approach that helps them gain insight into their problems, while another may need more emphasis on support rather than insight.

Unfortunately, some practitioners may apply the same approach rigidly with every client rather than sensitively varying their practice to the clients’ individualized needs. In this regard, one purpose of a practice evaluation can be to explore variations in the implementation of a practice approach and how variations may be linked to different types of client needs and outcomes. It may well be that some staff members use a specific practice theory without modifying it to what different clients need. In these instances, staff training and supervision can alert the worker to the problems inherent in doing this and the importance of tailoring the approach to individual clients. By continually having a special awareness of the implementation of the practice approach and discussing these processes with the clients, refinement of the intervention can be facilitated so that it can be most effective in helping each client reach their outcomes.

### Monitoring Practice

Practice quality is closely relatedtoprogram quality, asthequality ofanoverall program depends on the quality of each service component, usually the services of individual staff members. Monitoring practice is a discipline that can help practitioners improve what they do. Social work practitioners often monitor the quality of their practice; some of their efforts may not even be thought of as evaluations.

Process recordings are an example of monitoring practice. Supervisors and practice teachers often request process recordings of new staff members and field students to find out what happens in their interviews. The recordings are then carefully reviewed in a supervisory session to explore the appropriateness of the social worker’s responses to the client’s comments. Often the recordings are helpful for revealing missed opportunities for the worker to respond to the subtle messages and emotional reactions of clients. The supervisory discussions can be very useful in helping new workers and students become more disciplined in how they use themselves in practice situations and in providing supervisors with enough information to be satisfied that clients receive satisfactory help.

Direct observations of practice through a one-way mirror or by videotaping and audio taping interviews are other ways to evaluate practice interventions through the supervisor–worker relationship. Other devices such as case summaries and personal logs of staff members can also be used to help practitioners monitor their own practice and receive helpful feedback from a supervisor. Discussion of case summaries of sessions can lead to planning future interviews and to modifications in a worker’s approach.

Case managers carry a specific role that emphasizes monitoring the practice of other providers of service to a client (Frankel & Gelman, 2016). Social workers at the BSW level are especially equipped in their education to fill these positions. Case managers typically locate services for their clients, negotiate and facilitate contracts with agencies offering services, and monitor the range of services provided to the clients to ensure that they are appropriate to their needs and provided in a high-quality way.

### Practice Accessibility

Problems of practice accessibility are inherent in an individual’s practice, as they are in programs. Practitioners frequently face no-shows and early dropouts among their clients, which often points to accessibility barriers that need addressing. For example, prospective clients may explore the services of a practitioner by phone and agree to come to an initial interview. Yet they may neither show up nor call in to explain. Often clients come to an initial interview or the first session of a group but do not return. Unless these no-shows and early dropouts are followed up to find out what happened to them, practitioners cannot know whether accessibility barriers existed that could be overcome.

>**The Wrong Motives for Dealing With “No-Shows”**
>
>One mental health agency decided to address its high number of no-shows with two strategies. First, clinicians were encouraged to double-schedule appointments in hope that at least one of the two clients scheduled for each time slot would show up. If both clients showed up, the program director promised to find someone to meet with the second client. The other strategy was to employ a decision committee, which a client who had missed three or more appointments had to talk     to before being scheduled for another session. The decision committee was used to stress clients’ responsibility to attend all appointments and to inform clients  that additional sessions would not be offered if they missed another session. The decision committee was not used to seriously explore the clients’ reasons for noshows. Unfortunately, both of these strategies were motivated by an administrative need to generate a maximum number of client reimbursements for their private, for-profit agency.

No-shows and dropouts can result from accessibility barriers, which should be carefully considered and addressed. Otherwise, the circumstances that cause or exacerbate the barriers could easily continue for prospective clients in the future. Some types of clients may easily use the services of a practitioner or program (e.g., a middleclass, married client, experienced in using a multitude of services, and who generally copes well with and manages personal issues). Others, possibly racial or ethnic minorities, low-income families, recent immigrants, single parents, unmarried and inactive parents, older lesbians, people with AIDS, people with chronic mental illness, and people with numerous other challenging characteristics are known to have been excluded or discouraged from using services. Studies have shown, for example, that no-show behavior can be correlated with low income (e.g., Allan, 1988), lower socioeconomic status (e.g., Lefebrve, Sommerauer, Cohen, Waldron, & Perry, 1983), age (e.g., Carpenter, Morrow, Del Guadio, & Ritzler, 1981), and substance abuse (e.g., Paolillo & Moore, 1984), among other factors.

Concluding that clients and prospective clients are simply resistant or unmotivated to use services is a frequently given excuse and a superficial way to ignore some of the more complicated and valid explanations. Some more valid explanations can be explored by considering the types of barriers described previously in the chapter (e.g. physical, psychological, cultural, geographic, institutional).

Practitioners are encouraged to devise a plan to address accessibility barriers when they face a considerable number of no-shows or dropouts. Meyer (2001) offers some suggestions. One barrier may be the wait time before a first visit, as the longer the wait time to obtain a first appointment, the less likely a client is to keep the next appointment. Thus, workers should minimize any wait time initially or before any session. Another barrier could be the unreasonable expectation that clients accomplish speedy outcomes because of the growing pressures on practitioners to provide short-term assistance and address only immediate problems.

Another concern, according to Meyer (2001), may be that the client’s problems with service providers in the past were barriers because former practitioners may have been unresponsive or unhelpful. In this case, special efforts may be needed to help clients openly talk about their negative past experiences, and they may need to be reassured that such experiences will not be repeated. Another suggestion is that the practitioner demonstrate the qualities of the therapeutic relationship from the very first contact, whether by phone or in person; these qualities include reliability, trustworthiness, calmness, respect, good listening, warmth, empathic responses, and not rushing. Well-thought-out and sensitive follow-up efforts are also encouraged when a client does not show up for an appointment. Such inquiries could include asking a client in a direct, caring but nonjudgmental way why he or she missed the appointment and exploring how the client wants to proceed.

Staudt, Bates, Blake, and Shoffner (2004) offer further suggestions for preventing no-shows. Besides emphasizing with clients such central practice qualities as conveying a nonjudgmental attitude and clearly explaining the confidentiality policy, they suggest some helpful practice notions that could enhance the possibility that clients will return for their second and later interviews. These practice techniques include contracting for a set number of sessions, eliciting what the client wants to get out of treatment, asking clients what might prevent them from returning, and educating clients about their role and the role of the social worker.

### Client Satisfaction with Practice

Obtaining feedback from clients on their satisfaction is important to consider not only in programs but also in practice. Social workers can be encouraged to periodically have informal discussions with clients or ask them to fill out questionnaires to elicit their feedback on how helpful the services have been and what can be done to improve them. A social worker can ask clients several basic questions, such as the following:

- How satisfied are you with the services that you have been receiving?
- Are you feeling satisfied generally? In what ways?
- How might you be feeling dissatisfied or disappointed? In what specific areas?
- What am I doing that is helpful to you? How is it helpful?
- What additionally would you like me to do?
- What am I doing that is not helpful to you?
- How am I helping you reach your goals?
- How do you feel about the session today (a good overall question at the end of each session)?

>**Client Satisfaction Using a Focus Group**
>
>A graduate student (Borys, 2006) decided to conduct a client satisfaction study with her clients to find out what the aftercare residents of a substance abuse agency for women thought were the strengths and weaknesses of their program. She used a focus group format and asked the following questions:
>
>1. What do you feel are the strengths of the program?
>2. What do you see as some areas that need to be improved?
>3. What part of the program have you struggled with the most?
>4. If you could change anything about the program, what would it be?
>
>In brief, a consensus was reached about bringing back a residential program that had been discontinued; providing more advocacy for housing, jobs, and day care; having a more supportive staff; and involving board members more in advocating for the residents.

Client satisfaction is especially important to explore at the time that a client is terminating services. Yet it is strongly advised that client satisfaction explorations also occur periodically while services are still being offered. In this way, changes can be incorporated into a worker’s practice during sessions that follow. In addition, periodic client satisfaction conversations can create greater bonding with clients because clients feel they are being heard and that their voice is important.

## SUMMARY

The chapter is devoted entirely to the evaluation issues at the implementation stage of a program or practice intervention. During the implementation stage, many overall evaluation questions can be asked and answered that revolve around the theme of how well the program or practice intervention are working. They are (1) Is the intervention linked to the clients’ problems? (2) Is the intervention implemented as initially proposed? (3) Is the intervention evidence-based?

(4) several questions about staff members, (5) Is the intervention accessible? (6) Is the intervention implemented at a high level of quality? and (7) Are the client who are receiving the services of the intervention satisfied with it? Numerous implementation studies are described illustrating how each of these questions can be addressed. Another section of the chapter goes into more depth about how implementation studies can be helpful in evaluating practice interventions. They include exploring refinement o f a practice approach based o n t he individualize needs of each client, monitoring practice, overcoming access problems when using practice interventions, and using client satisfaction studies to evaluate practice interventions.

## DISCUSSION QUESTIONS AND ASSIGNMENTS

1. Identify one key element of a program at your field agency that is necessary for it to be effective in addressing the causes of clients’ problems. Figure 5.1 in Chapter 5 lists some of these elements. Then ask three different staff members to identify how these elements are present and operable in the program.
2. Review Figure 1.2 in Chapter 1 on the “Contextual Factors in Evaluations.”. Look closely at the factors comprising the “Agency” and “Agency Environment” in the diagram. In your opinion, what are two or three of the factors that are essential to consider for the effective functioning of a program? List one or two that may be optional. Give reasons for your answers. Which factors are essential for accountability? Which are important in responding to the sociocultural characteristics of a program? Give examples.
3. Assume that you work for a child welfare agency that previously provided protective services to families with child neglect and abuse. Now the agency is preparing to provide secondary prevention services to families who may be most prone to neglecting their children resulting from a range of potential problems. Identify a list of questions that you can ask families to determine which families would be most qualified to be clients of this secondary prevention program.
4. You plan an education symposium on preventing strokes for the older adults in your community and hold the symposium at the local hospital. You are pleased that the symposium has a good turnout of older and middle-aged adults but as you look around the room you see almost no African American adults present. You know that African Americans have a higher incidence of stroke than other racial groups. Design a plan for another community education program that will reach this underrepresented group of older adults.
5. Review the two strategies used to address clients who were no-shows in “The Wrong Motives for Dealing with No-Shows” box in this chapter. What is ethically wrong with these strategies? How, if at all, could either strategy be modified in some way to make it ethical and possibly effective?
6. Conduct a role-play of a client satisfaction exploration between a social worker and a client after meeting for five sessions. Make up some details about the client’s needs and the services provided by the social worker. You can use some of the questions for the social worker found in the chapter in the sections on “Client Satisfaction” and you can use the following questions if you wish.

    1. Generally, how are we doing?
    2. How am I doing?
    3. What’s helpful?
    4. What’s not so helpful?
    5. What would you like to have that you are not getting? 

Now add the following variations in five new interviews:

1. A client is Asian American and tends to defer to the person in authority.
2. A client is from the Southern region of the United States and tends not to want to directly question the person in authority; the client tends to be less direct and less open but has some complaints to share.
3. Vary the client’s age (e.g., young adult, older adult).
1. A client is the victim of domestic violence.
2. A client is a perpetrator of domestic violence in an involuntary setting.

## REFERENCES

- Acker, G. M. (1999). The impact of clients’ mental illness on social workers’ job satisfaction and burnout. _Health and Social Work, 24_(2), 112–119.
- Allan, A. T. (1988). No-shows at a community mental health center: A pilot study. _The International Journal of Social Psychiatry, 34_(1), 40–46.
- Americans with Disabilities Act of 1990. Pub. L. 110-325.
- Baker, L., Zucker, P., & Gross, M. (1998). Using client satisfaction surveys to evaluate and improve services in locked and unlocked adult inpatient facilities. _Journal of Behavioral_ _Health_ _Services Research, 25_(1), 51–68.
- Borys, L. (2006). _Focus group assignment for a program evaluation course_. Unpublished manuscript, University of North Carolina at Charlotte.
- Brodkin, E. Z. (1997). Inside the welfare contract: Discretion and accountability in state welfare administration. _Social Service Review, 71_(1), 1–33.
- Brownson, R. C., Colditz, G. A., & Proctor, E. K. (2018). _Dissemination and implementation research in health: Translating science to practice_ (2nd ed.). New York, NY: Oxford University Press.
- Carpenter, P., Morrow, G., Del Guadio, A., & Ritzler, B. A. (1981). Who keeps the first outpatient appointment? _American Journal of Psychiatry, 138,_ 102–105.
- Cohen, J. A., & Mannarino, A. P. (2008). Trauma-focused cognitive behavioral therapy for children and parents. _Child and Adolescent Mental Health, 13,_ 158–162.
- Dansky, K. H., Colbert, C. J., & Irwin, P. (1996). Developing and using a patient satisfaction survey: A case study. _Clinical and Program Notes, 45,_ 83–88.
- Dixon, L., Lyles, A., Scott, J., Lehman, A., Postrado, L., Goldman, H., & McGlynn, E. (1999). Services to families of adults with schizophrenia: From treatment recommendations to dissemination. _Psychiatric Services, 50_(2), 233–238.
- Dudley, J. (2016). _Spirituality matters in social work: Connecting spirituality, religion, and practice._ New York, NY: Routledge/Taylor & Francis.
- Durlak, J. A., & DuPre, E. P. (2008). Implementation matters: A review of research on the influence of implementation on program outcomes and the factors affecting implementation. _American Journal of Community Psychology, 41,_ 327–350_._
- Eyberg, S. M., Boggs, S., & Algina, J. (1995). Parent–child interaction therapy: A psychosocial model for the treatment of young children with conduct problem behavior and their families. _Psychopharmacology Bulletin, 31,_ 83–91.
- Fischer, R. L. (2002). Gaining access to one’s children: An evaluation of a visitation program for noncustodial parents. _Families in Society, 83_(2), 163–174.
- Fixsen, D. L., Naoom, S. F., Blase, K. A., Friedman, R. M., & Wallace, F. (2005). _Implementation research: A synthesis of the literature_ (FMHI Publication #231). Tampa, FL: University of South Florida, Louis de la Parte Florida Mental Health Institute, The National Implementation Research Network. Retrieved from [https://fpg.unc.edu/node/4445](https://fpg.unc.edu/node/4445)
- Forgatch, M. S., Patterson, G. R., & DeGarmo, D. S. (2005). Evaluating fidelity: Predictive validity for a measure of competent adherence to the Oregon model of parent management training. _Behavior Therapy, 36,_ 3–13.
- Foster, J., & Hope, T. (1993). _Housing, community and crime: The impact of the Priority Estates Project_. London, England: HMSO.
- Frankel, A. J., & Gelman, S. R. (2016). _Case management: An introduction to concepts and skills_ (3rd ed.). New York, NY: Oxford University Press.
- Gardner, F. (2000). Design evaluation: Illuminating social work practice for better outcomes._Social Work, 45_(2), 176–182.
- Glisson, C., & Green, P. (2006). The effects of organizational culture and climate on the access to mental health care in child welfare and juvenile justice systems. _Administration_ _and Policy in Mental Health and Mental Health Services Research, 33_(4), 443–448.
- Hallfors, D., & Cho, H. (2007). Moving behavioral science from efficacy to effectiveness._International Journal of Behavioral Consultation and Therapy, 3_(2), 236–249.
- Hepworth, D. H., Rooney, R. H., Rooney, G. D., Strom-Gottfried, K., & Larsen, J. H. (2012)._Direct social work practice: Theory and skills_ (9th ed.). Belmont, CA: Brooks/Cole.
- Hogue, A., Henderson, C. E., Dauber, S., Barajas, P. C., Fried, A., & Liddle, H. A. (2008). Treatment adherence, competence, and outcome in individual and family therapy for adolescent behavior problems. _Journal of Consulting and Clinical Psychology, 4_(4), 544−555.
- Huntersville Oaks Nursing Home. (2005). _Nursing home admissions satisfaction survey._Huntersville, NC: Author.
- Joyce, B., & Showers, B. (2002). _Student achievement through staff development_ (3rd ed.).
- Alexandria, VA: Association for Supervision and Curriculum Development.
- Kennedy, E., Wilson, B., Valladares, S., & Bronte-Tinkew, J. (2007). Improving attendance and retention in out-of-school time programs. _Child Trends_, Publication 2007-17.
- Lefebrve, A., Sommerauer, J., Cohen, N., Waldron, S., & Perry, I. (1983). Where did all the “no-shows” go? _Canadian Journal of Psychiatry, 28,_ 387–390.
- Lieberman, A. F., & Van Horn, P. (2004). _Don’t hit my mommy: A manual for child parent psychotherapy with young witnesses of family violence_. Washington, DC: Zero to Three. Lieberman, A. F., & Van Horn, P. (2008). _Psychotherapy with infants and young children: Repairing the effects of stress and trauma on early attachment_. New York, NY: Guilford.
- Lieberman, A. F., Van Horn, P., & Ippen, C. (2005). Toward evidence-based treatment: Child Parent Psychotherapy with preschoolers exposed to marital violence. _Journal of the_ _American Academy of Child and Adolescent Psychiatry, 44_(12), 1241–1248.
- Meyer, W. S. (2001). Why they don’t come back: A clinical perspective on the no-show client. _Clinical Social Work Journal, 29_(4), 325–339.
- Mitchell, P. F. (2011). Evidence-based practice in real-world services for young people with complex needs: New opportunities suggested by recent implementation science. _Children and Youth Services Review, 33,_ 207–216.
- Mor Barak, M. E., Nissly, J. A., & Levin, A. (2001). Antecedents to retention and turnover among child welfare, social work, and other human service employees: What can we learn from past research? A review and metanalysis. _Social Service Review, 75_(4), 625–661.
- Murphy, B. C., & Dillon, C. (2015). _Interviewing in action in a multicultural world_ (5th ed.). Stamford, CT: Cengage.
- Oliphant, E., Brown, J. H., Cambron, M. L., & Yankeelov, P. (2002). Measuring children’s perceptions of the ‘Families in Transition’ Program (FIT): A qualitative evaluation. _Journal_ _of Divorce and Remarriage, 37_(4), 157–164.
- Paolillo, J. G., & Moore, T. W. (1984). Appointment compliance behavior of community mental health patients: A discriminant analysis. _Community Mental Health Journal,_ _20_(2), 103–108.
- Pawson, R., & Tilley, N. (1997). _Realistic evaluation._ Thousand Oaks, CA: SAGE.
- Prior, V., Lynch, M. A., & Glaser, D. (1999). Responding to child sexual abuse: An evaluation of social work by children and carers. _Child and Family Social Work, 4_, 131–143.
- Proctor, E., Knudsen, K., Fedoravicius, N., Hovmand, P., Rosen, A., & Perron, B. (2007). Implementation of evidence-based practice in community behavioral health: Agency director perspectives. _Administration and Policy in Mental Health, 34,_ 479–488.
- Proctor, E., Silmere, H., Raghavan, R., Hovmand, P., Aarons, G., Bunger, A., . . . Hensley,
- M. (2010). Outcomes for implementation research: Conceptual distinctions, measurement challenges, and research agenda. _Administration and Policy in Mental Health and_ _Mental Health_ _Services Research, 38,_ 65–76.
- Reese, D., Ahern, R., Nair, S., O’Faire, J., & Warren, C. (1999). Hospice access and use by African Americans: Addressing cultural and institutional barriers through participatory action research. _Social Work, 44_(6), 549–559.
- Reid, K. E. (1997). _Social work practice with groups: A clinical perspective_ (2nd ed.). Belmont, CA: Thomson Learning–Brooks/Cole.
- Schectman, J. M., Schroth, W. S., Verme, D., & Voss, J. D. (2003). Randomized controlled trial of education and feedback for implementation of guidelines for acute low back pain. _Journal of General Internal Medicine, 18_(10), 773–780.
- Spouse, J. (2001). Bridging theory and practice in the supervisory relationship: A sociocultural perspective. _Journal of Advanced Nursing, 33_(4), 512–522.
- Staudt, M. M., Bates, D., Blake, G. H., & Shoffner, J. S. (2004). Practice evaluation: Moving beyond single system designs. _Arete, 27_(2), 71–78.
- Stein, K. F., Sargent, J. T., & Rafaels, N. (2007) Intervention research: Establishing fidelity of the independent variable in nursing clinical trials. _Nursing Research, 56_(1), 54–62.
- Weinbach, R. W. (2005). _Evaluating social work services and programs_. Boston, MA: Allyn & Bacon.