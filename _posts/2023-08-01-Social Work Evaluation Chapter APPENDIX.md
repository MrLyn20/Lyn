---
title: Social Work Evaluation APPENDIX
date: 2023-08-12
author: JAMES R. DUDLEY
---

# APPENDIX A

American Evaluation Association Guiding Principles for Evaluators: 2018 Updated Guiding Principles

## A. SYSTEMATIC INQUIRY: EVALUATORS CONDUCT DATA-BASED INQUIRIES THAT ARE THOROUGH, METHODICAL, AND CONTEXTUALLY RELEVANT

- **A1.** Adhere to the highest technical standards appropriate to the methods being used while attending to the evaluation’s scale and available resources.
- **A2.** Explore with primary stakeholders the limitations and strengths of the core evaluations questions and the approaches that might be used for answering those questions.
- **A3.** Communicate methods and approaches accurately, and in sufficient detail, to allow others to understand, interpret, and critique the work.
- **A4.** Make clear the limitations of the evaluation and its results.
- **A5.** Discuss in contextually appropriate ways the values, assumptions, theories, methods, results, and analyses that significantly affect the evaluator’s interpretations of the findings.
- **A6.** Carefully consider the ethical implications of the use of emerging technologies in evaluation practice.

## B. COMPETENCE: EVALUATORS PROVIDE SKILLED PROFESSIONAL SERVICES TO STAKEHOLDERS

- **B1.** Ensure that the evaluation team possesses the education, abilities, skills, and experiences required to complete the evaluation competently.
- **B2.** When the most ethical option is to proceed with a commission or request outside the boundaries of the evaluation team’s professional preparation and competence, clearly communicate any significant limitations to the evaluation that might result. Make every effort to supplement missing or weak competencies directly or through the assistance of others.
- **B3.** Ensure that the evaluation team collectively possesses or seeks out the competencies necessary to work in the cultural context of the evaluation.
- **B4.** Continually undertake relevant education, training or supervised practice to learn new concepts, techniques, skills, and services necessary for competent evaluation practice. Ongoing professional development might include: formal coursework and workshops, self-study, self-or externally commissioned evaluations of one’s own practice, and working with other evaluators to learn and refine evaluative skills expertise.

## C. INTEGRITY: EVALUATORS BEHAVE WITH HONESTY AND TRANSPARENCY IN ORDER TO ENSURE THE INTEGRITY OF THE EVALUATION

- **C1.** Communicate truthfully and openly with clients and relevant stakeholders concerning all aspects of the evaluation, including its limitations.
- **C2.** Disclose any conflicts of interest (or appearance of a conflict) prior to accepting an evaluation assignment and manage or mitigate any conflicts during the evaluation.
- **C3.** Record and promptly communicate any changes to the originally negotiated evaluation plans, that rationale for those changes, and the potential impacts on the evaluation’s scope and results.
- **C4.** Assess and make explicit the stakeholders’, clients’, and evaluators’ values, perspectives, and interests concerning the conduct and outcome of the evaluation.
- **C5.** Accurately and transparently represent evaluation procedures, data, and findings.
- **C6.** Clearly communicate, justify, and address concerns related to procedures or activities that are likely to produce misleading evaluative information or conclusions. Consult colleagues for suggestions on proper ways to proceed if concerns cannot be resolved, and decline the evaluation when necessary.
- **C7.** Disclose all sources of financial support for an evaluation, and the source of the request for the evaluation.

## D. RESPECT FOR PEOPLE: EVALUATORS HONOR THE DIGNITY, WELL-BEING, AND SELF-WORTH OF INDIVIDUALS AND ACKNOWLEDGE THE INFLUENCE OF CULTURE WITHIN AND ACROSS GROUPS

- **D1.** Strive to gain an understanding of, and treat fairly, the range of perspectives and interests that individuals and groups bring to the evaluation, including those that are not usually included or are oppositional.
- **D2.** Abide by current professional ethics, standards, and regulations (including informed consent, confidentiality, and prevention of harm) pertaining to evaluation participants.
- **D3.** Strive to maximize the benefits and reduce unnecessary risks or harms for groups and individuals associated with the evaluation.
- **D4.** Ensure that those who contribute data and incur risks do so willingly, and that they have knowledge of and opportunity to obtain benefits of the evaluation.

# E. COMMON GOOD AND EQUITY: EVALUATORS STRIVE TO CONTRIBUTE TO THE COMMON GOOD AND ADVANCEMENT OF AN EQUITABLE AND JUST SOCIETY

- **E1.** Recognize and balance the interests of the client, other stakeholders, and the common good while also protecting the integrity of the evaluation.
- **E2.** Identify and make efforts to address the evaluation’s potential threats to the common good especially when specific stakeholder interests conflict with the goals of a democratic, equitable, and just society.
- **E3.** Identify and make efforts to address the evaluation’s potential risks of exacerbating historic disadvantage or inequity.
- **E4.** Promote transparency and active sharing of data and findings with the goal of equitable access to information in forms that respect people and honor promises of confidentiality.
- **E5.** Mitigate the bias and potential power imbalances that can occur as a result of the evaluation’s context. Self-assess one’s own privilege and positioning within that context.

# APPENDIX B

## Glossary

1. **Accessibility** refers to the extent to which clients can freely seek out and fully use the services of an agency. A lack of accessibility could be reflected in barriers that are physical, geographic, psychological, or cultural.
2. **ANOVA** (one-way analysis of variance) is a bivariate statistical test that is used to determine whether three or more groups are significantly different from one another with respect to a specific characteristic such as a client outcome variable.
3. **Assessments of client strengths (and weaknesses)** are assessment tools in practice that include questions to explore clients’ strengths and weaknesses; the assessment of clients’ strengths is often an area that is otherwise overlooked in assessments.
4. **Best practices** are the “best” or most effective programs or practices known to help people overcome their problems. Evidence-based practices, in many ways, are synonymous with “best practices.”
5. **Biopsychosocial assessment tools** are assessment tools in practice that identify questions and topical areas to explore with clients in three areas of their lives: biological, psychological, and social. Sometimes these tools also include assessment questions about the spiritual, mental, and cultural aspects of a client.
6. **Case managers** monitor the range of services provided to their clients to ensure their quality and appropriateness. Case managers typically find services for clients, negotiate or facilitate contracts with the agencies that offer services, monitor services to ensure that the agency delivers the contracted services, and advocate when necessary on behalf of clients.
7. **Central tendency measures** are univariate statistics that describe a central point in the distribution of a set of scores. Three types of measures of central tendency are available for use in evaluations: mean (average score), median (middle score), and mode (most frequent score).
8. **Chi-square test** is a bivariate statistic used to determine whether the values of one variable are associated with the values of another variable. This test is also sometimes referred to as a “cross-tab,” or cross-tabulation, of these variables.
9. **Client satisfaction studies** have the purpose of finding out how the clients are feeling about and perceiving the services that they are receiving.
10. **Clinical significance** is a standard used to determine whether enough improvement occurred in the client’s outcome variable after implementation of the intervention. Clinical significance is based on clinical criteria established by a professional group or agency. In this case, the amount of improvement in the client’s goal is enough if it meets the established clinical standard.
11. **Community forums** are sessions in which several people come together to discuss a topic of interest to all of them. Participants are likely to belong to the same community or neighborhood, and the topic is likely to be an issue that has communitywide implications.
12. **Condition** is a property of a measurable objective that identifies prior circumstances or conditions required before a performance can be expected.
13. **Confidentiality** refers to an ethical safeguard required in evaluations. It is intended to insure the privacy of the research participants. Their responses and behaviors, when recorded, are purposely not to be associated in any way with their names or identity. In other words, no participant is to be identified by the evaluator in a report, presentation, informal conversation, or any other communication.
14. **Correlation tes**t is a bivariate statistical test used to determine whether there is a statistically significant association between two variables. A correlation test can be used only when both variables are at the interval/ratio level of measurement.
15. **Cost–benefit analysis study** is a type of evaluation that determines whether a program is reaching its goals in a cost-effective way.
16. **Cross-sectional study** collects data from participants at one time only.
17. **Criterion** is a property of a measurable objective that is a standard of acceptable performance, reflected in such things as speed, accuracy, or quality.
18. **Critical consumer role** refers to being capable of reading and evaluating research studies in a critical manner so that they can identify the strengths and limitations of studies. Using a critical consumer role, a person can understand a study, evaluate how well it was conducted and presented, and know how to apply the findings appropriately and cautiously to the consumer’s practice with clients.
19. **Critical thinkers** are natural skeptics about how well an evaluation is conducted, whether it is someone else’s evaluation or one’s own. Critical thinkers do not automatically assume that what they read in a report is correct before considering how it was determined and sometimes considering alternative possibilities.
20. **Descriptive statistics** are statistics used to summarize responses or scores for each variable. These statistics are also referred to as univariate statistics because they analyze one variable at a time.
21. **Ecomaps** are an assessment tool in practice that visually describe the client’s personin-environment constellation, including relationships with friends and family members who are significant to the client, agencies and schools that are involved, jobs, and other social supports.
22. **Effectiveness** refers to interventions being successful in bringing about the expected changes in clients’ lives.
23. **Efficiency** refers to being careful to channel available resources to the intended target problem or concern without any waste.
24. **Effort** refers to what staff members, volunteers, and administrators put into a program or practice area when implementing an intervention.
25. **Elements of programs and practice** are distinct entities critical to their functioning and a central focus of evaluations. Several elements can exist in each of the stages. For example, input elements during the planning stage could include the clients’ proposed goals in a program or practice intervention. Process elements during the implementation stage could be a theoretical practice approach that is being used. Outcome elements can be the measures of client progress in reaching their goals.
26. **Empowerment approaches** refer to concepts, techniques, and findings that help specific oppressed groups or communities develop and use their self-determining abilities. An example of empowerment in an evaluation would be involving program participants in a leadership role in conducting the evaluation and using outside evaluators and other experts as coaches, facilitators, consultants, and critical friends.
27. **Evaluation design** is the plan for carrying out an evaluation. It includes a set of study questions to explore and/or hypotheses to test, a source from whom the data are gathered, a specific method of collecting the data, and a data analysis plan. A plan to protect human participants of the study is also an important component to complete and have approved.
28. **Evidence-based interventions** are interventions for which there is the best available external evidence that they are effective. Sources of evidence come mostly from research studies using quasi-experimental or experimental designs and practice experience. Evidence-based sources should also be consistent with the values and expectations of the clients receiving these interventions.
29. **Experimental designs** are longitudinal group designs that can be used to test whether there is a causal relationship between an intervention and a client outcome variable. These designs are the strongest designs for proving causality and have two groups, a group that receives the intervention and a control group that does not. Participants are selected and randomly assigned to the two groups. The control group is considered identical to the group receiving the intervention, except that it does not receive the intervention.
30. **Experimental design approaches** use evaluation perspectives that posit that experimental and quasi-experimental designs are superior to other evaluation designs because they deliver scientifically credible evidence of the impact of programs on clients’ welfare. The perspective posits that using experimental designs and randomized samples are usually feasible and ethical, and they are the only known way to rule out other influences such as other sources of help.
31. **Family genograms** are an assessment tool in practice that provide a visual display of a family across two or more generations and they offer insights into understanding the patterns evident in the client’s family.
32. **Feminist perspectives on evaluation** are defined both by the research methods that feminists prefer and the substantive areas they choose to study. Most feminist researchers prefer qualitative over quantitative methods because of the flexibility built into the method and the value it places on uncovering new knowledge at deeper levels of meaning. This evaluation focus tends to be concerned primarily with studying the relative positions and experiences of women in relation to men and the effects that gender issues have on both sexes, such as gender discrimination in hiring, the low representation of women in administrative and other leadership roles, and salary inequities based on gender.
33. **Focus groups** are a special type of group in which participants are invited to discuss a specific topic. Focus groups can be used to find out the views of several people on an issue of importance to a needs assessment. Members of focus groups are usually selected because they have personal knowledge or expertise about a specific topic. The leader of a focus group facilitates a group discussion by asking each member several general questions on the topic.
34. **Formative evaluations** focus on planning for a program and improving its delivery of services. Such evaluations are usually initiated and conducted by the agency providing the program/practice intervention and are implemented internally by staff of the agency and outside consultants whom they select.
35. **Fourth-generation evaluations** refer to a new approach for this era that strives to redress power imbalances and expand the repertoire of data gathering and analysis methods used in evaluations. Someone with this perspective distrusts all methods equally and can recognize and admit the situational limitations of the knower, whether based in science, literature, or another vantage point. This perspective particularly seeks to hear and value the often-unheard indigenous voices of other societies in the world.
36. **Frequency distribution** is a univariate statistic that describes the number and percentage of times that each of the values of a variable is observed in a sample. For example, a sample is made up of 25 (38.5%) females and 40 (61.5%) males.
37. **General study questions** for evaluations serve two purposes. First, they provide parameters for an evaluation, limiting what will be investigated. Second, they provide enough specificity to know what is to be explored in a specific sense. In other words, they are not too broad to be unrealistic and not too specific to be too restricting.
38. **Goal** is an end-result or outcome of an intervention. Goals are important because they provide an intervention with a direction to pursue and an outcome to reach. Without goals, an intervention may be an aimless effort that does not seem to go anywhere important or relevant. Goals are often too broad to be measured and need measurable objectives or specific indicators that provide measures that the goals have been reached.
39. **Goal attainment scale** (GAS) is a practice evaluation tool that can be used to evaluate the extent to which the worker’s interventions resulted in a change in the client’s goals in a favorable way. A GAS is an incremental scale that can be used to measure degrees of client progress.
40. **Implementation evaluations** investigate the procedures and processes established to carry out a program or practice intervention. Among the overall questions are, How well has the agency followed proposed procedures in implementing a new intervention? and How does the introduction of these changes seem to be impacting clients as they are implemented?
41. **Infographics** is an electronic resource that can be used to provide a visual presentation of complex concepts in an easy to read and enjoyable graphic form. This tool is designed to condenses large amounts of information into a form in which it is much easier for readers to absorb.
42. **Informed consent** means that the participants of an evaluation are thoroughly informed about the study and expectations of them before they are expected to consent to participate.
43. **Institutional review boards** (IRBs) are a formal group of people at an agency or university who have been designated to promulgate ethical standards and to approve and monitor the ethical provisions of all studies sponsored by the organization or institution. Evaluators and other researchers are required to submit a formal statement to their IRB that describes their study, its procedures and benefits, and how it protects research participants from risks of harm and invasion of privacy.
44. **Logic model** is a tool to theoretically analyze a program or practice intervention. The logic model helps highlight how the stages and elements of an intervention should be logically linked to one another in an organic whole. Using this model, the sequence of steps in an intervention’s development is important to examine, beginning with the problems of prospective clients and culminating in anticipated client outcomes.
45. **Longitudinal study** involves collecting data from research participants at two or more times.
46. **Mapping** is a computerized approach that visually locates specific groups of people or problems in a specific geographic area. Mapping helps the user recognize where there are concentrations of problems; also, if they are associated with specific groups of people and how they may have an association with other relevant factors.
47. **Measurable objectives** are statements that identify indicators of whether a client or client system reaches a goal. Usually each goal has more than one measurable objective.
48. **Mixed methods** refer to using quantitative and qualitative methods of data collection and analysis that can be combined in evaluations. Use of mixed methods offers several advantages, including use of the data of one to complement the other in evaluation findings, as well as triangulating the collected data, which can strengthen the accuracy of the findings.
49. **Needs assessment** is an organized and systematic effort to assess the need for something, in this case the need for a new program or practice intervention. Executing a needs assessment of some or all prospective recipients of a program is often the wisest way to conduct an assessment. The specific ways to conduct a needs assessment can take many forms, including use of existing data, construction and administration of a questionnaire, conducting interviews or focus groups, arranging of public forums, and observations or a combination of these methods.
50. **Outcomes** are measures of client progress in reaching their goals.
51. **Overt indicator** is a word or phrase added after a performance to help make the performance more observable in a measurable objective.
52. **Participatory action research (PAR)** is an empowerment approach that can be used in evaluations. As its perspective, PAR has an interest in actively involving the research participants or subjects in all or most steps of the process.
53. **Performance** is a property of a measurable objective that identifies what a person is expected to do.
54. **Posttest** is a measure of the client outcome variable after the intervention is completed.
55. **Practice** involves the interventions of one individual social worker or human service worker. This contrasts with a program that involves the interventions of all the staff members in that program.
56. **Practice evaluation** is a study of one social worker’s interventions with clients. Like a program evaluation, it uses methods of research. However, in contrast to a program evaluation, it focuses on the client interventions of only one social worker.
57. **Pre-experimental designs** are group designs that can be used to explore a causal relationship between an intervention and a client outcome variable. These designs have only an intervention group and no other group such as a control group. They are the weakest group designs and have limited use, but they are often considered initially because they can be implemented more easily and take less effort than experimental and quasi-experimental designs.
58. **Pretest** is a measure of the client outcome variable before a client is introduced to an intervention. A pretest is also sometimes referred to a baseline measure.
59. **Program** is an organizational entity in an agency that offers a set of goods and/or services with common goals. The goods and services are typically provided to a target population of clients who either seek them out or are selected to use them. A program typically employs more than one and usually several staff members.
60. **Program evaluation** is a study of a social program that uses the methods of scientific research. It concerns itself with the practical needs of an organization, not theoretical issues, and it focuses on how well an overall program implements its services.
61. **Quality** refers to a program or practice intervention being delivered as intended and implemented at a high standard.
62. **Quality assurance** is a set of evaluation activities implemented to gather information about the processes of programs. Its distinguishing characteristics include peer reviews, heavy reliance on client records, observation of small samples of the services, assurance that minimal standards are met, and immediate incorporation of useful results to improve the program. Quality assurance is also sometimes referred to as quality improvement or quality control.
63. **Quasi-experimental designs** are longitudinal group designs that can be used to explore a causal relationship between an intervention and a client outcome variable. These designs have features that control for some of the extraneous influences by using a comparison group or having multiple measures of the outcome variable.
64. **Regression tests** are statistical tests that can predict a client outcome from more than one intervention.
65. **Relevance** refers to an intervention meeting the clients’ need and contributing to solving the larger social problem that clients are experiencing.
66. **Results-oriented approaches** use evaluation perspectives that focus on performance, outcomes, and accountability. Advocates of the results-oriented approaches believe that it is important to convince various stakeholders that the outcomes they achieve are the ultimate reason why they are in business.
67. **Secondary research** analyzes data from a previous study. Many research firms offer their data sets to interested people for secondary analysis, often for a reasonable fee.
68. **Sequencing blocks** are an exercise that helps clients explore the causal relationships among prior behaviors that have resulted in a positive or negative outcome. In this case, the outcome behavior is known and the behavior that caused it is to be discovered.
69. **Services** is a general term describing the activities provided to clients by programs and practice interventions. Services comprise the processes that help clients reach their goals. Helping processes are the major focus of practice courses in social work programs and draw from a broad range of practice theories, such as cognitive-behavioral, person-centered, solution-focused, and advocacy.
70. **Single-system design** (SSD) is a practice evaluation tool that determines the impact of one or more interventions of a social worker on a client system. An SSD uses a graph that visually presents the changes that a client makes over time. An SSD can be used to observe whether an intervention is responsible for improvements in a client’s behavior.
71. **Stakeholders** are the people invested and/or involved in a program or practice intervention in some way. Examples of stakeholders include representatives of a funding agency, an agency board of directors, staff members, and clients.
72. **Statistical significance** is one way to determine whether enough improvement occurred in the client’s outcome variable after implementation of the intervention. Statistically significant associations can be calculated by applying a statistical test, which determines whether the probability of an association between the intervention and the outcome measure is high. Usually a claim of significance is made when the probability of being in error is less than 5% (expressed as _p_ < 0.05).
73. **Summative evaluations** focus on the outcomes of programs and attempt to answer the ultimate question: Did the intervention reach its goals? These evaluations have a finality to them in that they attempt to measure whether a program was effective. Summative evaluations are typically conducted by an external agent, such as an independent evaluator or a governmental agency, to ensure that they are objective. Funding and regulatory agencies are most interested in summative evaluations because they provide the kinds of information they need to make major decisions around future funding.
74. **_t_-test** is a bivariate statistical test used in evaluations to determine whether two groups are significantly different based on measures of a client outcome. One variable in this statistical formula is the group variable (group 1 or group 2). The second variable is the client outcome measure being compared.
75. **Target problem scale** (TPS) is a practice evaluation tool that can be used particularly when client goals are difficult to identify. A TPS is helpful if clients remain focused on their problems and are not prepared to pursue goals beyond these problems.
76. **Technical report** is a comprehensive report of the findings and recommendations of an evaluation that also usually includes a review of the pertinent background literature and a description of the methods and research design used in the evaluation. Technical reports are usually most helpful to funding agencies, administrators of agencies sponsoring an intervention, and some key staff members. They are also ideal to publish in some professional journals.
77. **Theme analysis** is a type of qualitative data analysis that involves identifying common themes or patterns prevalent in narrative data.
78. **Three-stage approach** refers to defining evaluation broadly and occurring at any of the stages of development of a program or practice intervention. The three basic stages that are important to evaluations are the planning, implementation, and outcome stages.
79. **Variability** measures are univariate statistics that summarize how the responses to a question vary or disperse away from their central tendency. These statistics complement measures of central tendencies because they provide information about the variability of scores away from the clustering or central point. Two common statistics that measure the variability of scores are range and standard deviation. Both can be used with interval/ratio level variables only.