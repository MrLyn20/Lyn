---
title: Chapter 12 Becoming Critical Consumers of Evaluations
date: 2023-08-12
author: JAMES R. DUDLEY
---

**PART VII**

Consuming Evaluation Reports

Chapter 12 is the sole chapter in this section. It is devoted entirely to the consumer role that involves being able to critically read and utilize evaluation reports. All the previous chapters have focused on the producer role of conducting evaluations. Several different kinds of stakeholders who consume evaluation reports are discussed in term of their varying needs and capacities to consume these reports. The chapter primarily discusses the special needs of five of these groups—students, practitioners, administrators, reimbursement and regulatory entities, and clients of social services. The chapter goes on to describe in detail how an actual evaluation report can be critically consumed so that it is has maximum meaning and utility for a consumer. The chapter closes with a discussion about how evaluation reports can be prepared in special ways so that clients, their families, and communities can fully benefit from them.

# CHAPTER 12 Becoming Critical Consumers of Evaluations

Daniel Freedman and James R. Dudley

Increasingly, sparse resources and finite amounts of reimbursable services are characteristics that seem to exemplify social work practices in the twenty-first century. As a result, social work practitioners must have the keen ability to implement effective interventions in a timely and efficient manner. As such, the ability to identify evidence-based practices from existing reported research is essential for the profession (Howard, McMillen, & Pollio, 2003). In consonance with this assumption, the accrediting body of social work education, the Council on Social Work Education (CSWE), requires curriculum content on research and evaluation methods. Similarly, the Code of Ethics promulgated by the National Association of Social Workers (NASW) has an entire section on Evaluation and Research Standards that is elaborated on in Chapter 3 (Bolin, Lee, Glenmaye, & Yoon, 2012; Staudt, 2007). One of the specific standards of the Code of Ethics directly supports critical consumption of research. It refers to social workers’ obligation to “critically examine and keep current with emerging knowledge relevant to social work and fully use evaluation and research evidence in their professional practice” (NASW, 2017). Thus, the focus of this chapter is to highlight and illustrate how to be a _critical_ consumer of evaluation reports.

In brief, a critical consumer role involves (a) understanding a study or evaluation report, (b) assessing how well it was conducted and presented, and (c) applying the findings to your work or practice (Dudley, 2011). First, a critical consumer should be able to understand the evaluation reports they read. After reading a study a couple of times, a skillful consumer will be able to understand the purpose of the study, the characteristics of people being studied, the data collection approach used, the findings that are presented, and the recommendations. These are some of the most important aspects to understand. Second, critical thinking skills need to be used to assess how well the study was conducted. This involves being prepared to be skeptical about a study rather than assuming it was conducted and presented perfectly.

Assume that all research studies have their imperfections or limitations. At times, authors of studies may leave important information out of their report or may fail to report important limitations inherent in their studies. These unseen limitations or errors are not easily found unless a critical consumer assesses the study closely. Third, you as a consumer need to know how to apply the findings of a study or evaluation report to your own practice or other use. You might ask several questions, such as “What is the relevance of the study for me and my clients?” “Can the findings be generalized to the clients I serve?” and “What recommendations are offered that can help my practice?”

The sections that follow describe some of the key stakeholders that consume evaluation reports, followed by a critical examination of a recent evaluation report, and some thoughts on how to help stakeholders utilize information in evaluation reports. The Seven Evaluation Steps, introduced in Chapter 1, are used to help organize this chapter. As a reminder, these steps are as follows.

> **Seven Evaluation Steps**
> 
> - Step 1: Identify the Problem or Concern to Be Evaluated
> - Step 2: Identify and Explore Ways to Involve Stakeholders
> - Step 3: Determine the Purpose of the Evaluation
> - Step 4: Plan the Evaluation
> - Step 5: Implement the Evaluation
> - Step 6: Prepare a Written or Oral Report
> - Step 7: Disseminate the Findings

## STAKEHOLDERS WHO CONSUME EVALUATION REPORTS

Stakeholders who are interested in evaluation reports will have varying needs and capacities to consume these reports. Stakeholders of evaluations who have been discussed in previous chapters include a long list—students, administrators and practitioners working in agencies, board members, regulatory and funding agency personnel, clients, advisory groups, community groups, and others. This chapter briefly discusses the special needs of five of these groups—students, practitioners, administrators, reimbursement and regulatory entities, and clients of social work services.

### Social Work Students

As mentioned above, the CSWE mandates that students of social work become critical consumers of research. Since 1981, the CSWE has required research methods courses for master’s level curriculum, and later this became a requirement at the baccalaureate level. Unfortunately, many students are sometimes fearful and weary of research methods. Plausible explanations for this discomfort are that most students have low levels of interest in research, some have problems developing research competencies, and sometimes they are unaware of the critical relevance of research to social work practice. Interestingly, studies reveal that experiential strategies of teaching have heightened students’ interest and competencies in these courses (e.g., Harder, 2010; Lundahl, 2008). These strategies usually include students being actively involved in the design, implementation, and evaluation of a research project. Lundahl (2008) found that this type of involvement was not only meaningful, but also promoted a more thorough understanding of research.

On an academic level, developing a skill set for critically consuming evaluation reports is crucial. Accordingly, CSWE’s most recent policy standards continue to support the “prominence” of research within social work education (e.g., Bolin et al., 2012). On the level of professional development, this skill set remains important as social work interns must be able to provide effective interventions to their client systems in their field placements.

### Practitioners

Having research skills may be even more important for practitioners. Professionals in social agencies are encouraged and sometimes expected to understand evaluation methods and to be advocates for the implementation of evidence-based practices (Gibbs, 2007). Many practitioners are using evaluations to inform their professional interventions and to improve their ability to provide evidence-based practice to their client systems. Imagine the ill consequences that can emerge if they do not have these critical consumer skills. That is, consequences as significant as fatalities can materialize if evaluations are misinterpreted and ineffective practices are implemented with populations diagnosed with substance use, borderline personality, eating disorders, and other mental health conditions.

In an ideal practice milieu, practitioners will apply the most effective interventions for empowering their client populations. Of course, this ideal is often not the reality due to multiple circumstances including lack of adequate resources, training, and administrative support. Nonetheless, the professional development of social work practitioners includes identifying, critiquing, and accumulating knowledge of evidence-based practices, and the critical consumption of evaluation reports can be at the core of this endeavor. Practitioners with an advanced skill set in critically evaluating research may also be able to examine the strengths and weaknesses of psychometric instruments and data analysis strategies used to measure client outcomes.

### Administrators

Administrators are usually a heterogeneous group that can consist of program and agency directors, quality assurance and human resource specialists, and members of leadership teams and boards of directors. As a group of stakeholders, administrators are a crucial cog in the process of making agency-level decisions, including deciding on the implementation of evidence-informed practice (Collins-Camargo & Millar, 2010). In an ideal organizational climate, the decision of which programs and practice interventions to implement will be at least partially based on a critical consumption of evaluation reports (e.g., Long, Tice, & Morrison, 2006). For administrators, this preference is important. However, the reality of social service organizations is that administrators must also take into consideration the implications of budgets, personnel issues, agency mission and philosophy, and multiple political factors. In addition, Long and colleagues argue for the inclusion of clients and client systems in making organizational decisions. In their view, client input can result in an empowerment-oriented infrastructure that can best meet the needs of its clients. In theory, this inclusion of client input along with the critical consumption of evaluations will hopefully be used by administrators to make its decisions about which programs to implement (Petr & Walter, 2005).

### Reimbursement and Regulatory Entities

Without a funding stream, the implementation of evidence-based programs and practices is a nonexistent consideration. Likewise, without having an accredited status, neither funding nor implementation issues have much bearing, as such human service agencies will cease to exist. These salient issues arise through the functioning of reimbursement and regulatory entities. Examples of these systems include third-party payers such as Medicaid and Medicare, and accreditation organizations such as the Commission on Accreditation of Rehabilitation Facilities (CARF) and the Joint Commission on Accreditation of Healthcare Organizations (JCAHO). Third-party payment systems will be interested in evaluation research as it provides evidence of the effectiveness of programs and practice interventions. What these entities have in common is that they are concerned about budgets and limiting a profit margin. As such, evaluations that demonstrate short-term and less expensive interventions have a better chance of being reimbursed than services that are long-term and expensive.

In addition to these considerations, organizations such as JCAHO and CARF place more emphasis on whether agencies are maintaining practice standards that benefit clients and client systems. For instance, JCAHO is concerned about whether standards of health care assessment and treatment are being met, and whether clinical ethics and client rights are adequate. CARF focuses on similar components within health care settings with an emphasis on rehabilitation programs in mental health, substance abuse, retirement living, and multiple services offered in the community (Wilkerson, Migas, & Slaven, 2000). To the extent that evaluations address accreditation standards, regulatory entities will have vested stakes in consuming evaluation reports.

### Clients

Clients are a heterogeneous population representing a diversity of races, social classes, ethnicities, educational levels, and income levels, as well as a wide range of psychosocial characteristics and intellectual capabilities. Many are disenfranchised, vulnerable, and desperate for help. Some may be struggling with maintaining housing, others may be involved in situations of intimate partner violence, and still others may struggle with child care or parenting issues. The common denominator of most clients is that they are seeking assistance with navigating their social environment or relieving personal hardships and distress; they need interventions that will work for them in a most effective way!

Yet, it may seem unrealistic to expect that most clients of social services will be motivated to understand evaluation terminology and methodologies, let alone know how to locate relevant evaluation reports. Instead, it may seem more than enough to expect them to partake as fully as possible as recipients of services. Further, there may be the assumption of clients that professional providers will identify and implement empirically supported practices (Mildon & Shlonsky, 2011). Nevertheless, it seems plausible that clients of social services will have the most at stake in evaluation reports being critically consumed and used. As recipients of the interventions, they will want to know that they are effective. For fortunate clients, they may have support and assistance from family and friends, or special interest groups. These special interest groups could include the National Alliance on Mental Illness, the ARC for People with Intellectual and Developmental Disabilities, and Parents, Families, & Friends of Lesbians and Gays. Generally, advocacy and client advisory groups, mentors, and coaches are, at times, essential to assist them in assessing the effectiveness of the interventions they receive. Unfortunately, many clients will not have good connections with such support groups or others having technical expertise. Overall, it is fair to state that recipients of social services have a tremendous stake in the evaluations of the interventions they receive, and advocacy groups that are involved in supporting them can be essential partners in insuring the effectiveness of their services.

## CRITICAL CONSUMPTION OF AN EVALUATION REPORT

As earlier chapters have pointed out, there are multiple types of evaluations of interest to social workers including needs assessments, program monitoring, client satisfaction studies, outcome studies, efficiency and cost–benefit analysis, program licensing, auditing, accreditation, and quality assurance. All these evaluations have many characteristics in common, with the use of research methods being one of them. Research methodologies used in evaluations can vary widely, for instance, ranging from being exploratory as in an ethnographic case study to experimental designs. Evaluations also use a wide range of measures, both quantitative and qualitative, for determining the effectiveness of practitioners’ interventions and programs alike.

The evaluation report selected as an example in this chapter is only one type of evaluation, but it reflects many of the characteristics of some of the more common reports that need to be consumed. The report, “Evaluation of a Spiritually Focused Intervention with Older Trauma Survivors,” focused on evaluating the effectiveness of a spiritually oriented group intervention intended to lessen the physical and mental health symptoms of a group of older female trauma survivors (Bowland, Edmond, & Fallot, 2012). The research design that was used was an experimental design that tested the effectiveness of an eleven-session spiritual intervention based on similar interventions used in the past. In some ways, this evaluation is a template for testing evidence-based practice because it uses an experimental design. This evaluation report will hereafter be referred to as the Bowland Report in this chapter. It will be necessary to obtain a copy of the Bowland Report to fully understand the examination of the study that follows. Note the report is fully cited at the end of this chapter (Bowland et al., 2012).

The Bowland Report has several strengths. The methodology is a rigorously developed experimental design having strong implications for determining causal relationships. In addition, there are comprehensive details in many areas such as in its description of the intervention used and previous interventions that the intervention is based on. Further, it gives helpful details about the four client outcome measures that were used and the recruitment process involved in locating a sample of older women experiencing abuse and trauma in the past. Such details can be helpful to others who wish to replicate the evaluation in other settings.

Yet, the report could be better organized and more succinct in a few areas. Sometimes, material is not located in the sections where one would expect it to be. For example, the hypotheses that were tested were not located just before the method section. Thus, some critical consumers may have difficulty finding them. Overall, this report is probably typical of most evaluation reports and reflects the kinds of strengths and limitations that are found in reports generally. In this chapter, we will assume a critical consumer role and critique the Bowland Report. The Seven Evaluation Steps described in the book provide a framework for this critique. Some key comments will be made in introducing each step before delving into how Bowland and colleagues reported on them. It is important to note that the emphasis in this critique will be placed primarily on the concepts covered in earlier chapters of the book that can help the reader become a more critical consumer of evaluation reports; an exhaustive critique that covers all the issues involved in each step is well beyond the purpose of this chapter.

A critical consumer can ask many good questions. For example, how well does the author(s) define the problem, goals, interventions, outcome measures, and other concepts? Does the author provide enough details about them and is the information succinct and fully relevant? Is the literature reviewed in the report helpful in elaborating on how others have viewed the problem? Does it appear that all the appropriate groups of clients who could be recipients of the intervention are included? You, the reader, are encouraged to attempt to answer these questions largely on your own and in small groups as these questions will only be addressed in general terms in the critique in this chapter. As mentioned earlier, the reader is strongly encouraged to obtain a copy of the Bowland Report and read it before continuing to get the most benefit out of the chapter. A link to the abstract of the report is http://sw.oxfordjournals.org/content/57/1/73.short.

### Step 1: Identify the Problem or Concern to Be Evaluated

Critiquing step 1 is a good time for a critical consumer to learn something about a problem or concern that is being addressed by an evaluator. A well-written report offers a problem definition that includes a literature review, an introduction of some theoretical concepts, and a perspective on how to solve this problem. Several sections in earlier chapters of the book address some of the issues related to defining the problem and finding solutions to solve it. Needs assessments are a major focus of Chapter 6 and can be useful in defining the problem being addressed. In this regard, a good question to ask is whether a needs assessment was used to formulate the problem definition.

The concepts of problem, specific needs, and the causes of the problems are also discussed in Chapter 6. The concept of specific needs is an important concept because it refers to an aspect of the larger problem identified by the client and selected because it is perceived to be reasonable to meet. These three elements (problem, specific needs, and causes) are all linked together.

In addition, the goals and objectives for addressing the problem naturally follow next based on the logic model. The logic model is a helpful framework to use in assessing how well an author has defined the problem. For example, this model helps the critical consumer determine how much of the larger problem faced by the clients in the study is reduced to “specific needs” identified by them and viewed as something that can be met. Also, it’s important to figure out how much the authors of a report have seriously considered the logic model.

Looking at the Bowland Report related to step 1, the literature review about the problem itself tends to be brief, representing most of the first page of the report. The problem is described as the long-term health and physical problems of older women (forty-five years and older) resulting from repeated and mostly earlier experiences of being sexually and physically abused. Several references to specific literature provide documentation of the problem and why older women have greater problems in these areas than younger women or men; this is given as the reason why these women were chosen for the study. This cited literature was largely published in the 1990s and early 2000s, raising a question of how up-to-date it is; for example, is more recent important literature available? The concepts of problems and specific needs were not differentiated in this section, and the causes of the problems were briefly touched on. Child sexual and physical abuse, and domestic violence, often continuing in these abusive relationships, were among the causes identified. Overall, this section of the study could have given more detailed attention to what the problem is, and a needs assessment could have been a helpful way of doing this.

Additionally, in the introduction, other concepts are reported related to addressing the problem of past abuse and trauma. The concept of spirituality is defined and described as the key concept for overcoming current difficulties that these women were experiencing. Spirituality is defined broadly and is viewed as including both strengths and possible struggles of the past to address. Later in the article, these spiritual strengths and struggles are described in greater detail in the form of topics that were covered in the eleven sessions of the group intervention.

### Step 2: Identify and Explore Ways to Involve Stakeholders

Critiquing step 2 is important to consider because it reminds the critical consumer that a political process occurred while an evaluation was taking form and being implemented. Yet, unfortunately this “messy” process is largely absent from evaluation reports. A report of an evaluation does not usually describe who the stakeholders are, how they may agree or disagree with each other, and how these differences are handled by the evaluator. In this case, we miss an important underlying process that could be useful if it was recreated in our minds. Stakeholders of the evaluation process have been discussed in several previous chapters of the book. The emphasis on involving stakeholders distinguishes an evaluation report from other kinds of research studies that depend much less on stakeholders. Evaluators ideally encourage the stakeholders to be involved in informing the purpose of an evaluation, improving its design, and disseminating the findings to others. This input can include recruiting clients as evaluation participants, contributing to the formulation of useful research questions, and identifying relevant outcomes. Moving to a broader context, an evaluation does not happen in a vacuum; instead, it operates within the context of an ever-changing social environment, which includes stakeholder systems. Critical consumers of evaluation reports should be aware of how stakeholders’ involvement may strengthen or possibly weaken the evaluation’s efforts. Because the involvement of stakeholders is not often explicitly described in evaluation reports, critical consumers need to read between the lines and possibly speculate about whom the stakeholders were and how much each was involved and influenced the evaluation process.

The Bowland Report seems to follow this familiar pattern of not describing its stakeholder involvement. A senior newspaper and university hospital were the only organizations mentioned, and they appeared to only play a role in advertising for research participants. The university sponsoring the evaluation approved of the ethical plan of the evaluation, so it could also be considered a stakeholder. The Bowland Report addressed the needs of older women experiencing costly physical and mental health trauma. As such, the research team may have involved several types of stakeholders in the development of the evaluation, including mental health providers, spiritual leaders, and other groups specifically interested in the well-being of older adults. Clients of social services and survivors of traumas and abuse may also have been involved. The input of these groups, if consulted, would have significantly informed the evaluators about the needs of survivors of complex traumas, the interventions they required, outcome measures of importance, and other research components. Unfortunately, we do not know what the roles of stakeholders were, and this could be considered a weakness of the report.

### Step 3: Determine the Purpose of the Evaluation

Step 3 is possibly the most important step for the critical consumer to understand— what was the purpose of the evaluation? After reviewing the abstract of a report, a critical consumer would be wise to look for the overall purpose statement! The purpose of an evaluation is the pivotal section of a report that is most important to understand early on. If a critical consumer does not understand the evaluation’s general purpose, the rest of the report becomes much more difficult to comprehend. The purpose of an evaluation can be either in the form of general questions to explore or hypotheses to test or both. When general questions are used, an evaluation tends to be more exploratory and tentative about what it finds, while the use of hypotheses suggests an explanatory study providing more definitive answers. The purpose statement is typically located just prior to the method section of a report; the method section then elaborates on how the purpose is to be implemented. A report needs to have clear and conceptually sound questions or hypotheses. These questions or hypotheses signal whether the evaluation will be relevant to a critical consumer’s practice area and other interests.

The Bowland Report has four hypotheses that it tests. The hypotheses are somewhat ambiguously lumped together and could be misunderstood as one hypothesis. They are described as “a group intervention supporting the discussion of spiritual struggles and strengths and the development of new spiritual resources would reduce trauma-related posttraumatic stress, depression, anxiety, and somatic symptoms for older survivors of interpersonal trauma who indicated interest in such a group” (Bowland et al., 2012, p. 74). It would be clearer to identify each hypothesis separately with the intervention being the independent variable and the client outcome being the dependent variable for each hypothesis. So, for example, the first hypothesis would be that the spiritual intervention will reduce the trauma-related posttraumatic stress of these women; the second hypothesis would be that the spiritual intervention will reduce their depression; and so on. The authors located the hypotheses well before the method section, which may confuse less experienced critical consumers in trying to find them.

The authors also discuss the concept of spirituality, citing several references. Perhaps they could have considered adding one or more client outcome variables measuring strengths and resiliencies as well as reducing mental symptoms. For example, they could have measured well-being, self-efficacy, happiness, and other concepts that approximate personal assets.

### Step 4: Plan the Evaluation

Step 4 is a very technical step for a critical consumer to comprehend and absorb, and having some background in research methodologies would certainly help. A critical consumer’s task in critiquing an evaluation plan or research design is quite challenging because it requires considerable technical knowledge about research methodologies. There are several aspects of a research design to critique including the intervention, client outcome measures, the group design used, sampling issues and characteristics of the sample, the data analysis plan, ethical concerns, and other issues. Chapters 6, 8, and 9 of the book describe different types of evaluations respectively, during the planning stage, implementation stage, and outcome stage. The Bowland Report that is the focus of this chapter uses an outcome evaluation. As Chapter 9 indicates, the intervention and the client outcomes that are affected by it are the primary focus. An outcome evaluation is attempting to determine whether there is a causal relationship between an intervention and these outcomes: does the introduction of the intervention improve the client outcome measures? Several group designs are described in Chapter 9 varying from a one-group posttest-only design to a pretest–posttest design with a control group. The latter design is the most effective one for determining whether the intervention has a causal effect. This design also creates ethical problems for those who are denied the intervention because some are selected randomly to a control group, not the treatment group. The sampling approach of this design is also important to examine to determine any biases in assigning clients to treatment and control groups. Measures of the outcome variables are also very important to examine in terms of evidence that they have validity and reliability. A further issue is whether the intervention is described in enough detail for replication by a reader.

Overall, Bowland and her colleagues did an excellent job describing the evaluation plan. The report reveals that they used an experimental design with a control group. Three points in time were designated for measuring the outcome variables—a pretest, eleven weeks later at the completion of the intervention as a posttest, and three months beyond the time they completed the intervention. Several things are important to include in describing such a design and the authors covered this material quite well. The intervention was described extensively. The definition of spirituality, a difficult one to obtain broad consensus on, was described with considerable detail. Spiritual strengths were highlighted as one of the areas of spirituality to be emphasized in the intervention, and some details were shared in the eleven-session curriculum about how spiritual strengths could help. The other major aspect of spirituality that was emphasized in the intervention was the difficulties that these older women may have had growing up with backgrounds involving patriarchal Christianity and the burdens it may have imposed.

The connection between the problems of these women and the proposed intervention is a most important question to ask. As was stated in Chapter 8, what are the mechanisms for change inherent in the intervention that can help these women counteract their existing symptoms and their underlying problems? This is another way of asking whether the intervention can have a causal effect on the client outcomes of these women. According to the authors, their spiritual intervention was based on a previous spiritual model that had some evidence of reducing trauma related mental health symptoms.

Client outcomes were also described clearly, including the four major ones: posttraumatic stress, depression, anxiety, and somatic symptoms. The measurement instruments for these outcomes were described in some detail and a table on “outcome and screening measures” was used to help the critical consumer visualize all this material in one place. Many of the psychometric properties of these measures were reported while some measures may have lacked validity or reliability testing.

The authors described their extensive efforts to recruit older women with trauma and abuse in their background. As was mentioned earlier in the chapter, it would not be easy to find such research participants if they were not already known to an agency. Some recruitment and screening strategies were described that could be very helpful to know if one were planning to replicate this study. Steps were taken to create a pool of women who met the criteria of the evaluators. They were selected through convenience and purposive sampling, which limits the authors’ ability to generalize the findings to others with similar characteristics with confidence.

The authors appropriately noted that the study was approved by the Human Research Protection Office of the sponsoring university. The ethics of using control groups and assigning some of the women to a control group on a random basis was partially addressed by offering participants assigned to control groups the opportunity to join the treatment group at a later time. However, the ethics of using control groups were not explicitly mentioned in terms of initially assigning some participants to control groups when they may have had an immediate need to benefit from the intervention. Also, only some of those in control groups later agreed to join a treatment group, leaving open the possibility that the experience of being in a control group may have discouraged them from seeking the treatment at a later time.

A very unusual and special strength of this design was a follow-up, a posttest three months after the intervention was completed. This second posttest measure gives the reader some evidence of whether the gains at the completion of treatment were still evident three months later. Unfortunately, follow-up measures several months after an intervention are seldom included in most evaluations.

A data analysis plan was described in the Bowland Report and seemed appropriate for a group design. It may be enough for a new critical consumer to merely be able to identify these tests and understand in general how they were used. While many of the statistical tests and analyses were complex and may be confusing to some critical consumers, the authors made a notable effort to explain them. Among the specific tests that were used were descriptive statistics (mean scores and standard deviations for each variable). The inferential statistics used to test the hypotheses were chi-square, analysis of variance (ANOVA), multivariate analysis of variance (MANOVA), and _t_-tests. The _t_-test was used to determine whether there was statistical significance in the changes in the client outcomes from pretest to posttest, while ANOVA and MANOVA were used to determine whether the changes were maintained at a three-month follow-up.

### Step 5: Implement the Evaluation

Step 5, like step 2, is difficult for critical consumers to critique because most evaluation reports provide little if any information about implementation processes, at least in a technical report like the Bowland Report. Some of the questions about the implementation process raised in Chapter 8 are important for critical consumers to consider here. Is the intervention being implemented as it was proposed? Also, is the quality of the intervention high enough to satisfy the stakeholders? These two questions can possibly begin to be answered by finding out if the implementation process was monitored. If it was not monitored, it would be highly unlikely that these questions could be answered. Some further questions could also be asked in attempting to find out whether a program is implemented as proposed. Are the program’s goals and objectives evident or identifiable in the description of the way in which the intervention is implemented? Are the clients being served the ones who were intended to be served, and are existing staff members adequately qualified and trained to provide the proposed services?

Bowland and her colleagues reported on some of these implementation questions, which is a notable strength of their report. At least three instances of this are evident: they report (a) that they monitored how well the group leaders implemented the spirituality treatment model in their groups based on how it was designed, (2) whether the clients that were selected met the initial criteria, and (3) whether the data collectors were trained to do what was expected in measuring the client outcomes.

First, to assess “leader fidelity” or the group leaders’ ability to implement the spirituality treatment model as planned, an independent evaluator rated videotaped sessions randomly selected from each of the treatment and control groups (Bowland et al., 2012, p. 76). The mean adherence score for all sessions was 83.3 percent, which is above an existing ideal standard of leader fidelity of 80 percent.

Second, selection of the research participants was described to be carefully implemented. The research participants who were selected had to meet stringent criteria, including being older women who experienced interpersonal trauma, had an absence of cognitive or psychotic impairments, were not involved in psychotherapy, had a history of practicing the Christian tradition, and had no issues with being suicidal. The minimum age requirement was not specified, but the youngest participant was 55 years old. Descriptions of how this screening was done were evident in the report using a phone screening protocol followed by an in-person clinical interview. As a final check, all the participants in the sample were found to have more than one traumatic event, and all but one of them reported multiple types of violence and abuse. Third, data were gathered from the research participants on the outcome variables at three points in time, before and after the intervention, as well as at a three-month follow-up. The authors indicated that the data collectors were five PhD students who were trained in the interview protocols and had clinical experience treating trauma survivors. To preclude the potential for bias, the data collectors were not informed of the group assignments (treatment or control) of each research participant. In terms of the reliability of the measurements of the four outcomes variables, all four instrument measures had Cronbach’s alphas, a reliability measure, at or above 0.78 out of 1.00.

### Step 6: Prepare a Written or Oral Report

Strategies in preparing a report are described in Chapter 11. The data are organized, analyzed, and finally interpreted. Critiquing step 6 involves examining the finding section, recommendations, limitations, and other issues. Considerable thoughtful work is required in creating presentations of the findings that will be understandable to those reading the report. Evaluation reports usually tend to address the major questions of stakeholders reflected in the initial purpose statements worked out in step 3. This report can be oral, written, or both. Visual aids, such as tables and graphs, are often included in the findings section to assist in highlighting the most important findings; thus, tables and graphs are important for critical consumers to critique along with the findings in text form. For example, are the tables clear and do they serve a useful purpose in highlighting or elaborating on a set of findings?

Theinterpretationsofthefindingsalsorequiremuchthoughtandcareandshould   be developed in keeping with the needs of the various stakeholders. Interpretations usually begin with a summary of the most important findings followed by recommendations that are supported by the findings. These recommendations also need to be useful and practical to the various stakeholders. Limitations in the interpretations should be acknowledged based on limitations in the sampling procedures, some group designs, and measurement instruments.

The findings of the Bowland Report are complicated because they use a group design that involves many factors. With a group design, a critical consumer must determine to what extent it can be claimed that the intervention and not something else has a positive impact on the outcome variables. This is referred to as internal validity. Similarly, a critical consumer must also determine the extent to which the impact on the outcomes can be generalized to other client groups in other settings. This is referred to as external validity. Reading over the findings several times can help detect many of these issues.

The Bowland Report reported their findings in steps. First, they presented an analysis of the pretest scores of the participants’ background characteristics and the four outcome variables for both the treatment and control groups; the results in Table 2 of the report indicated that these scores were not significantly different, and therefore they conclude that the groups were equivalent. This is an important step to take to determine if the differences in the participants in the two groups could be an extraneous factor influencing the intervention’s effects on the outcome variables.

Second, a complex statistical analysis of the four hypotheses was presented, which indicated that the treatment groups made significant progress in reducing the scores of the four outcome variables. The women’s scores for three of the outcomes (depression, anxiety, and physical health symptoms) in the treatment groups were reduced quite significantly, while they were not reduced in the control groups. This is displayed clearly in Table 4 (Bowland et al., 2012, p. 79). The intervention’s impact on the posttraumatic stress symptoms of the women, the other outcome variable, was found to be significant but less so than the impact the intervention had on the other three outcome variables. This may explain why these scores are not included in Table 4 of the report, but their absence is not explained.

The complexity of the statistical tests that were used was probably beyond what most critical consumers could fully understand, raising the question of whether this presentation could have been simplified. It is likely that the editorial board of the journal requested this degree of analysis. Yet, it would be wise to simplify the analysis for other stakeholders who may need less of this statistical evidence. This could easily be done by preparing a different report format for some stakeholders needing less emphasis on the statistical analysis. We will return to using multiple report formats later.

A central question for critical consumers that was not addressed is, to whom could these findings be generalized? This is a very important question for critical consumers who may wish to use these findings with their client populations. Generalizing to other groups should always be done with extreme caution even if a critical consumer’s clients have characteristics similar to those in the sample. Among the reasons caution is important is because the initial participant pool was selected using nonprobability sampling approaches and could have been skewed by a selection process that inevitably favored some women over others largely in unknown ways.

The discussion section followed the findings and involved four sets of recommendations. It is most important for critical consumers to first identify each recommendation and then determine what findings, if any, support each recommendation. The first recommendation was that trauma-informed spiritual services should be provided in both community-based agencies and long-term care facilities. The findings supporting this recommendation were that all four interpersonal trauma symptoms were significantly reduced by the spiritual intervention. The second recommendation stressed the value of practitioners seriously attending to clients’ spiritual issues and resources to help them cope more effectively with the impact of trauma. This was supported by the finding that the intervention reducing these symptoms focused on spirituality issues.

The third recommendation was more in the form of a question to the critical consumers or readers. The question seemed to suggest that critical consumers can decide where group treatment models like their spiritual intervention could be offered. They asked, can these models be situated in assisted living centers in longterm care facilities or a church congregation? Stigma issues were mentioned as a reason to resist considering using mental health settings. Findings did not directly support this recommendation; it was not altogether clear why this is an important topic to raise without identifying some of the obstacles that the authors found in introducing their group model.

The fourth recommendation was also offered in the form of a question about whether social workers are qualified to address spiritual issues as a group leader or whether they may need to work collaboratively with clergy who presumably have this expertise. Growing numbers of social workers have been incorporating spirituality into their practice over the last thirty years and a growing body of social work knowledge in spirituality work could have been referenced here but was not. The authors did mention later in the conclusion that all the facilitators in the study had training in theology and pastoral care. Possibly they could have suggested specific spirituality competencies or skills that would be important for those wanting to replicate their study; they did mention a feminist theological perspective.

Limitations were discussed in the discussion section. The authors identified a somewhat unusual one, namely the first author implemented the recruitment, screening, and intervention. Social desirability biases brought about by the direct involvement of the first author were identified as an issue, but no further explanation was given for why these were a limitation. Other limitations that were briefly identified in the report were limited funds for advertising and having only one person directly involved; this was explained to make the point that the screening process took longer than desired. A small sample was mentioned as another limitation, and reliability was noted as missing in measuring the outcomes because only one evaluator was involved.

Other limitations were not mentioned. One important one could have been the ethical issues of using control groups. The authors explained that all the participants in the control groups were later offered participation in a treatment group. However, possible ethical issues remained because the participants’ access to the intervention was delayed and possibly inadvertently tainted by being forced to first participate in a control group. Another limitation that was indirectly noted related to the sample. Older women with interpersonal trauma are a difficult group to find just by the undesirability of having and sharing trauma experiences. The authors noted initial challenges in recruiting a broadly representative sample; most of the participants were self-selected as they volunteered because “they were interested in the project.” Limitations in including only Christian participants and leaving out a measure of social support were mentioned as issues to address in future evaluations.

### Step 7: Disseminate the Findings

Step 7, the last step of an evaluation, is to disseminate the findings to stakeholders and other interested people. Unfortunately, this step is often overlooked or minimized in importance, as explained in Chapter 11. This is unfortunate because if the findings are not disseminated to stakeholders in several formats, they will inevitably be underutilized. Many stakeholders can only be effectively reached in brief written formats such as one-page summaries, brief bulletins, newsletter inserts, and timely handouts posted on bulletin boards. Many other stakeholders can only be reached with oral presentations in forums such as public hearings, informal conversations, workshops, perhaps a series of topical seminars, or other venues.

Some stakeholders could be overlooked in the dissemination of the findings and recommendations based on a variety of questionable motives such as the following: that some stakeholders have nothing to offer, that they will only bring more questions and headaches with them, that they will take too much time, or they will simply not understand the more “technical” issues. Unfortunately, clients, family members, and many community groups are among those typically overlooked or viewed as irrelevant to the evaluation process for such inappropriate reasons as those given above. These questionable motives need to be challenged and overcome as the next section suggests.

## THE NEED FOR MULTIPLE STRATEGIES ON REPORTS

Ideally, a variety of evaluation reports need to be prepared and disseminated that are relevant to different stakeholders, rather than taking the position that one report “fits all.” As indicated in Chapter 11, stakeholders have widely varying needs and capacities as critical consumers. Different evaluation reports are ideally prepared and tailored to the needs of funding agencies, administrators, staff members, agency boards, and clients. Technical reports can be ideal for funding agencies, administrators of agencies sponsoring an intervention, and some key staff members. They are also ideal for most journal submissions. In contrast, technical reports are too lengthy and may be challenging for others to understand. This usually includes most board members, most staff members, volunteers, clients, community groups, and others. Executive summaries of technical reports often work well for many legislators, board members, and others who may be interested in reading a succinct summary of the findings and recommendations of a technical report. These stakeholders usually do not have the time or sometimes the capacity to thoroughly understand a technical report. In these instances, specific sections of the report could also be provided on an as-needed basis. This could be handled informally and in an individualized manner.

Staff members are primarily interested in how the findings of a report will affect them and their roles as practitioners. In these cases, staff workshops, training sessions, informal presentations and dialogues, focused supervisory sessions, and other staff venues can be targeted to specific areas of the report that directly involve staff. Chapter 11 provides more information about the importance of informing staff about the findings of an evaluation and how this can be done. Reports for clients are usually different from reports for all the other stakeholders and will be discussed in the final section of the chapter.

The Bowland Report is clearly a technical report. It is similar to the technical report described in Chapter 11 and includes a number of required content areas including a literature review, relevant theoretical concepts, research questions to be asked or hypotheses to be tested, the sampling approach and characteristics of the sample, the data collection approach, data analysis plan, findings, recommendations, and limitations of the study. The Bowland Report, when critiqued as a technical report, scores highly. It covers all the sections of a technical report and generally describes them well.

As has been mentioned earlier, consulting all the important stakeholders is an important step to take prior to determining what reports can directly benefit each of them. Unfortunately, it is not possible to know how step 7 was taken by Bowland and colleagues based on what the report leaves out about dissemination. Perhaps other reports were prepared for clients and other stakeholders. We just do not know from their report. However, if Bowland and her colleagues only prepared the technical report published in the _Social Work_ journal, they would probably have fallen short of preparing and disseminating the kinds of information needed by all their important stakeholders.

In review, several of the steps in the Seven Evaluation Steps are left out or are downplayed in the Bowland Report as they are in most technical reports. A description of step 2 (identify and explore ways to involve stakeholders) is not evident even though the stakeholders can have a profound effect on an evaluation. While stakeholder participation involves a political process, there are ways to thoughtfully and appropriately disclose how stakeholders assisted in shaping the evaluation. Step 5 (implement the evaluation) is also largely left out of the Bowland Report and is an important aspect of an evaluation to disclose. Finally, step 7 (disseminate the findings) is largely ignored.

## HELPING CLIENTS BECOME CRITICAL CONSUMERS

Clients who are recipients of interventions are seldom seen as relevant stakeholders of an evaluation. Many reasons can be given for this, such as clients are viewed as incapable of being critical consumers, they have nothing to offer, involving them will take too much effort, and giving them a critical consumer role conflicts with their client role. Perhaps this kind of questionable reasoning is also based on the assumption that the technical report is the only “official” evaluation report. It is true that most clients would have enormous challenges understanding a technical report like the Bowland Report.

We have pointed out in both this chapter and Chapter 11 that many types of reports and many dissemination strategies may be necessary to use if maximum benefits are to come from an evaluation. It is also important to stress that the clients who are the recipients of these interventions have special needs to be addressed as critical consumers that are unlike any other stakeholders. Let’s review what some of these needs are, how we can help them become critical consumers, and what the benefits could be to agencies.

Evaluation reports can be simplified for various stakeholders without necessarily removing important meaning. As an example, the statistical analysis involved in an evaluation is necessary to complete and report on, but it doesn’t have to be displayed in all its detail in the reports going to many of the stakeholders. Reports prepared for many of the stakeholders of an evaluation, especially clients, should, by necessity, be simplified. Many of the aspects of an evaluation, including the purpose, research design, data analysis, findings, and recommendations can be simplified and still retain sufficient meaning for clients and some other stakeholders. Here are some examples of how concepts can be simplified in reports to some groups, using terms used in the Bowland Report:

> _Example of a simplified definition of a needs assessment:_ A study that gathers accurate information about the needs of a group of people.
> 
> _Example of a simplified hypothesis:_ Clients with trauma can be relieved of some of their trauma by participating in a spirituality treatment group.
> 
> _Example of a simplified finding:_ The women’s trauma symptoms were reduced by participating in the spirituality treatment group.
> 
> _Example of a simplified recommendation_: Services need to be provided in both community and institutional programs that help older women with past traumas.

Evaluators must be at least somewhat convinced that sharing the results of an evaluation with clients is beneficial. This would not necessarily mean that all clients would be expected to be such critical consumers. Perhaps, evaluators should start with clients who are higher functioning, seem to have more verbal skills and greater intellect, and, of course, express a willingness to do this. Client volunteers could be sought by various means such as an open agency invitation to come to a social event where the findings of the evaluation will be presented. The event could include refreshments and other enticements.

Clients would also need to be convinced that they could benefit from taking on a critical consumer role. Also, they would need to have the confidence that they could understand the basics of an evaluation report including its purpose, research design, results, and recommendations. They may also need some technical assistance. The benefits to the agency of involving clients as critical consumers could be potentially enormous. One benefit would be that clients are given an opportunity to give feedback on the findings, especially findings about how they were reported to be helped. Further, the clients could be asked to give their explanations for why some of the findings may have been incomplete or ambiguous to other stakeholders. For example, in the Bowland Study, the posttraumatic stress symptoms were not found to be reduced as easily and as much as the other symptoms. Why might that be? Clients with these symptoms may have some possible explanations. Also, the spiritual intervention that the agency introduced, like any intervention, has strengths and weaknesses. What might the clients think were some of its strengths and weaknesses? How could it be improved? Empowering clients with a critical consumer role can strengthen the trust that clients have in the provider agency because they are being recognized as an important source of information. Also, many clients may feel better about themselves and their provider because they are being treated with greater dignity.

It may seem odd to some to end the book with a discussion of how clients can be encouraged to become more actively involved in the evaluation process. Yet, this is a very upbeat and relevant note to end with because we still have much to learn about how to involve clients. Indeed, evaluations are a dynamic and ever-changing enterprise. Evaluations will continue to grow in importance in the future as a tool for holding social programs accountable, and clients will likely be expected to be significant partners in this endeavor.

## SUMMARY

This chapter focuses on the consumer role of reading evaluation reports. All the previous chapters have focused on the producer role of conducting evaluations. Several different kinds of stakeholders who consume evaluation reports are discussed in term of their varying needs and capacities to consume these reports. Stakeholders of evaluations potentially include a long list of people including university students, administrators and practitioners in agencies, board members, regulatory and funding agency personnel, clients, advisory groups, community groups, and others. This chapter briefly discusses the special needs of five of these groups—students, practitioners, administrators, reimbursement and regulatory entities, and clients of social work services. The chapter goes on to describe in detail how an actual evaluation report can be critically consumed so that it is has maximum meaning and utility for a consumer. The article chosen as an example of an evaluation report reflects many of the characteristics of the types of reports that need to be consumed. The report, “ Evaluation of a Spiritually Focused Intervention with Older Trauma Survivors,” focused on evaluating the effectiveness of a spiritually oriented group intervention intended to lessen the physical and mental health symptoms of a group of older female trauma survivors (Bowland, Edmond, & Fallot, 2012). The chapter closes with a discussion about how evaluation reports can be prepared in special ways so that clients, their families, and communities can fully benefit from them.

## DISCUSSION QUESTIONS AND ASSIGNMENTS

1. Examine for yourself how the Bowland Report addresses each of the seven evaluation steps in the chapter. Then answer how well you think Bowland and associates completed each of these steps. How did your answers compare to the answers given in this chapter?
2. Assume you are the evaluators of the Bowland Report. Prepare a report of the evaluation that will be useful to staff members. First, choose a format for your presentation using one of the formats described in Chapter 11. Next, prepare an actual presentation. It can either be done orally or in written form.
3. Assume that you have prepared an oral report of the findings pertaining to the four hypotheses in the Bowland Report to be given to staff members. Now roleplay this presentation with two people being the co-presenters and four or five people being staff members hearing and reacting to these findings for the first time. Discuss the implications for how their practice can benefit from these findings.
4. In a role-play, select one of the hypotheses in the Bowland Report pertinent to a client who is an older woman experiencing severe physical and mental health trauma. Now discuss this hypothesis and the results of testing it with this client (or a client group). Help the client compare these results with what her situation is and facilitate a discussion that is helpful to the client.

# REFERENCES

- Bolin, B. B., Lee, K. H., Glenmaye, L. F., & Yoon, D. P. (2012). Impact of research orientation on attitudes toward research of social work students. _Journal of Social Work Education,_ _48,_ 223–243.
- Bowland, S., Edmond, T., & Fallot, R. D. (2012). Evaluation of a spiritually focused intervention with older trauma survivors. _Social Work, 57,_ 73–82.
- Collins-Camargo, C., & Millar, K. (2010). The potential for a more clinical approach to child welfare supervision to promote practice and case outcomes: A qualitative study in four states. _The Clinical Supervisor, 29,_ 164–187.
- Dudley, J. R. (2011). _Research methods for social work: Being producers and consumers of research_ (upd. 2nd ed.). Boston, MA: Pearson.
- Gibbs, L. (2007). Applying research to make life-affecting judgments and decisions. _Research_ _on Social Work Practice, 17,_ 143–150.
- Harder, J. (2010). Overcoming MSW students’ reluctance to engage in research. _Journal of_ _Teaching_ _in Social Work, 30,_ 195–209.
- Howard, M. O., McMillen, C. J., & Pollio, D. E. (2003). Teaching evidence-based practice: Toward a new paradigm for social work education. _Research on Social Work Education, 13,_ 234–259.
- Long, D. D., Tice, C. J., & Morrison, J. D. (2006). _Macro social work practice._ Belmont, CA: Brooks/Cole.
- Lundahl, B. W. (2008). Teaching research methodology through active learning. _Journal of_ _Teaching_ _in Social Work, 28,_ 273–288.
- Mildon, R., & Shlonsky, A. (2011). Bridge over troubled water: Using implementation science to facilitate effective services in child welfare. _Child Abuse & Neglect, 35,_ 753–756.
- National Association of Social Workers. (2017). _Code of ethics._ Washington, DC: Author.
- Petr, C. G., & Walter, U. M. (2005). Best practices inquiry: A multidimensional, valuecritical framework. _Journal of Social Work Education, 41,_ 251–267.
- Staudt, M. M. (2007). Two years later: Former students’ perceptions of a clinical evaluation course and current evaluation practices. _Journal of Teaching in Research, 27,_ 125–139. Wilkerson, D., Migas, N., & Slaven, T. (2000). Outcome-oriented standards and performance indicators for substance dependency rehabilitation programs. _Substance Use &_ _Misuse, 35,_ 1679–1703.